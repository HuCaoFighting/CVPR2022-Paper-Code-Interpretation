* 推荐阅读：<br>
  * [ICCV2021/2019/2017 论文/代码/解读/直播合集](https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation)
  * [2020-2021年计算机视觉综述论文汇总](https://github.com/extreme-assistant/survey-computer-vision)
  * [国内外优秀的计算机视觉团队汇总](https://github.com/extreme-assistant/Awesome-CV-Team)

------

# CVPR2023最新信息及论文下载（Papers/Codes/Project/PaperReading／Demos/直播分享／论文分享会等）

官网链接：https://cvpr.thecvf.com/Conferences/2023<br>
论文接收公布时间：2023年2月28日<br>

相关问题：[如何评价 CVPR 2023 的论文接收结果？](https://www.zhihu.com/question/585474435)<br>
相关报道：[CVPR 2023 接收结果出炉！录用2360篇，接收数量上升12%](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637344&idx=1&sn=62f70870c3f76e6b5a401373986a6ee5&chksm=ec123499db65bd8ffc8987ec1f61b1389e666ed32772149e7c7e76923e2f427ef57e55b57d72&token=1890936456&lang=zh_CN#rd)


>update: <br>
>2023/2/28 [更新13篇](https://www.cvmart.net/community/detail/7212)<br>
>2023/3/02 [更新54篇](https://www.cvmart.net/community/detail/7388)<br>
>2023/3/09 [更新35篇](https://www.cvmart.net/community/detail/7403)<br>
>2023/3/15 [更新29篇](https://www.cvmart.net/community/detail/7419)<br>
>2023/3/16 [更新8篇](https://www.cvmart.net/community/detail/7421)<br>
>2023/3/17 [更新19篇](https://www.cvmart.net/community/detail/7428)<br>
>2023/3/20 [更新37篇](https://www.cvmart.net/community/detail/7435)<br>
>2023/3/22 [更新37篇](https://www.cvmart.net/community/detail/7444)<br>

<br><br>

# 目录

[1. CVPR2023 接受论文/代码分方向汇总（更新中）](#1)<br>
[2. CVPR2023 spotlight（更新中）](#2)<br>
[3. CVPR2023 论文解读汇总（更新中）](#3)<br>
[4. CVPR2023 极市论文分享](#4)<br>
[5. To do list](#5)<br>

<br>

<a name="1"/> 

# 1.CVPR2023接受论文/代码分方向整理(持续更新)


## 分类目录：

### [1. 检测](#detection)

* [2D目标检测(2D Object Detection)](#IOD)
* [视频目标检测(Video Object Detection)](#VOD)
* [3D目标检测(3D Object Detection)](#3DOD)
* [人物交互检测(HOI Detection)](#HOI)
* [伪装目标检测(Camouflaged Object Detection)](#COD)
* [旋转目标检测(Rotation Object Detection)](#ROD)
* [显著性目标检测(Saliency Object Detection)](#SOD)
* [关键点检测(Keypoint Detection)](#KeypointDetection)
* [车道线检测(Lane Detection)](#LaneDetection)
* [边缘检测(Edge Detection)](#EdgeDetection)
* [消失点检测(Vanishing Point Detection)](#VPD)
* [异常检测(Anomaly Detection)](#AnomalyDetection)

### [2. 分割(Segmentation)](#Segmentation)

* [图像分割(Image Segmentation)](#ImageSegmentation)
* [全景分割(Panoptic Segmentation)](#PanopticSegmentation)
* [语义分割(Semantic Segmentation)](#SemanticSegmentation)
* [实例分割(Instance Segmentation)](#InstanceSegmentation)
* [超像素(Superpixel)](#Superpixel)
* [视频目标分割(Video Object Segmentation)](#VOS)
* [抠图(Matting)](#Matting)
* [密集预测(Dense Prediction)](#DensePrediction)

### [3. 图像处理(Image Processing)](#ImageProcessing)

* [超分辨率(Super Resolution)](#SuperResolution)
* [图像复原/图像增强/图像重建(Image Restoration/Image Reconstruction)](#ImageRestoration)
* [图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)](#ISR)
* [图像去噪/去模糊/去雨去雾(Image Denoising)](#ImageDenoising)
* [图像编辑/图像修复(Image Edit/Image Inpainting)](#ImageEdit)
* [图像翻译(Image Translation)](#ImageTranslation)
* [图像质量评估(Image Quality Assessment)](#IQA)
* [风格迁移(Style Transfer)](#StyleTransfer)
* [图像配准(Image Registration)](#ImageRegistration)

### [4. 视频处理(Video Processing)](#VideoProcessing)

* [视频编辑(Video Editing)](#VideoEditing)
* [视频生成/视频合成(Video Generation/Video Synthesis)](#VideoGeneration)
* [视频超分(Video Super-Resolution)](#VideoSR)

### [5. 估计(Estimation)](#Estimation)

* [光流/运动估计(Flow/Motion Estimation)](#Flow/Pose/MotionEstimation)
* [深度估计(Depth Estimation)](#DepthEstimation)
* [人体解析/人体姿态估计(Human Parsing/Human Pose Estimation)](#HumanPoseEstimation)
* [手势估计(Gesture Estimation)](#GestureEstimation)

### [6. 图像&视频检索/(Image&Video Retrieval/Video Understanding)](#ImageRetrieval)

* [行为识别/行为识别/动作识别/检测/分割(Action/Activity Recognition)](#ActionRecognition)
* [行人重识别/检测(Re-Identification/Detection)](#Re-Identification)
* [图像/视频字幕(Image/Video Caption)](#VideoCaption)

### [7. 人脸(Face)](#Face)

* [人脸识别/检测(Facial Recognition/Detection)](#FacialRecognition)
* [人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)](#FaceSynthesis)
* [人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)](#FaceAnti-Spoofing)

### [8. 三维视觉(3D Vision)](#3DVision)

* [点云(Point Cloud)](#3DPC)
* [三维重建(3D Reconstruction)](#3DReconstruction)
* [场景重建/视图合成/新视角合成(Novel View Synthesis)](#NeRF)

### [9. 目标跟踪(Object Tracking)](#ObjectTracking)

### [10. 医学影像(Medical Imaging)](#MedicalImaging)

### [11. 文本检测/识别/理解(Text Detection/Recognition/Understanding)](#TDR)

### [12. 遥感图像(Remote Sensing Image)](#RSI)

### [13. GAN/生成式/对抗式(GAN/Generative/Adversarial)](#GAN)

### [14. 图像生成/图像合成(Image Generation/Image Synthesis)](#IGIS)

### [15. 场景图(Scene Graph](#SG)

* [场景图生成(Scene Graph Generation)](#SGG)
* [场景图预测(Scene Graph Prediction)](#SGP)
* [场景图理解(Scene Graph Understanding)](#SGU)

### [16. 视觉定位/位姿估计(Visual Localization/Pose Estimation)](#VisualLocalization)

### [17. 视觉推理/视觉问答(Visual Reasoning/VQA)](#VisualReasoning)

### [18. 视觉预测(Vision-based Prediction)](#Vision-basedPrediction)

### [19. 神经网络结构设计(Neural Network Structure Design)](#NNS)

* [CNN](#CNN)
* [Transformer](#Transformer)
* [图神经网络(GNN)](#GNN)
* [神经网络架构搜索(NAS)](#NAS)
* [MLP](#MLP)

### [20. 神经网络可解释性(Neural Network Interpretability)](#interpretability)

### [21. 数据集(Dataset)](#Dataset)

### [22. 数据处理(Data Processing)](#DataProcessing)

* [数据增广(Data Augmentation)](#DataAugmentation)
* [归一化/正则化(Batch Normalization)](#BatchNormalization)
* [图像聚类(Image Clustering)](#ImageClustering)
* [图像压缩(Image Compression)](#ImageCompression)

### [23. 图像特征提取与匹配(Image feature extraction and matching)](#matching)

### [24. 视觉表征学习(Visual Representation Learning)](#VisualRL)

### [25. 模型训练/泛化(Model Training/Generalization)](#ModelTraining)

* [噪声标签(Noisy Label)](#NoisyLabel)
* [长尾分布(Long-Tailed Distribution)](#Long-Tailed)

### [26. 模型压缩(Model Compression)](#ModelCompression)

* [知识蒸馏(Knowledge Distillation)](#KnowledgeDistillation)
* [剪枝(Pruning)](#Pruning)
* [量化(Quantization)](#Quantization)

### [27. 模型评估(Model Evaluation)](#ModelEvaluation)

### [28. 图像分类(Image Classification)](#ImageClassification)

### [29. 图像计数(Image Counting)](#CrowdCounting)

### [30. 机器人(Robotic)](#Robotic)

### [31. 半监督学习/弱监督学习/无监督学习/自监督学习(Self-supervised Learning/Semi-supervised Learning)](#self-supervisedlearning)

### [32. 多模态学习(Multi-Modal Learning)](#MMLearning)

* [视听学习(Audio-visual Learning)](#Audio-VisualLearning)
* [视觉-语言（Vision-language）](#VLRL)

### [33. 主动学习(Active Learning)](#ActiveLearning)

### [34. 小样本学习/零样本学习(Few-shot/Zero-shot Learning)](#Few-shotLearning)

### [35. 持续学习(Continual Learning/Life-long Learning)](#ContinualLearning)

### [36. 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)](#domain)

### [37. 度量学习(Metric Learning)](#MetricLearning)

### [38. 对比学习(Contrastive Learning)](#ContrastiveLearning)

### [39. 增量学习(Incremental Learning)](#IncrementalLearning)

### [40. 强化学习(Reinforcement Learning)](#RL)

### [41. 元学习(Meta Learning)](#MetaLearning)

### [42. 联邦学习(Federated Learning](#federatedlearning)

### [43. 自动驾驶(Federated Learning](#automatic driving)




### [其他](#100)



<br><br>

<a name="detection"/> 

## 检测



<br>

<a name="IOD"/> 

### 2D目标检测(2D Object Detection)

[4]CapDet: Unifying Dense Captioning and Open-World Detection Pretraining<br>
[paper](https://arxiv.org/abs/2303.02489) <br><br>

[3]Enhanced Training of Query-Based Object Detection via Selective Query Recollection<br>
[paper](https://arxiv.org/abs/2212.07593) | [code](https://github.com/Fangyi-Chen/SQR)<br><br>

[2]DETRs with Hybrid Matching<br>
[paper](https://arxiv.org/abs/2207.13080) | [code](https://github.com/HDETR)<br><br>

[1]YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors(YOLOv7)<br>
[paper](https://arxiv.org/abs/2207.02696) | [code](https://github.com/WongKinYiu/yolov7)<br><br>

<br>

<br>


<a name="VOD"/> 

### 视频目标检测(Video Object Detection)

[1]SCOTCH and SODA: A Transformer Video Shadow Detection Framework<br>
[paper](https://arxiv.org/abs/2211.06885) <br><br>

<br>

<br>

<a name="3DOD"/> 

### 3D目标检测(3D object detection)

[7]Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View<br>
[paper](https://arxiv.org/abs/2303.01686)<br><br>

[6]X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.02203)<br><br>

[5]Virtual Sparse Convolution for Multimodal 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.02314) | [code](https://github.com/hailanyi/virconv)<br><br>

[4]MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2209.03102) | [code](https://github.com/sxjyjay/msmdfusion)<br><br>

[3]Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.06880) | [code](https://github.com/PJLab-ADG/3DTrans)<br><br>

[2]LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion<br>
[paper](https://arxiv.org/abs/2303.03595) | [code](https://github.com/sankin97/LoGoNet)<br><br>

[1]ConQueR: Query Contrast Voxel-DETR for 3D Object Detection(3D 目标检测的Query Contrast Voxel-DETR)
[paper](https://arxiv.org/abs/2212.07289) | [code](https://github.com/poodarchu/ConQueR)<br><br>

<br>

<br>

<a name="HOI"/> 

### 人物交互检测(HOI Detection)

[1]Detecting Human-Object Contact in Images<br>
[paper](https://arxiv.org/abs/2303.03373)<br><br>

<br>

<br>

<a name="COD"/> 

### 伪装目标检测(Camouflaged Object Detection)

<br>

<br>

<a name="ROD"/> 

### 旋转目标检测(Rotation Object Detection)

<br>

<a name="SOD"/> 

### 显著性目标检测(Saliency Object Detection)

[1]Texture-guided Saliency Distilling for Unsupervised Salient Object Detection<br>
[paper](https://arxiv.org/abs/2207.05921) | [code](https://github.com/moothes/A2S-v2)<br><br>

<br>


<br>

<a name="KeypointDetection"/> 

### 关键点检测(Keypoint Detection)

<br>

<br>

<a name="LaneDetection"/> 

### 车道线检测(Lane Detection)

[1]BEV-LaneDet: a Simple and Effective 3D Lane Detection Baseline<br>
[paper](https://arxiv.org/abs/2210.06006)<br><br>

<br>

<br>

<a name="EdgeDetection"/> 

### 边缘检测(Edge Detection)

[1]Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections<br>
[paper](https://arxiv.org/abs/2212.03022) | [code](https://github.com/alexander-g/inbd)<br><br>

<br>

<br>

<a name="VPD"/> 

### 消失点检测(Vanishing Point Detection)

<br>

<br>

<a name="AnomalyDetection"/> 

### 异常检测(Anomaly Detection)

[4]Diversity-Measurable Anomaly Detection<br>
[paper](https://arxiv.org/abs/2303.05047)<br><br>

[3]Block Selection Method for Using Feature Norm in Out-of-distribution Detection<br>
[paper](https://arxiv.org/abs/2212.02295)<br><br>

[2]Lossy Compression for Robust Unsupervised Time-Series Anomaly Detection<br>
[paper](https://arxiv.org/abs/2212.02303) <br><br>

[1]Multimodal Industrial Anomaly Detection via Hybrid Fusion<br>
[paper](https://arxiv.org/abs/2303.00601) | [code](https://github.com/nomewang/M3DM)<br><br>

<br>


<br>

<a name="Segmentation"/> 


## 分割(Segmentation)

<br>

<a name="ImageSegmentation"/> 

### 图像分割(Image Segmentation)

[2]MP-Former: Mask-Piloted Transformer for Image Segmentation<br>
[paper](https://arxiv.org/abs/2303.07336) | [code](https://github.com/IDEA-Research/MP-Former)<br><br>

[1]Interactive Segmentation as Gaussian Process Classification<br>
[paper](https://arxiv.org/abs/2302.14578)<br><br>

<br>

<br>

<a name="PanopticSegmentation"/> 

### 全景分割(Panoptic Segmentation)

[1]Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.04803)<br><br>

<br>

<br>

<a name="SemanticSegmentation"/> 

### 语义分割(Semantic Segmentation)

[10]Token Contrast for Weakly-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.01267) | [code](https://github.com/rulixiang/toco)<br><br>

[9]Delivering Arbitrary-Modal Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.01480) | [code](https://github.com/jamycheung/DELIVER)<br><br>

[8]Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2211.12268)<br><br>

[7]Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP<br>
[paper](http://arxiv.org/abs/2210.04150) | [code](https://github.com/facebookresearch/ov-seg)<br><br>

[6]Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos<br>
[paper](https://arxiv.org/abs/2303.07224) | [code](https://github.com/THU-LYJ-Lab/AR-Seg)<br><br>

[5]SCPNet: Semantic Scene Completion on Point Cloud<br>
[paper](https://arxiv.org/abs/2303.06884) <br><br>

[4]On Calibrating Semantic Segmentation Models: Analyses and An Algorithm<br>
[paper](https://arxiv.org/abs/2212.12053) <br><br>

[3]Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision<br>
[paper](https://arxiv.org/abs/2301.09121) <br><br>

[2]Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2208.09910) | [code](https://github.com/LiheYoung/UniMatch)<br><br>

[1]Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2302.14250) <br><br>

<br>

<br>

<a name="InstanceSegmentation"/> 

### 实例分割(Instance Segmentation)

[3]Beyond mAP: Towards better evaluation of instance segmentation<br>
[paper](https://arxiv.org/abs/2207.01614)<br><br>

[2]ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution<br>
[paper](https://arxiv.org/abs/2303.00246)<br><br>

[1]PolyFormer: Referring Image Segmentation as Sequential Polygon Generation(PolyFormer：将图像分割表述为顺序多边形生成)<br>
[paper](https://arxiv.org/abs/2302.07387) <br><br>

<br>

<br>

<a name="Superpixel"/> 

### 超像素(Superpixel)

<br>

<a name="VOS"/> 

### 视频目标分割(Video Object Segmentation)

<br>

<br>

<a name="Matting"/> 

### 抠图(Matting)

<br>

<a name="DensePrediction"/> 

### 密集预测(Dense Prediction)

[1]DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction<br>
[paper](https://arxiv.org/abs/2303.01573)<br><br>

<br>

<br>

<a name="VideoProcessing"/> 

## 视频处理(Video Processing)

[3]Learning Transferable Spatiotemporal Representations from Natural Script Knowledge<br>
[paper](https://arxiv.org/abs/2209.15280) | [code](https://github.com/tencentarc/tvts)<br><br>

[2]UV Volumes for Real-time Rendering of Editable Free-view Human Performance<br>
[paper]([arxiv.org](https://arxiv.org/abs/2203.14402)) | [code]([github.com](https://github.com/fanegg/UV-Volumes))<br><br>

[1]Exploring Discontinuity for Video Frame Interpolation<br>
[paper]([[2202.07291\] Exploring Discontinuity for Video Frame Interpolation (arxiv.org)](https://arxiv.org/abs/2202.07291))<br><br>

<br>

<br>

<a name="VideoEditing"/> 

### 视频编辑(Video Editing)

[2]Text-Visual Prompting for Efficient 2D Temporal Video Grounding<br>
[paper](https://arxiv.org/abs/2303.04995)<br><br>

[1]Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation<br>
[paper](https://arxiv.org/abs/2303.00440) | [code](https://github.com/MCG-NJU/EMA-VFI)<br><br>

<br>

<br>

<a name="VideoGeneration"/> 

### 视频生成/视频合成(Video Generation/Video Synthesis)

[3]MOSO: Decomposing MOtion, Scene and Object for Video Prediction<br>
[paper](https://arxiv.org/abs/2303.03684) | [code](https://github.com/anonymous202203/moso)<br><br>

[2]SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation<br>
[paper](https://arxiv.org/abs/2211.12194) | [code](https://github.com/Winfredy/SadTalker)<br><br>

[1]Video Probabilistic Diffusion Models in Projected Latent Space(投影潜在空间中的视频概率扩散模型)<br>
[paper](https://arxiv.org/abs/2302.07685) | [project](https://sihyun.me/PVDM)<br><br>


<br>

<br>

<a name="VideoSR"/> 

### 视频超分(Video Super-Resolution)

<br>

<br>

<a name="Estimation"/> 

## 估计(Estimation)


<br>

<a name="Flow/Pose/MotionEstimation"/> 

### 光流/运动估计(Optical Flow/Motion Estimation)

<br>

<br>

<a name="DepthEstimation"/> 

### 深度估计(Depth Estimation)

[1] Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation<br>
[paper](https://arxiv.org/abs/2211.13202) | [code](https://github.com/noahzn/Lite-Mono)<br><br>

<br>


<br>

<a name="HumanPoseEstimation"/> 

### 人体解析/人体姿态估计(Human Parsing/Human Pose Estimation)

[5]TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation<br>
[paper](https://arxiv.org/abs/2212.12902)<br><br>

[4]Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting<br>
[paper](https://arxiv.org/abs/2303.05095) <br><br>

[3]PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation<br>
[paper](https://arxiv.org/abs/2303.07337)<br><br>

[2]DistilPose: Tokenized Pose Regression with Heatmap Distillation<br>
[paper](https://arxiv.org/abs/2303.02455)<br><br>

[1]Relightable Neural Human Assets from Multi-view Gradient Illuminations(来自多视图渐变照明的可照明神经人类资产)<br>
[paper](https://arxiv.org/abs/2212.07648)<br><br>

<br>

<br>

<a name="GestureEstimation"/> 

### 手势估计(Gesture Estimation)

[2]Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement<br>
[paper](https://arxiv.org/abs/2303.01765)<br><br>

[1]Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos<br>
[paper](https://arxiv.org/abs/2209.09484) | [code](https://github.com/fylwen/htt)<br><br>

<br>


<br>

<a name="ImageProcessing"/> 


## 图像处理(Image Processing)

<br>

<a name="SuperResolution"/> 

### 超分辨率(Super Resolution)

[6]Activating More Pixels in Image Super-Resolution Transformer<br>
[paper](https://arxiv.org/abs/2205.04437) | [code](https://github.com/chxy95/hat)<br><br>

[5]Super-Resolution Neural Operator<br>
[paper](https://arxiv.org/abs/2303.02584) | [code](https://github.com/2y7c3/super-resolution-neural-operator)<br><br>

[4]Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2303.05156)<br><br>

[3]Perception-Oriented Single Image Super-Resolution using Optimal Objective Estimation<br>
[paper](https://arxiv.org/abs/2211.13676) | [code](https://github.com/seungho-snu/SROOE)<br><br>

[2]N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2211.11436) | [code](https://github.com/rami0205/ngramswin)<br><br>

[1]Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild(野外鲁棒图像超分辨率的去噪扩散概率模型)<br>
[paper](https://arxiv.org/abs/2302.07864) | [project](https://sihyun.me/PVDM/)<br><br>

<br>

<br>

<a name="ImageRestoration"/>

###  图像复原/图像增强/图像重建(Image Restoration/Image Reconstruction)

[9]Masked Image Modeling with Local Multi-Scale Reconstruction<br>
[paper](https://arxiv.org/abs/2303.05251) | [code](https://github.com/huawei-noah/Efficient-Computing)<br><br>

[8]Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective<br>
[paper](https://arxiv.org/abs/2303.06859) | [code](https://github.com/lixinustc/Casual-IRDIL)<br><br>

[7]DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration<br>
[paper](https://arxiv.org/abs/2303.06885) <br><br>

[6]Robust Unsupervised StyleGAN Image Restoration<br>
[paper](https://arxiv.org/abs/2302.06733)<br><br>

[5]Raw Image Reconstruction with Learned Compact Metadata<br>
[paper](https://arxiv.org/abs/2302.12995)<br><br>

[4]Efficient and Explicit Modelling of Image Hierarchies for Image Restoration<br>
[paper](https://arxiv.org/abs/2303.00748) | [code](https://github.com/ofsoundof/GRL-Image-Restoration)<br><br>

[3]Imagic: Text-Based Real Image Editing with Diffusion Models<br>
[paper](https://arxiv.org/abs/2210.09276) | [project](https://imagic-editing.github.io/)<br><br>

[2]High-resolution image reconstruction with latent diffusion models from human brain activity<br>
[paper](https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2) | [project](https://sites.google.com/view/stablediffusion-with-brain/)<br><br>

[1]Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.10655)<br><br>

<br>

<br>


<a name="ISR"/> 

### 图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)

<br>



<a name="ImageDenoising"/> 

### 图像去噪/去模糊/去雨去雾(Image Denoising)

[4]Uncertainty-Aware Unsupervised Image Deblurring with Deep Residual Prior<br>
[paper](https://arxiv.org/abs/2210.05361)<br><br>

[3]Polarized Color Image Denoising using Pocoformer<br>
[paper](https://arxiv.org/abs/2207.00215)<br><br>

[2]Blur Interpolation Transformer for Real-World Motion from Blur<br>
[paper](https://arxiv.org/abs/2211.11423) | [code](https://github.com/zzh-tech/BiT)<br><br>

[1]Structured Kernel Estimation for Photon-Limited Deconvolution<br>
[paper](https://arxiv.org/abs/2303.03472) | [code](https://github.com/sanghviyashiitb/structured-kernel-cvpr23)<br><br>

<br>

<br>

<a name="ImageEdit"/> 

### 图像编辑/图像修复(Image Edit/Inpainting)

[5]CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing<br>
[paper](https://arxiv.org/abs/2303.05031)<br><br>

[4]SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model<br>
[paper](https://arxiv.org/abs/2212.05034)<br><br>

[3]Interactive Cartoonization with Controllable Perceptual Factors<br>
[paper](https://arxiv.org/abs/2212.09555)<br><br>

[2]Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint<br>
[paper](https://arxiv.org/abs/2211.11448) | [code](https://github.com/kumapowerliu/clcae)<br><br>

[1]LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data<br>
[paper](https://arxiv.org/abs/2208.14889) | [code](https://github.com/KU-CVLAB/LANIT)<br><br>

<br>

<br>

<a name="ImageTranslation"/> 

### 图像翻译(Image Translation)

<br>

<br>

<a name="IQA"/> 

### 图像质量评估(Image Quality Assessment)

[2]CR-FIQA: Face Image Quality Assessment by Learning Sample Relative Classifiability<br>
[paper](https://arxiv.org/abs/2112.06592) <br><br>

[1]Quality-aware Pre-trained Models for Blind Image Quality Assessment<br>
[paper](https://arxiv.org/abs/2303.00521)<br><br>

<br>

<a name="StyleTransfer"/> 

### 风格迁移(Style Transfer)

<br>

<br>

<a name="ImageRegistration"/> 

### 图像配准(Image Registration)

[1]Indescribable Multi-modal Spatial Evaluator<br>
[paper](https://arxiv.org/abs/2303.00369) | [code](https://github.com/Kid-Liet/IMSE/pulse)<br><br>

<br>

<br>

<a name="Face"/> 

## 人脸(Face)

<br>

<br>

<a name="FacialRecognition"/> 

### 人脸识别/检测(Facial Recognition/Detection)

<br>

<br>

<a name="FaceSynthesis"/> 

### 人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)

[2]A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images<br>
[paper](https://arxiv.org/abs/2302.14434)<br><br>

[1]MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation(MetaPortrait：具有快速个性化适应的身份保持谈话头像生成)<br>
[paper](https://arxiv.org/abs/2212.08062) | [code](https://github.com/Meta-Portrait/MetaPortrait)<br><br>

<br>

<br>

<a name="FaceAnti-Spoofing"/> 

### 人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)

[2]Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization<br>
[paper](https://arxiv.org/abs/2210.14457) | [code](https://github.com/megvii-research/caddm)<br><br>

[1]Physical-World Optical Adversarial Attacks on 3D Face Recognition<br>
[paper](https://arxiv.org/abs/2205.13412)<br><br>

<br>


<br>

<a name="ObjectTracking"/> 

## 目标跟踪(Object Tracking)

[4]Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking<br>
[paper](https://arxiv.org/abs/2203.14360v2) | [code](https://github.com/noahcao/OC_SORT)<br><br>

[3]Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation<br>
[paper](https://arxiv.org/abs/2302.14589) <br><br>

[2]Referring Multi-Object Tracking<br>
[paper](https://arxiv.org/abs/2303.03366) <br><br>

[1]Simple Cues Lead to a Strong Multi-Object Tracker<br>
[paper](https://arxiv.org/abs/2206.04656)<br><br>

<br>

<br>
<a name="ImageRetrieval"/> 

## 图像&视频检索/视频理解(Image&Video Retrieval/Video Understanding)

[2]VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval<br>
[paper](https://arxiv.org/abs/2211.12764) | [code](https://github.com/bighuang624/vop)<br><br>

[1]Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval<br>
[paper](https://arxiv.org/abs/2206.02082)<br><br>

<br>



<a name="ActionRecognition"/> 

### 行为识别/动作识别/检测/分割/定位(Action/Activity Recognition)

[4]Post-Processing Temporal Action Detection<br>
[paper](https://arxiv.org/abs/2211.14924)<br><br>

[3]TriDet: Temporal Action Detection with Relative Boundary Modeling<br>
[paper](https://arxiv.org/abs/2303.07347) | [code](https://github.com/sssste/TriDet)<br><br>

[2]Learning Discriminative Representations for Skeleton Based Action Recognition<br>
[paper](https://arxiv.org/abs/2303.03729) <br><br>

[1]Continuous Sign Language Recognition with Correlation Network<br>
[paper](https://arxiv.org/abs/2303.03202) | [code](https://github.com/hulianyuyy/CorrNet)<br><br>

<br>

<a name="Re-Identification"/> 

### 行人重识别/检测(Re-Identification/Detection)

[1]MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID<br>
[paper](https://arxiv.org/abs/2303.07065) | [code](https://github.com/vimar-gu/MSINet)<br><br>

<br>

<a name="VideoCaption"/> 

### 图像/视频字幕(Image/Video Caption)

[3]ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing<br>
[paper](https://arxiv.org/abs/2303.02437) | [code](https://github.com/joeyz0z/conzic)<br><br>

[2]Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training<br>
[paper](https://arxiv.org/abs/2303.00040) <br><br>

[1]Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning<br>
[paper](https://arxiv.org/abs/2302.14115) | [code](https://antoyang.github.io/vid2seq.html)<br><br>

<br>


<a name="MedicalImaging"/> 

## 医学影像(Medical Imaging)

[2]Deep Feature In-painting for Unsupervised Anomaly Detection in X-ray Images<br>
[paper](https://arxiv.org/pdf/2111.13495.pdf) | [code](https://github.com/tiangexiang/SQUID)<br><br>

[1]Label-Free Liver Tumor Segmentation<br>
[paper](https://arxiv.org/pdf/2210.14845.pdf) | [code](https://github.com/MrGiovanni/SyntheticTumors)<br><br>

<br>


<a name="TDR"/> 


## 文本检测/识别/理解(Text Detection/Recognition/Understanding)

[3]Unifying Vision, Text, and Layout for Universal Document Processing<br>
[paper](https://arxiv.org/abs/2212.02623)<br><br>

[2]Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling<br>
[paper](https://arxiv.org/abs/2303.06949) <br><br>

[1]DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting<br>
[paper](https://arxiv.org/pdf/2211.10772v3.pdf) | [code](https://github.com/ViTAE-Transformer/DeepSolo)<br><br>

<br>



<a name="RSI"/> 

## 遥感图像(Remote Sensing Image)

<br>


<a name="GAN"/> 

## GAN/生成式/对抗式(GAN/Generative/Adversarial)

[4]Improving GAN Training via Feature Space Shrinkage<br>
[paper](https://arxiv.org/abs/2303.01559) | [code](https://github.com/wentianzhang-ml/adaptivemix)<br><br>

[3]Adversarial Attack with Raindrops<br>
[paper](https://arxiv.org/abs/2302.14267) <br><br>

[2]T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations<br>
[paper](https://arxiv.org/abs/2301.06052) | [project](https://mael-zys.github.io/T2M-GPT/)<br><br>

[1]Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars<br>
[paper](https://arxiv.org/abs/2211.11208) | [project](https://mrtornado24.github.io/Next3D/)<br><br>


<br>

<a name="IGIS"/> 

## 图像生成/图像合成(Image Generation/Image Synthesis)

[11]SpaText: Spatio-Textual Representation for Controllable Image Generation<br>
[paper](https://arxiv.org/abs/2211.14305)<br><br>

[10]Unifying Layout Generation with a Decoupled Diffusion Model<br>
[paper](https://arxiv.org/abs/2303.05049)<br><br>

[9]Scaling up GANs for Text-to-Image Synthesis<br>
[paper](https://arxiv.org/abs/2303.05511)<br><br>

[8]Inversion-Based Style Transfer with Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.13203) | [code](https://github.com/zyxelsa/InST)<br><br>

[7]Perspective Fields for Single Image Camera Calibration<br>
[paper](https://arxiv.org/abs/2212.03239)<br><br>

[6]VGFlow: Visibility guided Flow Network for Human Reposing<br>
[paper](https://arxiv.org/abs/2211.08540)<br><br>

[5]DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation<br>
[paper](https://arxiv.org/abs/2208.12242) | [code](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/dreambooth)<br><br>

[4]Progressive Open Space Expansion for Open-Set Model Attribution<br>
[paper](https://arxiv.org/abs/2303.06877) | [code](https://github.com/tianyunyoung/pose)<br><br>

[3]Person Image Synthesis via Denoising Diffusion Model<br>
[paper](https://arxiv.org/abs/2211.12500) <br><br>

[2]Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models(使用预训练的 2D 扩散模型解决 3D 逆问题)<br>
[paper](https://arxiv.org/abs/2211.10655) <br><br>

[1]Parallel Diffusion Models of Operator and Image for Blind Inverse Problems(盲反问题算子和图像的并行扩散模型)<br>
[paper](https://arxiv.org/abs/2211.10656) <br><br>

<br>

<a name="3DVision"/> 

## 三维视觉(3D Vision)

[1]Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction<br>
[paper](https://arxiv.org/abs/2302.07817) | [code](https://github.com/wzzheng/tpvformer)<br><br>

<br>

<a name="3DPC"/> 

### 点云(Point Cloud)

[9]GraVoS: Voxel Selection for 3D Point-Cloud Detection<br>
[paper](https://arxiv.org/abs/2208.08780)<br><br>

[8]DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets<br>
[paper](https://arxiv.org/abs/2301.06051) | [code](https://github.com/haiyang-w/dsvt)<br><br>

[7]PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees<br>
[paper](https://arxiv.org/abs/2303.01959)<br><br>

[6]ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud Completion<br>
[paper](https://arxiv.org/abs/2303.01979) | [code](https://github.com/sangminhong/acl-spc_pytorch)<br><br>

[5]DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization<br>
[paper](https://arxiv.org/abs/2212.06331)<br><br>

[4]Frequency-Modulated Point Cloud Rendering with Easy Editing<br>
[paper](https://arxiv.org/abs/2303.07596v1)<br><br>

[3]Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss<br>
[paper](https://arxiv.org/abs/2301.05709)<br><br>

[2]ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer<br>
[paper](https://arxiv.org/abs/2302.14435) | [code](https://github.com/I2-Multimedia-Lab/ProxyFormer)<br><br>

[1]Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting<br>
[paper](https://arxiv.org/abs/2302.13130) | [code](https://github.com/tarashakhurana/4d-occ-forecasting)<br><br>

<br>


<a name="3DReconstruction"/> 

### 三维重建(3D Reconstruction)

[11]Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion<br>
[paper](https://arxiv.org/abs/2211.11674) | [code](https://github.com/google-research/nerf-from-image)<br><br>

[10]MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices<br>
[paper](https://arxiv.org/abs/2303.01932) | [code](https://github.com/ActiveVisionLab/MobileBrick)<br><br>

[9]Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly<br>
[paper](https://arxiv.org/abs/2303.01999)<br><br>

[8]NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction<br>
[paper](https://arxiv.org/abs/2303.02375)<br><br>

[7]HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling<br>
[paper](https://arxiv.org/abs/2303.02700)<br><br>

[6]MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision<br>
[paper](https://arxiv.org/abs/2303.03315)<br><br>

[4]Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness<br>
[paper](https://arxiv.org/abs/2303.00971) | [code](https://github.com/zhijieshen-bjtu/DOPNet)<br><br>

[3]Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes<br>
[paper](https://arxiv.org/abs/2302.14348) | [code](https://github.com/jyunlee/Im2Hands)<br><br>

[2]ECON: Explicit Clothed humans Obtained from Normals<br>
[paper](https://arxiv.org/abs/2212.07422) | [code](https://github.com/YuliangXiu/ECON)<br><br>

[1]Structured 3D Features for Reconstructing Relightable and Animatable Avatars<br>
[paper](https://arxiv.org/abs/2212.06820) | [project](https://enriccorona.github.io/s3f/)<br><br>

<br>

<a name="NeRF"/> 

### 场景重建/视图合成/新视角合成(Novel View Synthesis)

[12]Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image<br>
[paper](https://arxiv.org/abs/2211.13901)<br><br>

[11]Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision<br>
[paper](https://arxiv.org/abs/2303.03361)<br><br>

[10]Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2211.11505)<br><br>

[9]DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors<br>
[paper](https://arxiv.org/abs/2211.12046) | [code](https://github.com/dogyoonlee/dp-nerf)<br><br>

[8]SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2211.12254)<br><br>

[7]3D Video Loops from Asynchronous Input<br>
[paper](https://arxiv.org/abs/2303.05312) | [code](https://github.com/limacv/VideoLoop3D)<br><br>

[6]NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer<br>
[paper](https://arxiv.org/abs/2303.06919) | [code](https://t.co/uNiTd9ujCv)<br><br>

[5]NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation<br>
[paper](https://arxiv.org/abs/2212.14710) <br><br>

[4]Renderable Neural Radiance Map for Visual Navigation<br>
[paper](https://arxiv.org/abs/2303.00304)<br><br>

[3]Real-Time Neural Light Field on Mobile Devices<br>
[paper](https://arxiv.org/abs/2212.08057) | [project](https://snap-research.github.io/MobileR2L/)<br><br>

[2]Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures<br>
[paper](https://arxiv.org/abs/2211.07600) | [code](https://github.com/eladrich/latent-nerf)<br><br>

[1]NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior<br>
[paper](https://arxiv.org/abs/2212.07388) | [project](https://nope-nerf.active.vision/)<br><br>

<br>

<a name="ModelCompression"/> 

## 模型压缩(Model Compression)

[1]Neural Video Compression with Diverse Contexts	<br>
[paper](http://arxiv.org/abs/2302.14402) | [code](https://github.com/microsoft/dcvc)<br><br>

<br>

<a name="KnowledgeDistillation"/> 

### 知识蒸馏(Knowledge Distillation)

[3]Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2302.14290) <br><br>

[2]Generic-to-Specific Distillation of Masked Autoencoders<br>
[paper](https://arxiv.org/abs/2302.14771) | [code](https://github.com/pengzhiliang/G2SD)<br><br>

[1]CLIPPING: Distilling CLIP-based Models for Video-Language Understanding(CLIPPING：为视频语言理解提炼基于 CLIP 的模型)<br>
[paper](https://openreview.net/forum?id=aqIvCsRsYt) <br><br>

<br>

<a name="Pruning"/> 

### 剪枝(Pruning)

[1]DepGraph: Towards Any Structural Pruning<br>
[paper](https://arxiv.org/abs/2301.12900) | [code](https://github.com/VainF/Torch-Pruning)<br><br>

<br>

<a name="Quantization"/> 

### 量化(Quantization)

[2]Post-training Quantization on Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.15736) | [code](https://github.com/42shawn/ptq4dm)<br><br>

[1]Adaptive Data-Free Quantization<br>
[paper](https://arxiv.org/abs/2303.06869) | [code](https://github.com/hfutqian/AdaDFQ)<br><br>

<br>

<a name="NNS"/> 

## 神经网络结构设计(Neural Network Structure Design)


[1]Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks<br>
[paper](https://arxiv.org/abs/2303.03667) | [code](https://github.com/JierunChen/FasterNet)<br><br>

<br>

<a name="CNN"/> 

### CNN

[3]DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network<br>
[paper](https://arxiv.org/abs/2303.02165) | [code](https://github.com/alibaba/lightweight-neural-architecture-search)<br><br>

[2]Demystify Transformers & Convolutions in Modern Image Deep Networks<br>
[paper](https://arxiv.org/abs/2211.05781) | [code](https://github.com/OpenGVLab/STM-Evaluation)<br><br>

[1]InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions<br>
[paper](https://arxiv.org/abs/2211.05778) | [code](https://github.com/OpenGVLab/InternImage)<br><br>


<br>

<a name="Transformer"/> 

### Transformer

[6]Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves<br>
[paper](https://arxiv.org/abs/2303.01112)<br><br>

[5]Learning Imbalanced Data with Vision Transformers<br>
[paper](https://arxiv.org/abs/2212.02015) | [code](https://github.com/xuzhengzhuo/livt)<br><br>

[4]SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency<br>
[paper](https://arxiv.org/abs/2211.02006)<br><br>

[3]Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers<br>
[paper](https://arxiv.org/abs/2205.12551) | [code](https://github.com/yhlleo/mjp)<br><br>

[2]Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR<br>
[paper](https://arxiv.org/abs/2303.07335) | [code](https://github.com/IDEA-Research/Lite-DETR)<br><br>

[1]Integrally Pre-Trained Transformer Pyramid Networks<br>
[paper](https://arxiv.org/abs/2211.12735) | [code](https://github.com/sunsmarterjie/iTPN)<br><br>

<br>

<a name="GNN"/> 

### 图神经网络(GNN)

[1]From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm<br>
[paper](https://arxiv.org/abs/2211.11761)<br><br>


<br>

<a name="NAS"/> 

### 神经网络架构搜索(NAS)

[2]PA&DA: Jointly Sampling PAth and DAta for Consistent NAS<br>
[paper](https://arxiv.org/abs/2302.14772) | [code](https://github.com/ShunLu91/PA-DA)<br><br>

[1]Stitchable Neural Networks(可缝合神经网络)<br>
[paper](https://arxiv.org/abs/2302.06586) | [code](https://github.com/ziplab/SN-Net)<br><br>

<br>

<a name="MLP"/> 

### MLP


<br>

<a name="M AE"/> 

### MAE

[1]Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders
[paper](https://arxiv.org/abs/2212.06785) | [code](https://github.com/ZrrSkywalker/I2P-MAE)<br><br>



<br>

<a name="DataProcessing"/> 

## 数据处理(Data Processing)

<br>

<a name="DataAugmentation"/> 

### 数据增广(Data Augmentation)





<br>

<a name="BatchNormalization"/> 

### 归一化/正则化(Batch Normalization)

[1]Masked Images Are Counterfactual Samples for Robust Fine-tuning<br>
[paper](https://arxiv.org/abs/2303.03052)<br><br>

<br>

<a name="ImageClustering"/> 

### 图像聚类(Image Clustering)



<br>


<a name="ImageCompression"/> 

### 图像压缩(Image Compression)

[1]Context-Based Trit-Plane Coding for Progressive Image Compression<br>
[paper](https://arxiv.org/abs/2303.05715) | [code](https://github.com/seungminjeon-github/CTC)<br><br>

<br>

<a name="ModelTraining"/> 

## 模型训练/泛化(Model Training/Generalization)

[5]Practical Network Acceleration with Tiny Sets<br>
[paper]([arxiv.org](https://arxiv.org/abs/2202.07861)) | [code]([github.com](https://github.com/doctorkey/practise))<br><br>

[4]Towards Bridging the Performance Gaps of Joint Energy-based Models<br>
[paper](https://arxiv.org/abs/2209.07959v2) | [code](https://github.com/sndnyang/sadajem)<br><br>

[3]DropKey<br>
[paper](https://arxiv.org/abs/2208.02646) <br><br>

[2]Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization<br>
[paper](https://arxiv.org/abs/2303.03108)<br><br>

[1]DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks<br>
[paper](https://arxiv.org/abs/2302.14685)<br><br>

<br>

<a name="NoisyLabel"/> 

### 噪声标签(Noisy Label)

[2]Fine-Grained Classification with Noisy Labels<br>
[paper](https://arxiv.org/abs/2303.02404)<br><br>

[1]Combating noisy labels in object detection datasets<br>
[paper](https://arxiv.org/abs/2211.13993)<br><br>



<br>

<a name="Long-Tailed"/> 

### 长尾分布(Long-Tailed Distribution)




<br>

<a name="matching"/> 


## 图像特征提取与匹配(Image feature extraction and matching)

[1]Modality-Agnostic Debiasing for Single Domain Generalization<br>
[paper](https://arxiv.org/abs/2303.07123) <br><br>

<br>

<a name="VisualRL"/> 

## 视觉表征学习(Visual Representation Learning)

[7]Hierarchical discriminative learning improves visual representations of biomedical microscopy<br>
[paper](https://arxiv.org/abs/2303.01605)<br><br>

[6]Fine-tuned CLIP Models are Efficient Video Learners<br>
[paper](https://arxiv.org/abs/2212.03640) | [code](https://github.com/muzairkhattak/vifi-clip)<br><br>

[5]Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning<br>
[paper](https://arxiv.org/abs/2212.04500) | [code](https://github.com/ruiwang2021/mvd)<br><br>

[4]Open-Set Representation Learning through Combinatorial Embedding<br>
[paper](https://arxiv.org/abs/2106.15278	)<br><br>

[3]NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction<br>
[paper](https://arxiv.org/abs/2211.08024)<br><br>

[2]Stare at What You See: Masked Image Modeling without Reconstruction<br>
[paper](https://arxiv.org/abs/2211.08887) | [code](https://github.com/openperceptionx/maskalign)<br><br>

[1]Switchable Representation Learning Framework with Self-compatibility<br>
[paper](https://arxiv.org/abs/2206.08289)<br><br>

<br>

<a name="ModelEvaluation"/> 

## 模型评估(Model Evaluation)



<br>

<a name="MMLearning"/> 

## 多模态学习(Multi-Modal Learning)

[4]Multimodal Prompting with Missing Modalities for Visual Recognition<br>
[paper](https://arxiv.org/abs/2303.03369) | [code](https://github.com/yilunlee/missing_aware_prompts)<br><br>

[3]Align and Attend: Multimodal Summarization with Dual Contrastive Losses<br>
[paper](https://arxiv.org/abs/2303.07284) | [code](https://boheumd.github.io/A2Summ/)<br><br>

[2]Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information(通过最大化多模态互信息实现一体化预训练)<br>
[paper](https://arxiv.org/abs/2211.09807) | [code](https://github.com/OpenGVLab/M3I-Pretraining)<br><br>

[1]Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks(Uni-Perceiver v2：用于大规模视觉和视觉语言任务的通才模型)<br>
[paper](https://arxiv.org/abs/2211.09808) | [code](https://github.com/fundamentalvision/Uni-Perceiver)<br><br>


<br>

<a name="Audio-VisualLearning"/> 

### 视听学习(Audio-visual Learning)

[1]A Light Weight Model for Active Speaker Detection<br>
[paper](https://arxiv.org/abs/2303.04439) | [code](https://github.com/junhua-liao/light-asd)<br><br>


<br>

<a name="VLRL"/> 

### 视觉-语言（Vision-language）

[9]FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks<br>
[paper](https://arxiv.org/abs/2303.02483) | [code](https://github.com/brandonhanx/fame-vil)<br><br>

[8]Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding<br>
[paper](https://arxiv.org/abs/2303.04077)<br><br>

[7]Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing<br>
[paper](https://arxiv.org/abs/2301.04558)<br><br>

[6]Connecting Vision and Language with Video Localized Narratives<br>
[paper](https://arxiv.org/abs/2302.11217) | [code](https://github.com/google/video-localized-narratives)<br><br>

[5]Policy Adaptation from Foundation Model Feedback<br>
[paper](https://arxiv.org/abs/2212.07398)<br><br>

[4]Open-vocabulary Attribute Detection<br>
[paper](https://arxiv.org/abs/2211.12914)<br><br>

[3]Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training<br>
[paper](https://arxiv.org/abs/2303.00040) <br><br>

[2]Turning a CLIP Model into a Scene Text Detector<br>
[paper](https://arxiv.org/abs/2302.14338) | [code](https://github.com/wenwenyu/TCM)<br><br>

[1]GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods<br>
[paper](https://arxiv.org/abs/2301.01893) <br><br>


<br>
<a name="Vision-basedPrediction"/> 

## 视觉预测(Vision-based Prediction)

[3]Intention-Conditioned Long-Term Human Egocentric Action Forecasting<br>
[paper](https://arxiv.org/abs/2207.12080) | [code](https://github.com/Evm7/ego4dlta-icvae)<br><br>

[2]Computational Choreography using Human Motion Synthesis<br>
[paper](https://arxiv.org/abs/2210.04366) <br><br>

[1]IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction<br>
[paper](https://arxiv.org/abs/2303.00575) <br><br>



<br>
<a name="Dataset"/> 

## 数据集(Dataset)

[4]Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo<br>
[paper](https://arxiv.org/abs/2303.01943)<br><br>

[3]CUDA: Convolution-based Unlearnable Datasets<br>
[paper](https://arxiv.org/abs/2303.04278)<br><br>

[2]V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception<br>
[paper](http://arxiv.org/abs/2303.07601)<br><br>

[1]Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes<br>
[paper](https://arxiv.org/abs/2303.02760)<br><br>



<br>

<a name="ActiveLearning"/> 

## 主动学习(Active Learning)



<br>

<a name="Few-shotLearning"/> 

## 小样本学习/零样本学习(Few-shot Learning/Zero-shot Learning)

[4]Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation<br>
[paper](https://arxiv.org/abs/2303.01311)<br><br>

[3]Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners<br>
[paper](https://arxiv.org/abs/2303.02151) | [code](https://github.com/zrrskywalker/cafo)<br><br>

[2]NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging<br>
[paper](https://arxiv.org/abs/2303.04958)<br><br>

[1]FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization<br>
[paper](http://arxiv.org/abs/2303.07418) | [code](https://github.com/jiawei-yang/freenerf)<br><br>

<br>

<a name="ContinualLearning"/> 

## 持续学习(Continual Learning/Life-long Learning)



<br>

<a name="SG"/> 

## 场景图(Scene Graph)

[1]Probabilistic Debiasing of Scene Graphs<br>
[paper](https://arxiv.org/abs/2211.06444) | [code](https://github.com/bashirulazam/within-triplet-debias)<br><br>

<br>

<a name="SGG"/> 

### 场景图生成(Scene Graph Generation)

[1]Prototype-based Embedding Network for Scene Graph Generation<br>
[paper](https://paperswithcode.com/paper/prototype-based-embedding-network-for-scene) <br><br>

<br>

<a name="SGP"/> 

### 场景图预测(Scene Graph Prediction)

<br>

<a name="SGU"/> 

### 场景图理解(Scene Graph Understanding)

<br>

<a name="VisualLocalization"/> 

## 视觉定位/位姿估计(Visual Localization/Pose Estimation)

[1]PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow<br>
[paper](https://arxiv.org/abs/2303.02595) <br><br>



<br>

<a name="VisualReasoning"/> 

## 视觉推理/视觉问答(Visual Reasoning/VQA)

[3]MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering<br>
[paper](https://arxiv.org/abs/2303.01239) | [code](https://github.com/jingjing12110/mixphm)<br><br>

[2]Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering<br>
[paper](https://arxiv.org/abs/2303.01903) | [code](https://github.com/milvlg/prophet)<br><br>

[1]From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models<br>
[paper](https://arxiv.org/abs/2212.10846) | [code](https://github.com/salesforce/lavis)<br><br>

<br>

<a name="ImageClassification"/> 

## 图像分类(Image Classification)

[1]I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification(I2MVFormer：用于零样本图像分类的大型语言模型生成的多视图文档监督)<br>
[paper](https://arxiv.org/abs/2212.02291)<br><br>

<br>

<a name="domain"/> 

## 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)

[2]Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation<br>
[paper](https://arxiv.org/abs/2303.03770) | [code](https://github.com/mattialitrico/guiding-pseudo-labels-with-uncertainty-estimation-for-source-free-unsupervised-domain-adaptation)<br><br>

[1]Adaptive Assignment for Geometry Aware Local Feature Matching<br>
[paper](https://arxiv.org/abs/2207.08427)<br><br>

<br>

<a name="MetricLearning"/> 

## 度量学习(Metric Learning)



<br>

<a name="ContrastiveLearning"/> 

## 对比学习(Contrastive Learning)

[1]Twin Contrastive Learning with Noisy Labels<br>
[paper](https://arxiv.org/abs/2303.06930) | [code](https://github.com/Hzzone/TCL)<br><br>

<br>

<a name="IncrementalLearning"/> 

## 增量学习(Incremental Learning)



<br>

<a name="RL"/> 

## 强化学习(Reinforcement Learning)



<br>

<a name="MetaLearning"/> 

## 元学习(Meta Learning)

[1]A Meta-Learning Approach to Predicting Performance and Data Requirements<br>
[paper](https://arxiv.org/abs/2303.01598)<br><br>

<br>

<a name="Robotic"/> 

## 机器人(Robotic)

[1]PyPose: A Library for Robot Learning with Physics-based Optimization(PyPose：基于物理优化的机器人学习库)<br>
[paper](https://arxiv.org/abs/2209.15428) | [code](https://pypose.org/)<br><br>


<br>

<a name="self-supervisedlearning"/> 

## 半监督学习/弱监督学习/无监督学习/自监督学习(Self-supervised Learning/Semi-supervised Learning)

[7]Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems<br>
[paper](https://arxiv.org/abs/2303.01669) | [code](https://github.com/ganperf/lcr)<br><br>

[6]Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models<br>
[paper](https://arxiv.org/abs/2303.01869)<br><br>

[5]The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training<br>
[paper]([arxiv.org](https://arxiv.org/abs/2205.12502)) | [code](https://github.com/gicheonkang/gst-visdial)<br><br>

[4]Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2303.06870) | [code](https://github.com/megvii-research/US3L-CVPR2023)<br><br>

[3]Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors<br>
[paper](https://arxiv.org/abs/2302.14746) <br><br>

[2]Siamese Image Modeling for Self-Supervised Vision Representation Learning<br>
[paper](https://arxiv.org/abs/2206.01204) | [code](https://github.com/fundamentalvision/Siamese-Image-Modeling)<br><br>

[1]Cut and Learn for Unsupervised Object Detection and Instance Segmentation<br>
[paper](https://arxiv.org/abs/2301.11320) | [project](http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/)<br><br>

<br>

<a name="interpretability"/> 

## 神经网络可解释性(Neural Network Interpretability)

[2]Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis<br>
[paper](https://arxiv.org/abs/2202.07728)<br><br>

[1]SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries(SplineCam：深度网络几何和决策边界的精确可视化和表征)<br>
[paper](https://arxiv.org/abs/2302.12828) | [code](https://github.com/AhmedImtiazPrio/SplineCAM)<br><br>

<br>

<a name="CrowdCounting"/> 


## 图像计数(Image Counting)

[1]Zero-shot Object Counting<br>
[paper](https://arxiv.org/abs/2303.02001)<br><br>

<br>

<a name="federatedlearning"/> 


## 联邦学习(Federated Learning)


<br>

<a name="automatic driving"/> 


## 自动驾驶(automatic driving)


[1]BEVFormer v2: Adapting Modern Image Backbones to Bird’s-Eye-View Recognition via Perspective Supervision(BEVFormer v2：通过透视监督使现代图像主干适应鸟瞰图识别)<br>
[paper](https://arxiv.org/abs/2211.10439)<br><br>

<br>

<a name="100"/> 

## 其他

[22]ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges<br>
[paper](https://arxiv.org/abs/2303.01498)<br><br>

[21]Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.01788)<br><br>

[20]UniHCP: A Unified Model for Human-Centric Perceptions<br>
[paper](https://arxiv.org/abs/2303.02936) | [code](https://github.com/opengvlab/unihcp)<br><br>

[19]Where We Are and What We're Looking At: Query Based Worldwide Image Geo-localization Using Hierarchies and Scenes<br>
[paper](https://arxiv.org/abs/2303.04249)<br><br>

[18]Revisiting Rotation Averaging: Uncertainties and Robust Losses<br>
[paper](https://arxiv.org/abs/2303.05195) | [code](https://github.com/zhangganlin/globalsfmpy)<br><br>

[17]3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification<br>
[paper](https://arxiv.org/abs/2212.00338)<br><br>

[16]Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection<br>
[paper](https://arxiv.org/abs/2211.06368) | [code](https://github.com/open-mmlab/mmrotate)<br><br>

[15]Understanding and Improving Visual Prompting: A Label-Mapping Perspective<br>
[paper](https://arxiv.org/abs/2211.11635) | [code](https://github.com/optml-group/ilm-vp)<br><br>

[14]vMAP: Vectorised Object Mapping for Neural Field SLAM<br>
[paper](http://arxiv.org/abs/2302.01838) | [code](https://github.com/kxhit/vMAP)<br><br>

[13]EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization<br>
[paper](https://arxiv.org/abs/2303.01904) <br><br>

[12]Upcycling Models under Domain and Category Shift<br>
[paper](https://arxiv.org/abs/2303.07110) | [code](https://github.com/ispc-lab/GLC)<br><br>

[11]Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images<br>
[paper](https://arxiv.org/abs/2303.06873) | [code](https://github.com/HHHedo/IBMIL)<br><br>

[10]Dynamic Neural Network for Multi-Task Learning Searching across Diverse Network Topologies<br>
[paper](https://arxiv.org/abs/2303.06856)<br><br>

[9]Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples<br>
[paper](https://arxiv.org/abs/2301.01217) | [code](https://github.com/jiamingzhang94/Unlearnable-Clusters)<br><br>

[8]Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation<br>
[paper](https://arxiv.org/abs/2303.00914) <br><br>

[7]Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation<br>
[paper](https://arxiv.org/abs/2303.00914) <br><br>

[6]Physical-World Optical Adversarial Attacks on 3D Face Recognition<br>
[paper](https://arxiv.org/abs/2205.13412) <br><br>

[5]Improving Cross-Modal Retrieval with Set of Diverse Embeddings<br>
[paper](https://arxiv.org/abs/2211.16761) <br><br>

[4]Neural Video Compression with Diverse Contexts<br>
[paper](https://arxiv.org/abs/2302.14402) | [code](https://github.com/microsoft/DCVC)<br><br>

[3]Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger<br>
[paper](https://arxiv.org/abs/2302.14677) <br><br>

[2]Single Image Backdoor Inversion via Robust Smoothed Classifiers<br>
[paper](https://arxiv.org/abs/2303.00215) | [code](https://github.com/locuslab/smoothinv)<br><br>

[1]Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision<br>
[paper](https://arxiv.org/abs/2303.00462) | [code](https://github.com/Toytiny/CMFlow)<br><br>

<br>

<br>

<a name="2"/> 



# 3. CVPR2023 论文解读汇总

1.[CVPR2023｜打破对MIM（掩码图像建模）的数据缩放能力的误解！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638753&idx=2&sn=755ecd0052718db6c02f49f50e5934ef&chksm=ec123318db65ba0edc63d9a3c479ee1763300fd612761458a11f993dcb11517671041f141b88&token=1182786253&lang=zh_CN#rd))<br><br>

2.[CVPR 2023｜基于CLIP的微调新范式！训练速度和性能均创新高！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638629&idx=2&sn=4f24bbaf13968e9d0cabf4a984bd7bf4&chksm=ec12339cdb65ba8a45bd3fe6deb6aae9b5e9b53e7ac7683f462412a114e673e420b04effb99e&token=1182786253&lang=zh_CN#rd))<br><br>

3.[CVPR 2023｜浙大提出全归一化流模型PyramidFlow：高分辨率缺陷异常定位新范式]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638363&idx=1&sn=c1159fc20788f8087fc513d450d33a3f&chksm=ec1230a2db65b9b46a5b1a826d4f3d3743d1894394fe8c010c40773bd5a14ccf6abeaf80bdd3&token=1182786253&lang=zh_CN#rd))<br><br>

4.[CVPR 2023｜大脑视觉信号被Stable Diffusion复现图像！“人类的谋略和谎言不存在了”]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638334&idx=1&sn=e1cfe285aa51210de985ca3c82763d50&chksm=ec1230c7db65b9d10420dc3d332b9ec46a69922b7771b29238c2953ae018d7bca301bf6a8986&token=1182786253&lang=zh_CN#rd))<br><br>

5.[CVPR 2023｜港科大 DA-BEV: 3D目标检测新 SOTA，一种强大的深度信息挖掘方法]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638334&idx=2&sn=ceb12a2dcad0df502d72ab7c211e199f&chksm=ec1230c7db65b9d1e0e1fb43cf1d6c87c0adcabbac7fef1ed31277ee133db696722103709ba5&token=1182786253&lang=zh_CN#rd))<br><br>

6.[CVPR 23｜表征学习超MAE，谷歌等提出MAGE：无监督图像生成超越 Latent Diffusion]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638272&idx=1&sn=454898151d1489a6b668302f8087dd20&chksm=ec1230f9db65b9ef5535a6ecd6b683768dcadca752c074c6ea2d1768106600d3c6b19d07d2f3&token=1182786253&lang=zh_CN#rd))<br><br>

7.[CVPR2023｜不好意思我要加速度了！FasterNet：更高FLOPS才是更快更强的底气]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638140&idx=1&sn=ef59001e77950b1333111f6686a2bf71&chksm=ec123185db65b89334d21dbd993240bbe15d7d5f01460d57e937dfdc6e4032dadfb90a15586a&token=1182786253&lang=zh_CN#rd))<br><br>

8.[CVPR 2023｜大模型流行之下，SN-Net给出一份独特的答卷]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638044&idx=1&sn=c0c8117a6ab456e7344479b2f9452f9d&chksm=ec1231e5db65b8f375fc2f21cac46f23db6431b66c42a6d405fe7a63aab1e14655c0f56f55a6&token=1182786253&lang=zh_CN#rd))<br><br>

9.[CVPR 2023｜结合特征金字塔结构的自监督学习 iTPNs]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638044&idx=3&sn=9ff7c2f74531c958c100d5aa4deeb89d&chksm=ec1231e5db65b8f360c97314e545cad2e98a706a3bf78dcc99f56fc677d5ce45aa2f41805d22&token=1182786253&lang=zh_CN#rd))<br><br>

10.[CVPR 2023｜SQR：对于训练DETR-family目标检测的探索和思考]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637856&idx=1&sn=e72e7c78a5efb8be20880ae6351d8595&chksm=ec123699db65bf8f964bece49f549d6879f92c60976b4ab28ce5d4e63dc89b865fe36b8be62b&token=1182786253&lang=zh_CN#rd))<br><br>

11.[CVPR 2023｜COCO新纪录65.4mAP！InternImage：注入新机制，扩展DCNv3，探索视觉大模型]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637856&idx=2&sn=9ee6fbd7e4d7126a12692d9b0827a4f2&chksm=ec123699db65bf8f4519f6b48fc663dfafc9f6b9b42312cd0ae700db66888598524b36283b24&token=1182786253&lang=zh_CN#rd))<br><br>

12.[CVPR 2023｜YOLOv7强势收录！时隔6年，YOLOv系列再登CVPR！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637752&idx=1&sn=44f42365e781ee941a3470ea875d0f13&chksm=ec123701db65be176c3a6d783030e2565613ef3199753001df509e035a6cd813b4a57612b9af&token=1182786253&lang=zh_CN#rd))<br><br>

13.[CVPR 2023｜谷歌提出Imagic：扩散模型只用文字就能PS照片了！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637752&idx=3&sn=f9d479791e4b392b0790e5b5a0a5e5ef&chksm=ec123701db65be178f96580cfa74f58ff965dbcdbf274aa947c7b717c67e0b416c47322673a0&token=1182786253&lang=zh_CN#rd))<br><br>

14.[CVPR 2023｜Lite DETR：计算量减少60%！高效交错多尺度编码器]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639326&idx=3&sn=dd995e387abc37ede3794b39f92cb59d&chksm=ec124ce7db65c5f132c34a3ebfd0038b1532ab0418444dc46766c82724599c53b220bb0b6428&token=693221699&lang=zh_CN#rd))<br><br>

15.[CVPR 2023｜白翔团队新作：借助CLIP完成场景文字检测]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639207&idx=2&sn=90efb1cf701cf8fe09e1ba8711430a2a&chksm=ec124d5edb65c448da904439aa89c8f78d9f2acc6f03df94bdb983574dad58597dc0ca73345f&token=693221699&lang=zh_CN#rd))<br><br>

16.[CVPR'23｜即插即用系列！一种轻量高效的自注意力机制助力图像恢复网络问鼎 SOTA]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639153&idx=1&sn=cab1c897df12dd95d32f9fbb041fb727&chksm=ec124d88db65c49ef92f615aa0d06d735629cb80726ed57b98da38ebcf957b2d5d0b8e315c52&token=693221699&lang=zh_CN#rd))<br><br>

17.[CVPR 2023｜英伟达提出VoxFromer: 单目3D语义场景补全新SOTA]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639124&idx=2&sn=cfcffd7905063c14ccf74b1c952f8fe4&chksm=ec124daddb65c4bb5bad6bddb11b023716b5da38125af15531560ab96d0568f79a131112a95c&token=693221699&lang=zh_CN#rd))<br><br>

18.[CVPR 2023｜EMA-VFI: 基于帧间注意力提取运动和外观信息的高效视频插帧]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638921&idx=2&sn=0bb7f63c6cf4e58893e0d118fb34ece6&chksm=ec123270db65bb6645e82632228a959f67d776dd7c3ac6d7082e313b81b2588787d40625bf92&token=693221699&lang=zh_CN#rd))<br><br>



<br>



<br>

<a name="4"/> 

# 4. CVPR2023论文分享

[极市直播回放第108期丨 潘梓正：模型部署新范式—可缝合神经网络（CVPR 2023）]([url](https://www.cvmart.net/community/detail/7439))

<br>

<br>

<a name="5"/> 

# 5. To do list

* CVPR2023 Workshop
