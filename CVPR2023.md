* 推荐阅读：<br>
  * [ICCV2021/2019/2017 论文/代码/解读/直播合集](https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation)
  * [2020-2021年计算机视觉综述论文汇总](https://github.com/extreme-assistant/survey-computer-vision)
  * [国内外优秀的计算机视觉团队汇总](https://github.com/extreme-assistant/Awesome-CV-Team)

------

# CVPR2023最新信息及论文下载（Papers/Codes/Project/PaperReading／Demos/直播分享／论文分享会等）

官网链接：https://cvpr.thecvf.com/Conferences/2023<br>
论文接收公布时间：2023年2月28日<br>

相关问题：[如何评价 CVPR 2023 的论文接收结果？](https://www.zhihu.com/question/585474435)<br>
相关报道：[CVPR 2023 接收结果出炉！录用2360篇，接收数量上升12%](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637344&idx=1&sn=62f70870c3f76e6b5a401373986a6ee5&chksm=ec123499db65bd8ffc8987ec1f61b1389e666ed32772149e7c7e76923e2f427ef57e55b57d72&token=1890936456&lang=zh_CN#rd)


>update: （更新附打包下载链接）<br>
>2023/2/28 [更新13篇](https://www.cvmart.net/community/detail/7212)<br>
>2023/3/02 [更新54篇](https://www.cvmart.net/community/detail/7388)<br>
>2023/3/09 [更新35篇](https://www.cvmart.net/community/detail/7403)<br>
>2023/3/15 [更新29篇](https://www.cvmart.net/community/detail/7419)<br>
>2023/3/16 [更新8篇](https://www.cvmart.net/community/detail/7421)<br>
>2023/3/17 [更新19篇](https://www.cvmart.net/community/detail/7428)<br>
>2023/3/20 [更新37篇](https://www.cvmart.net/community/detail/7435)<br>
>2023/3/22 [更新61篇](https://www.cvmart.net/community/detail/7444)<br>
>2023/3/23 [更新55篇](https://www.cvmart.net/community/detail/7449)<br>
>2023/3/24 [更新70篇](https://www.cvmart.net/community/detail/7454)<br>
>2023/3/25 [更新99篇](https://www.cvmart.net/community/detail/7464)<br>
>2023/3/26 [更新23篇](https://www.cvmart.net/community/detail/7465)<br>
>2023/3/29 [更新101篇](https://www.cvmart.net/community/detail/7470)<br>
>2023/3/31 [更新89篇](https://www.cvmart.net/community/detail/7480)<br>
>2023/4/11 [更新127篇](https://www.cvmart.net/community/detail/7509)<br>
>2023/4/12 [更新48篇](https://www.cvmart.net/community/detail/7514)<br>
>2023/4/13 [更新51篇](https://www.cvmart.net/community/detail/7520)<br>

<br><br>

# 目录

[1. CVPR2023 接受论文/代码分方向汇总（更新中）](#1)<br>
[2. CVPR2023 spotlight（更新中）](#2)<br>
[3. CVPR2023 论文解读汇总（更新中）](#3)<br>
[4. CVPR2023 极市论文分享](#4)<br>
[5. To do list](#5)<br>

<br>

<a name="1"/> 

# 1.CVPR2023接受论文/代码分方向整理(持续更新)


## 分类目录：

### [1. 检测](#detection)

* [2D目标检测(2D Object Detection)](#IOD)
* [视频目标检测(Video Object Detection)](#VOD)
* [3D目标检测(3D Object Detection)](#3DOD)
* [人物交互检测(HOI Detection)](#HOI)
* [伪装目标检测(Camouflaged Object Detection)](#COD)
* [旋转目标检测(Rotation Object Detection)](#ROD)
* [显著性目标检测(Saliency Object Detection)](#SOD)
* [关键点检测(Keypoint Detection)](#KeypointDetection)
* [车道线检测(Lane Detection)](#LaneDetection)
* [边缘检测(Edge Detection)](#EdgeDetection)
* [消失点检测(Vanishing Point Detection)](#VPD)
* [异常检测(Anomaly Detection)](#AnomalyDetection)

### [2. 分割(Segmentation)](#Segmentation)

* [图像分割(Image Segmentation)](#ImageSegmentation)
* [全景分割(Panoptic Segmentation)](#PanopticSegmentation)
* [语义分割(Semantic Segmentation)](#SemanticSegmentation)
* [实例分割(Instance Segmentation)](#InstanceSegmentation)
* [超像素(Superpixel)](#Superpixel)
* [视频目标分割(Video Object Segmentation)](#VOS)
* [抠图(Matting)](#Matting)
* [密集预测(Dense Prediction)](#DensePrediction)

### [3. 图像处理(Image Processing)](#ImageProcessing)

* [超分辨率(Super Resolution)](#SuperResolution)
* [图像复原/图像增强/图像重建(Image Restoration/Image Reconstruction)](#ImageRestoration)
* [图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)](#ISR)
* [图像去噪/去模糊/去雨去雾(Image Denoising)](#ImageDenoising)
* [图像编辑/图像修复(Image Edit/Image Inpainting)](#ImageEdit)
* [图像翻译(Image Translation)](#ImageTranslation)
* [图像质量评估(Image Quality Assessment)](#IQA)
* [风格迁移(Style Transfer)](#StyleTransfer)
* [图像配准(Image Registration)](#ImageRegistration)

### [4. 视频处理(Video Processing)](#VideoProcessing)

* [视频编辑(Video Editing)](#VideoEditing)
* [视频生成/视频合成(Video Generation/Video Synthesis)](#VideoGeneration)
* [视频超分(Video Super-Resolution)](#VideoSR)

### [5. 估计(Estimation)](#Estimation)

* [光流/运动估计(Flow/Motion Estimation)](#Flow/Pose/MotionEstimation)
* [深度估计(Depth Estimation)](#DepthEstimation)
* [人体解析/人体姿态估计(Human Parsing/Human Pose Estimation)](#HumanPoseEstimation)
* [手势估计(Gesture Estimation)](#GestureEstimation)

### [6. 图像&视频检索/(Image&Video Retrieval/Video Understanding)](#ImageRetrieval)

* [行为识别/行为识别/动作识别/检测/分割(Action/Activity Recognition)](#ActionRecognition)
* [行人重识别/检测(Re-Identification/Detection)](#Re-Identification)
* [图像/视频字幕(Image/Video Caption)](#VideoCaption)

### [7. 人脸(Face)](#Face)

* [人脸识别/检测(Facial Recognition/Detection)](#FacialRecognition)
* [人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)](#FaceSynthesis)
* [人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)](#FaceAnti-Spoofing)

### [8. 三维视觉(3D Vision)](#3DVision)

* [点云(Point Cloud)](#3DPC)
* [三维重建(3D Reconstruction)](#3DReconstruction)
* [场景重建/视图合成/新视角合成(Novel View Synthesis)](#NeRF)

### [9. 目标跟踪(Object Tracking)](#ObjectTracking)

### [10. 医学影像(Medical Imaging)](#MedicalImaging)

### [11. 文本检测/识别/理解(Text Detection/Recognition/Understanding)](#TDR)

### [12. 遥感图像(Remote Sensing Image)](#RSI)

### [13. GAN/生成式/对抗式(GAN/Generative/Adversarial)](#GAN)

### [14. 图像生成/图像合成(Image Generation/Image Synthesis)](#IGIS)

### [15. 场景图(Scene Graph](#SG)

* [场景图生成(Scene Graph Generation)](#SGG)
* [场景图预测(Scene Graph Prediction)](#SGP)
* [场景图理解(Scene Graph Understanding)](#SGU)

### [16. 视觉定位/位姿估计(Visual Localization/Pose Estimation)](#VisualLocalization)

### [17. 视觉推理/视觉问答(Visual Reasoning/VQA)](#VisualReasoning)

### [18. 视觉预测(Vision-based Prediction)](#Vision-basedPrediction)

### [19. 神经网络结构设计(Neural Network Structure Design)](#NNS)

* [CNN](#CNN)
* [Transformer](#Transformer)
* [图神经网络(GNN)](#GNN)
* [神经网络架构搜索(NAS)](#NAS)
* [MLP](#MLP)

### [20. 神经网络可解释性(Neural Network Interpretability)](#interpretability)

### [21. 数据集(Dataset)](#Dataset)

### [22. 数据处理(Data Processing)](#DataProcessing)

* [数据增广(Data Augmentation)](#DataAugmentation)
* [归一化/正则化(Batch Normalization)](#BatchNormalization)
* [图像聚类(Image Clustering)](#ImageClustering)
* [图像压缩(Image Compression)](#ImageCompression)

### [23. 图像特征提取与匹配(Image feature extraction and matching)](#matching)

### [24. 视觉表征学习(Visual Representation Learning)](#VisualRL)

### [25. 模型训练/泛化(Model Training/Generalization)](#ModelTraining)

* [噪声标签(Noisy Label)](#NoisyLabel)
* [长尾分布(Long-Tailed Distribution)](#Long-Tailed)

### [26. 模型压缩(Model Compression)](#ModelCompression)

* [知识蒸馏(Knowledge Distillation)](#KnowledgeDistillation)
* [剪枝(Pruning)](#Pruning)
* [量化(Quantization)](#Quantization)

### [27. 模型评估(Model Evaluation)](#ModelEvaluation)

### [28. 图像分类(Image Classification)](#ImageClassification)

### [29. 图像计数(Image Counting)](#CrowdCounting)

### [30. 机器人(Robotic)](#Robotic)

### [31. 半监督学习/弱监督学习/无监督学习/自监督学习(Self-supervised Learning/Semi-supervised Learning)](#self-supervisedlearning)

### [32. 多模态学习(Multi-Modal Learning)](#MMLearning)

* [视听学习(Audio-visual Learning)](#Audio-VisualLearning)
* [视觉-语言（Vision-language）](#VLRL)

### [33. 主动学习(Active Learning)](#ActiveLearning)

### [34. 小样本学习/零样本学习(Few-shot/Zero-shot Learning)](#Few-shotLearning)

### [35. 持续学习(Continual Learning/Life-long Learning)](#ContinualLearning)

### [36. 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)](#domain)

### [37. 度量学习(Metric Learning)](#MetricLearning)

### [38. 对比学习(Contrastive Learning)](#ContrastiveLearning)

### [39. 增量学习(Incremental Learning)](#IncrementalLearning)

### [40. 强化学习(Reinforcement Learning)](#RL)

### [41. 元学习(Meta Learning)](#MetaLearning)

### [42. 联邦学习(Federated Learning](#federatedlearning)

### [43. 自动驾驶(Federated Learning](#automatic driving)




### [其他](#100)



<br><br>

<a name="detection"/> 

## 检测



<br>

<a name="IOD"/> 

### 2D目标检测(2D Object Detection)

[14]DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment<br>
[paper](https://arxiv.org/abs/2304.04514)<br><br>

[13]Benchmarking the Physical-world Adversarial Robustness of Vehicle Detection<br>
[paper](https://arxiv.org/abs/2304.05098)<br><br>

[12]Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision<br>
[paper](https://arxiv.org/abs/2304.01484) | [code](https://github.com/xinyiying/lesps)<br><br>

[11]Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains<br>
[paper](https://arxiv.org/abs/2304.02950)<br><br>

[10]Continual Detection Transformer for Incremental Object Detection<br>
[paper](https://arxiv.org/abs/2304.03110)<br><br>

[9]Object Discovery from Motion-Guided Tokens<br>
[paper](https://arxiv.org/abs/2303.15555) | [code](https://github.com/zpbao/motok)<br><br>

[8]What Can Human Sketches Do for Object Detection?<br>
[paper](https://arxiv.org/abs/2303.15149)<br><br>

[7]NeRF-RPN: A general framework for object detection in NeRFs<br>
[paper](https://arxiv.org/abs/2211.11646)<br><br>

[6]Detecting Everything in the Open World: Towards Universal Object Detection<br>
[paper](https://arxiv.org/abs/2303.11749)<br><br>

[5]Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection<br>
[paper](https://arxiv.org/abs/2303.05892)<br><br>

[4]CapDet: Unifying Dense Captioning and Open-World Detection Pretraining<br>
[paper](https://arxiv.org/abs/2303.02489) <br><br>

[3]Enhanced Training of Query-Based Object Detection via Selective Query Recollection<br>
[paper](https://arxiv.org/abs/2212.07593) | [code](https://github.com/Fangyi-Chen/SQR)<br><br>

[2]DETRs with Hybrid Matching<br>
[paper](https://arxiv.org/abs/2207.13080) | [code](https://github.com/HDETR)<br><br>

[1]YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors(YOLOv7)<br>
[paper](https://arxiv.org/abs/2207.02696) | [code](https://github.com/WongKinYiu/yolov7)<br><br>

<br>

<br>


<a name="VOD"/> 

### 视频目标检测(Video Object Detection)

[4]Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video<br>
[paper](https://arxiv.org/abs/2303.16053) | [code](https://github.com/wenzhengzeng/mpeblink)<br><br>

[3]Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies<br>
[paper](https://arxiv.org/abs/2303.14768) | [code](https://github.com/tencentyouturesearch/highlightdetection-clc)<br><br>

[2]3D Video Object Detection with Learnable Object-Centric Global Optimization<br>
[paper](https://arxiv.org/abs/2303.15416) | [code](https://github.com/jiaweihe1996/ba-det)<br><br>

[1]SCOTCH and SODA: A Transformer Video Shadow Detection Framework<br>
[paper](https://arxiv.org/abs/2211.06885) <br><br>

<br>

<br>

<a name="3DOD"/> 

### 3D目标检测(3D object detection)

[28]Curricular Object Manipulation in LiDAR-based Object Detection<br>
[paper](https://arxiv.org/abs/2304.04248) | [code](https://github.com/zzy816/com)<br><br>

[27]Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection<br>
[paper](https://arxiv.org/abs/2304.01464) | [code](https://github.com/azhuantou/hssda)<br><br>

[26]Understanding the Robustness of 3D Object Detection with Bird's-Eye-View Representations in Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.17297)<br><br>

[25]Learned Two-Plane Perspective Prior based Image Resampling for Efficient Object Detection<br>
[paper](https://arxiv.org/abs/2303.14311)<br><br>

[24]Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images<br>
[paper](https://arxiv.org/abs/2303.14488) | [code](https://github.com/cuogeihong/ceasc)<br><br>

[23]Viewpoint Equivariance for Multi-View 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.14548) | [code](https://github.com/tri-ml/vedet)<br><br>

[22]Neural Part Priors: Learning to Optimize Part-Based Object Completion in RGB-D Scans<br>
[paper](https://arxiv.org/abs/2203.09375)<br><br>

[21]itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2205.15531)<br><br>

[20]Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild<br>
[paper](https://arxiv.org/abs/2207.10660) | [code](https://github.com/facebookresearch/omni3d)<br><br>

[19]FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection<br>
[paper](https://arxiv.org/abs/2301.04467) | [code](https://github.com/robertwyq/frustum)<br><br>

[18]NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations<br>
[paper](https://arxiv.org/abs/2303.13483)<br><br>

[17]Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.11040)<br><br>

[16]VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking<br>
[paper](https://arxiv.org/abs/2303.11301) | [code](https://github.com/dvlab-research/VoxelNeXt)<br><br>

[15]OcTr: Octree-based Transformer for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.12621)<br><br>

[14]MonoATT: Online Monocular 3D Object Detection with Adaptive Token Transformer<br>
[paper](https://arxiv.org/abs/2303.13018)<br><br>

[13]CAPE: Camera View Position Embedding for Multi-View 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.10209) | [code](https://github.com/PaddlePaddle/Paddle3D)<br><br>

[12]Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency<br>
[paper](https://arxiv.org/abs/2303.08686)<br><br>

[11]AeDet: Azimuth-invariant Multi-view 3D Object Detection<br>
[paper](https://arxiv.org/abs/2211.12501) | [code](https://github.com/fcjian/AeDet)<br><br>

[10]Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.05886)<br><br>

[9]PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.08129) | [code](https://github.com/blvlab/pimae)<br><br>

[8]MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection from Point Cloud Sequences<br>
[paper](https://arxiv.org/abs/2303.08316)<br><br>

[7]Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View<br>
[paper](https://arxiv.org/abs/2303.01686)<br><br>

[6]X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.02203)<br><br>

[5]Virtual Sparse Convolution for Multimodal 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.02314) | [code](https://github.com/hailanyi/virconv)<br><br>

[4]MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2209.03102) | [code](https://github.com/sxjyjay/msmdfusion)<br><br>

[3]Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection<br>
[paper](https://arxiv.org/abs/2303.06880) | [code](https://github.com/PJLab-ADG/3DTrans)<br><br>

[2]LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion<br>
[paper](https://arxiv.org/abs/2303.03595) | [code](https://github.com/sankin97/LoGoNet)<br><br>

[1]ConQueR: Query Contrast Voxel-DETR for 3D Object Detection(3D 目标检测的Query Contrast Voxel-DETR)
[paper](https://arxiv.org/abs/2212.07289) | [code](https://github.com/poodarchu/ConQueR)<br><br>

<br>

<br>

<a name="HOI"/> 

### 人物交互检测(HOI Detection)

[4]Relational Context Learning for Human-Object Interaction Detection<br>
[paper](https://arxiv.org/abs/2304.04997)<br><br>

[3]Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream<br>
[paper](https://arxiv.org/abs/2304.03184)<br><br>

[2]Category Query Learning for Human-Object Interaction Classification<br>
[paper](https://arxiv.org/abs/2303.14005)<br><br>

[1]Detecting Human-Object Contact in Images<br>
[paper](https://arxiv.org/abs/2303.03373)<br><br>

<br>

<br>

<a name="COD"/> 

### 伪装目标检测(Camouflaged Object Detection)

[1]Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers<br>
[paper](https://arxiv.org/abs/2303.14816) | [code](https://github.com/zhouhuang23/fspnet)<br><br>

<br>

<br>

<a name="ROD"/> 

### 旋转目标检测(Rotation Object Detection)

<br>

<a name="SOD"/> 

### 显著性目标检测(Saliency Object Detection)

[2]Sketch2Saliency: Learning to Detect Salient Objects from Human Drawings<br>
[paper](https://arxiv.org/abs/2303.11502)<br><br>


[1]Texture-guided Saliency Distilling for Unsupervised Salient Object Detection<br>
[paper](https://arxiv.org/abs/2207.05921) | [code](https://github.com/moothes/A2S-v2)<br><br>

<br>


<br>

<a name="KeypointDetection"/> 

### 关键点检测(Keypoint Detection)

[2]Few-shot Geometry-Aware Keypoint Localization<br>
[paper](https://arxiv.org/abs/2303.17216)<br><br>

[1]Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling<br>
[paper](https://arxiv.org/abs/2303.15270)<br><br>

<br>

<br>

<a name="LaneDetection"/> 

### 车道线检测(Lane Detection)

[1]BEV-LaneDet: a Simple and Effective 3D Lane Detection Baseline<br>
[paper](https://arxiv.org/abs/2210.06006)<br><br>

<br>

<br>

<a name="EdgeDetection"/> 

### 边缘检测(Edge Detection)

[2]The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector<br>
[paper](https://arxiv.org/abs/2303.11828) | [code](https://github.com/zhoucx117/uaed)<br><br>

[1]Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections<br>
[paper](https://arxiv.org/abs/2212.03022) | [code](https://github.com/alexander-g/inbd)<br><br>

<br>

<br>

<a name="VPD"/> 

### 消失点检测(Vanishing Point Detection)

<br>

<br>

<a name="AnomalyDetection"/> 

### 异常检测(Anomaly Detection)

[14]Video Event Restoration Based on Keyframes for Video Anomaly Detection<br>
[paper](https://arxiv.org/abs/2304.05112)<br><br>

[13]Robust Outlier Rejection for 3D Registration with Variational Bayes<br>
[paper](https://arxiv.org/abs/2304.01514) | [code](https://github.com/jiang-hb/vbreg)<br><br>

[12]OpenMix: Exploring Outlier Samples for Misclassification Detection<br>
[paper](https://arxiv.org/abs/2303.17093) | [code](https://github.com/Impression2805/OpenMix)<br><br>

[11]WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation<br>
[paper](https://arxiv.org/abs/2303.14814)<br><br>

[10]SimpleNet: A Simple Network for Image Anomaly Detection and Localization<br>
[paper](https://arxiv.org/abs/2303.15140) | [code](https://github.com/donaldrr/simplenet)<br><br>

[9]Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep Skeleton Features<br>
[paper](https://arxiv.org/abs/2303.15167)<br><br>

[8]SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection<br>
[paper](https://arxiv.org/abs/2111.13495)<br><br>

[7]Normalizing Flow based Feature Synthesis for Outlier-Aware Object Detection<br>
[paper](https://arxiv.org/abs/2302.07106)<br><br>

[6]Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection<br>
[paper](https://arxiv.org/abs/2303.13051)<br><br>

[5]DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection<br>
[paper](https://arxiv.org/abs/2211.11317)<br><br>

[4]Diversity-Measurable Anomaly Detection<br>
[paper](https://arxiv.org/abs/2303.05047)<br><br>

[3]Block Selection Method for Using Feature Norm in Out-of-distribution Detection<br>
[paper](https://arxiv.org/abs/2212.02295)<br><br>

[2]Lossy Compression for Robust Unsupervised Time-Series Anomaly Detection<br>
[paper](https://arxiv.org/abs/2212.02303) <br><br>

[1]Multimodal Industrial Anomaly Detection via Hybrid Fusion<br>
[paper](https://arxiv.org/abs/2303.00601) | [code](https://github.com/nomewang/M3DM)<br><br>

<br>


<br>

<a name="Segmentation"/> 


## 分割(Segmentation)

<br>

<a name="ImageSegmentation"/> 

### 图像分割(Image Segmentation)

[7]FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation<br>
[paper](https://arxiv.org/abs/2303.17225)<br><br>

[6]Zero-shot Referring Image Segmentation with Global-Local Context Features<br>
[paper](https://arxiv.org/abs/2303.17811) | [code](https://github.com/seonghoon-yu/zero-shot-ris)<br><br>

[5]Parameter Efficient Local Implicit Image Function Network for Face Segmentation<br>
[paper](https://arxiv.org/abs/2303.15122)<br><br>

[4]EFEM: Equivariant Neural Field Expectation Maximization for 3D Object Segmentation Without Scene Supervision<br>
[paper](https://arxiv.org/abs/2303.15440)<br><br>

[3]Focused and Collaborative Feedback Integration for Interactive Image Segmentation<br>
[paper](https://arxiv.org/abs/2303.11880) | [code](https://github.com/veizgyauzgyauz/fcfi)<br><br>

[2]MP-Former: Mask-Piloted Transformer for Image Segmentation<br>
[paper](https://arxiv.org/abs/2303.07336) | [code](https://github.com/IDEA-Research/MP-Former)<br><br>

[1]Interactive Segmentation as Gaussian Process Classification<br>
[paper](https://arxiv.org/abs/2302.14578)<br><br>

<br>

<br>

<a name="PanopticSegmentation"/> 

### 全景分割(Panoptic Segmentation)

[3]You Only Segment Once: Towards Real-Time Panoptic Segmentation<br>
[paper](https://arxiv.org/abs/2303.14651) | [code](https://github.com/hujiecpp/yoso)<br><br>

[2]UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration<br>
[paper](https://arxiv.org/abs/2206.15083)<br><br>

[1]Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.04803)<br><br>

<br>

<br>

<a name="SemanticSegmentation"/> 

### 语义分割(Semantic Segmentation)

[28]Federated Incremental Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2304.04620) | [code](https://github.com/jiahuadong/fiss)<br><br>

[27]Continual Semantic Segmentation with Automatic Memory Sample Selection<br>
[paper](https://arxiv.org/abs/2304.05015)<br><br>

[26]DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2304.02222) | [code](https://github.com/fy-vision/diga)<br><br>

[25]Exploiting the Complementarity of 2D and 3D Networks to Address Domain-Shift in 3D Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2304.02991) | [code](https://github.com/cvlab-unibo/mm2d3d)<br><br>

[24]3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds<br>
[paper](https://arxiv.org/abs/2304.00690) | [code](https://github.com/xiaoaoran/semanticstf)<br><br>

[23]Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.14360)<br><br>

[22]Instant Domain Augmentation for LiDAR Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.14378)<br><br>

[21]Leveraging Hidden Positives for Unsupervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.15014) | [code](https://github.com/hynnsk/hp)<br><br>

[20]LaserMix for Semi-Supervised LiDAR Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2207.00026) | [code](https://github.com/ldkong1205/LaserMix)<br><br>

[19]Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2208.09910) | [code](https://github.com/LiheYoung/UniMatch)<br><br>

[18]Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs<br>
[paper](https://arxiv.org/abs/2212.00785) | [code](https://github.com/kakaobrain/tcl)<br><br>

[17]Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.11203) | [code](https://github.com/l1997i/lim3d)<br><br>

[16]Reliability in Semantic Segmentation: Are We on the Right Track?<br>
[paper](https://arxiv.org/abs/2303.11298) | [code](https://github.com/naver/relis)<br><br>

[15]Generative Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.11316) | [code](https://github.com/fudan-zvg/gss)<br><br>

[14]Novel Class Discovery for 3D Point Cloud Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.11610) | [code](https://github.com/luigiriz/nops)<br><br>

[13]MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.08600) | [code](https://github.com/jialeli1/lidarseg3d)<br><br>

[12]Side Adapter Network for Open-Vocabulary Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2302.12242) | [code](https://github.com/mendelxu/san)<br><br>

[11]Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes<br>
[paper](https://arxiv.org/abs/2211.10206)<br><br>

[10]Token Contrast for Weakly-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.01267) | [code](https://github.com/rulixiang/toco)<br><br>

[9]Delivering Arbitrary-Modal Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.01480) | [code](https://github.com/jamycheung/DELIVER)<br><br>

[8]Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2211.12268)<br><br>

[7]Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP<br>
[paper](http://arxiv.org/abs/2210.04150) | [code](https://github.com/facebookresearch/ov-seg)<br><br>

[6]Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos<br>
[paper](https://arxiv.org/abs/2303.07224) | [code](https://github.com/THU-LYJ-Lab/AR-Seg)<br><br>

[5]SCPNet: Semantic Scene Completion on Point Cloud<br>
[paper](https://arxiv.org/abs/2303.06884) <br><br>

[4]On Calibrating Semantic Segmentation Models: Analyses and An Algorithm<br>
[paper](https://arxiv.org/abs/2212.12053) <br><br>

[3]Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision<br>
[paper](https://arxiv.org/abs/2301.09121) <br><br>

[2]Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2208.09910) | [code](https://github.com/LiheYoung/UniMatch)<br><br>

[1]Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2302.14250) <br><br>

<br>

<br>

<a name="InstanceSegmentation"/> 

### 实例分割(Instance Segmentation)

[11]Mask-Free Video Instance Segmentation<br>
[paper](https://arxiv.org/abs/2303.15904) | [code](https://github.com/syscv/maskfreevis)<br><br>

[10]Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations<br>
[paper](https://arxiv.org/abs/2303.16891)<br><br>

[9]DoNet: Deep De-overlapping Network for Cytology Instance Segmentation<br>
[paper](https://arxiv.org/abs/2303.14373)<br><br>

[8]The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation<br>
[paper](https://arxiv.org/abs/2303.15062)<br><br>

[7]A Generalized Framework for Video Instance Segmentation<br>
[paper](https://arxiv.org/abs/2211.08834) | [code](https://github.com/miranheo/genvis)<br><br>

[6]FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation<br>
[paper](https://arxiv.org/abs/2303.08594)<br><br>

[5]SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation<br>
[paper](https://arxiv.org/abs/2303.08578) | [code](https://github.com/lslrh/sim)<br><br>

[4]DynaMask: Dynamic Mask Selection for Instance Segmentation<br>
[paper](https://arxiv.org/abs/2303.07868) | [code](https://github.com/lslrh/dynamask)<br><br>

[3]Beyond mAP: Towards better evaluation of instance segmentation<br>
[paper](https://arxiv.org/abs/2207.01614)<br><br>

[2]ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution<br>
[paper](https://arxiv.org/abs/2303.00246)<br><br>

[1]PolyFormer: Referring Image Segmentation as Sequential Polygon Generation(PolyFormer：将图像分割表述为顺序多边形生成)<br>
[paper](https://arxiv.org/abs/2302.07387) <br><br>

<br>

<br>

<a name="Superpixel"/> 

### 超像素(Superpixel)

<br>

<a name="VOS"/> 

### 视频目标分割(Video Object Segmentation)

[5]Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.14361) | [code](https://github.com/shaoyuanlo/stpl)<br><br>

[4]Two-shot Video Object Segmentation<br>
[paper](https://arxiv.org/abs/2303.12078)<br><br>

[3]Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation<br>
[paper](https://arxiv.org/abs/2303.10100)<br><br>

[2]MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2303.07815)<br><br>

[1]InstMove: Instance Motion for Object-centric Video Segmentation<br>
[paper](https://arxiv.org/abs/2303.08132) | [code](https://github.com/wjf5203/vnext)<br><br>

<br>

<br>

<a name="Matting"/> 

### 抠图(Matting)

<br>

<a name="DensePrediction"/> 

### 密集预测(Dense Prediction)

[5]Probabilistic Prompt Learning for Dense Prediction<br>
[paper](https://arxiv.org/abs/2304.00779)<br><br>

[4]Ensemble-based Blackbox Attacks on Dense Prediction<br>
[paper](https://arxiv.org/abs/2303.14304)<br><br>

[3]Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection<br>
[paper](https://arxiv.org/abs/2303.14960) | [code](https://github.com/PaddlePaddle/PaddleDetection)<br><br>

[2]One-to-Few Label Assignment for End-to-End Dense Detection<br>
[paper](https://arxiv.org/abs/2303.11567) | [code](https://github.com/strongwolf/o2f)<br><br>

[1]DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction<br>
[paper](https://arxiv.org/abs/2303.01573)<br><br>

<br>

<br>

<a name="VideoProcessing"/> 

## 视频处理(Video Processing)

[12]BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation<br>
[paper](https://arxiv.org/abs/2304.02225) | [code](https://github.com/junheum/biformer)<br><br>

[11]VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking<br>
[paper](https://arxiv.org/abs/2303.16727)<br><br>

[10]Implicit View-Time Interpolation of Stereo Videos using Multi-Plane Disparities and Non-Uniform Coordinates<br>
[paper](https://arxiv.org/abs/2303.17181)<br><br>

[9]Affordance Grounding from Demonstration Video to Target Image<br>
[paper](https://arxiv.org/abs/2303.14644) | [code](https://github.com/showlab/afformer)<br><br>

[8]Frame Flexible Network<br>
[paper](https://arxiv.org/abs/2303.14817) | [code](https://github.com/bespontaneous/ffn)<br><br>

[7]Joint Video Multi-Frame Interpolation and Deblurring under Unknown Exposure Time<br>
[paper](https://arxiv.org/abs/2303.15043) | [code](https://github.com/shangwei5/vidue)<br><br>

[6]A Unified Pyramid Recurrent Network for Video Frame Interpolation<br>
[paper](https://arxiv.org/abs/2211.03456)<br><br>

[5]Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior<br>
[paper](https://arxiv.org/abs/2303.09757) | [code](https://github.com/jiaqixuac/map-net)<br><br>

[4]Blind Video Deflickering by Neural Filtering with a Flawed Atlas<br>
[paper](https://arxiv.org/abs/2303.08120) | [code](https://github.com/chenyanglei/all-in-one-deflicker)<br><br>

[3]Learning Transferable Spatiotemporal Representations from Natural Script Knowledge<br>
[paper](https://arxiv.org/abs/2209.15280) | [code](https://github.com/tencentarc/tvts)<br><br>

[2]UV Volumes for Real-time Rendering of Editable Free-view Human Performance<br>
[paper]([arxiv.org](https://arxiv.org/abs/2203.14402)) | [code]([github.com](https://github.com/fanegg/UV-Volumes))<br><br>

[1]Exploring Discontinuity for Video Frame Interpolation<br>
[paper]([[2202.07291\] Exploring Discontinuity for Video Frame Interpolation (arxiv.org)](https://arxiv.org/abs/2202.07291))<br><br>

<br>

<br>

<a name="VideoEditing"/> 

### 视频编辑(Video Editing)

[4]VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs<br>
[paper](https://arxiv.org/abs/2303.15893)<br><br>

[3]Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding<br>
[paper](https://arxiv.org/abs/2212.02802)<br><br>

[2]Text-Visual Prompting for Efficient 2D Temporal Video Grounding<br>
[paper](https://arxiv.org/abs/2303.04995)<br><br>

[1]Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation<br>
[paper](https://arxiv.org/abs/2303.00440) | [code](https://github.com/MCG-NJU/EMA-VFI)<br><br>

<br>

<br>

<a name="VideoGeneration"/> 

### 视频生成/视频合成(Video Generation/Video Synthesis)

[7]Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers<br>
[paper](https://arxiv.org/abs/2303.11251) | [code](https://github.com/Ugness/MeBT)<br><br>

[6]Conditional Image-to-Video Generation with Latent Flow Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.13744)<br><br>

[5]3D Cinemagraphy from a Single Image<br>
[paper](https://arxiv.org/abs/2303.05724)<br><br>

[4]VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation<br>
[paper](https://arxiv.org/abs/2303.08320) | [code](https://github.com/modelscope/modelscope)<br><br>

[3]MOSO: Decomposing MOtion, Scene and Object for Video Prediction<br>
[paper](https://arxiv.org/abs/2303.03684) | [code](https://github.com/anonymous202203/moso)<br><br>

[2]SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation<br>
[paper](https://arxiv.org/abs/2211.12194) | [code](https://github.com/Winfredy/SadTalker)<br><br>

[1]Video Probabilistic Diffusion Models in Projected Latent Space(投影潜在空间中的视频概率扩散模型)<br>
[paper](https://arxiv.org/abs/2302.07685) | [project](https://sihyun.me/PVDM)<br><br>


<br>

<br>

<a name="VideoSR"/> 

### 视频超分(Video Super-Resolution)

[2]Structured Sparsity Learning for Efficient Video Super-Resolution<br>
[paper](https://arxiv.org/abs/2206.07687)<br><br>

[1]Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting<br>
[paper](https://arxiv.org/abs/2303.08331)<br><br>

<br>

<br>

<a name="Estimation"/> 

## 估计(Estimation)

[1]Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation<br>
[paper](https://arxiv.org/abs/2304.00152)<br><br>

<br>

<a name="Flow/Pose/MotionEstimation"/> 

### 光流/运动估计(Optical Flow/Motion Estimation)

[4]AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation<br>
[paper](https://arxiv.org/abs/2303.16493)<br><br>

[3]Semi-Weakly Supervised Object Kinematic Motion Prediction<br>
[paper](https://arxiv.org/abs/2303.17774)<br><br>

[2]DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling<br>
[paper](https://arxiv.org/abs/2303.14078)<br><br>

[1]Rethinking Optical Flow from Geometric Matching Consistent Perspective<br>
[paper](https://arxiv.org/abs/2303.08384) | [code](https://github.com/dqiaole/matchflow)<br>

<br>

<br>

<br>

<a name="DepthEstimation"/> 

### 深度估计(Depth Estimation)

[8]EGA-Depth: Efficient Guided Attention for Self-Supervised Multi-Camera Depth Estimation<br>
[paper](https://arxiv.org/abs/2304.03369)<br><br>

[7]DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium<br>
[paper](https://arxiv.org/abs/2304.03560) | [code](https://github.com/antabangun/dualrefine)<br><br>

[6]Single Image Depth Prediction Made Better: A Multivariate Gaussian Take<br>
[paper](https://arxiv.org/abs/2303.18164)<br><br>

[5]SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates<br>
[paper](https://arxiv.org/abs/2303.13582)<br><br>

[4]PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes<br>
[paper](https://arxiv.org/abs/2210.01612) | [code](https://github.com/svip-lab/planedepth)<br><br>

[3]HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions<br>
[paper](https://arxiv.org/abs/2303.11616)<br><br>

[2]Fully Self-Supervised Depth Estimation from Defocus Clue<br>
[paper](https://arxiv.org/abs/2303.10752) | [code](https://github.com/ehzoahis/dered)<br><br>

[1] Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation<br>
[paper](https://arxiv.org/abs/2211.13202) | [code](https://github.com/noahzn/Lite-Mono)<br><br>

<br>


<br>

<a name="HumanPoseEstimation"/> 

### 人体解析/人体姿态估计(Human Parsing/Human Pose Estimation)

[18]A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image<br>
[paper](https://arxiv.org/abs/2304.03635) | [code](https://github.com/changlongjianggit/a2j-transformer)<br><br>

[17]Monocular 3D Human Pose Estimation for Sports Broadcasts using Partial Sports Field Registration<br>
[paper](https://arxiv.org/abs/2304.04437) | [code](https://github.com/tobibaum/partialsportsfieldreg_3dhpe)<br><br>

[16]DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback<br>
[paper](https://arxiv.org/abs/2304.04496)<br><br>

[15]TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation<br>
[paper](https://arxiv.org/abs/2303.16730)<br><br>

[14]PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation<br>
[paper](https://arxiv.org/abs/2303.17472) | [code](https://github.com/qitaozhao/poseformerv2)<br><br>

[13]ScarceNet: Animal Pose Estimation with Scarce Annotations<br>
[paper](https://arxiv.org/abs/2303.15023) | [code](https://github.com/chaneyddtt/scarcenet)<br><br>

[12]Human Pose Estimation in Extremely Low-Light Conditions<br>
[paper](https://arxiv.org/abs/2303.15410)<br><br>

[11]Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation<br>
[paper](https://arxiv.org/abs/2303.11180)<br><br>

[10]3D Human Mesh Estimation from Virtual Markers<br>
[paper](https://arxiv.org/abs/2303.11726)<br><br>

[9]Object Pose Estimation with Statistical Guarantees: Conformal Keypoint Detection and Geometric Uncertainty Propagation<br>
[paper](https://arxiv.org/abs/2303.12246)<br><br>

[8]Rigidity-Aware Detection for 6D Object Pose Estimation<br>
[paper](https://arxiv.org/abs/2303.12396)<br><br>

[7]Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video<br>
[paper](https://arxiv.org/abs/2303.08475)<br><br>

[6]Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer<br>
[paper](https://arxiv.org/abs/2302.14338)<br><br>

[5]TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation<br>
[paper](https://arxiv.org/abs/2212.12902)<br><br>

[4]Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting<br>
[paper](https://arxiv.org/abs/2303.05095) <br><br>

[3]PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation<br>
[paper](https://arxiv.org/abs/2303.07337)<br><br>

[2]DistilPose: Tokenized Pose Regression with Heatmap Distillation<br>
[paper](https://arxiv.org/abs/2303.02455)<br><br>

[1]Relightable Neural Human Assets from Multi-view Gradient Illuminations(来自多视图渐变照明的可照明神经人类资产)<br>
[paper](https://arxiv.org/abs/2212.07648)<br><br>

<br>

<br>

<a name="GestureEstimation"/> 

### 手势估计(Gesture Estimation)

[6]CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis<br>
[paper](https://arxiv.org/abs/2303.15469)<br><br>

[5]Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild<br>
[paper](https://arxiv.org/abs/2303.13652)<br><br>

[4]Natural Language-Assisted Sign Language Recognition<br>
[paper](https://arxiv.org/abs/2303.12080) | [code](https://github.com/FangyunWei/SLRT)<br><br>

[3]CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment<br>
[paper](https://arxiv.org/abs/2303.05725) | [code](https://arxiv.org/abs/2303.05725)<br><br>

[2]Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement<br>
[paper](https://arxiv.org/abs/2303.01765)<br><br>

[1]Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos<br>
[paper](https://arxiv.org/abs/2209.09484) | [code](https://github.com/fylwen/htt)<br><br>

<br>


<br>

<a name="ImageProcessing"/> 


## 图像处理(Image Processing)

[3]Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR<br>
[paper](https://arxiv.org/abs/2303.13779)<br><br>

[2]PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment<br>
[paper](https://arxiv.org/abs/2303.11526)<br><br>

[1]DeltaEdit: Exploring Text-free Training for Text-Driven Image Manipulation<br>
[paper](https://arxiv.org/abs/2303.06285) | [code](https://github.com/yueming6568/deltaedit)<br><br>

<br>

<a name="SuperResolution"/> 

### 超分辨率(Super Resolution)

[11]Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2304.03542)<br><br>

[10]Implicit Diffusion Models for Continuous Super-Resolution<br>
[paper](https://arxiv.org/abs/2303.16491) | [code](https://github.com/ree1s/idm)<br><br>

[9]SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer<br>
[paper](https://arxiv.org/abs/2303.17605)<br><br>

[8]Learning Generative Structure Prior for Blind Text Image Super-resolution<br>
[paper](https://arxiv.org/abs/2303.14726) | [code](https://github.com/csxmli2016/marconet)<br><br>

[7]Learning to Zoom and Unzoom<br>
[paper](https://arxiv.org/abs/2303.15390)<br><br>

[6]Activating More Pixels in Image Super-Resolution Transformer<br>
[paper](https://arxiv.org/abs/2205.04437) | [code](https://github.com/chxy95/hat)<br><br>

[5]Super-Resolution Neural Operator<br>
[paper](https://arxiv.org/abs/2303.02584) | [code](https://github.com/2y7c3/super-resolution-neural-operator)<br><br>

[4]Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2303.05156)<br><br>

[3]Perception-Oriented Single Image Super-Resolution using Optimal Objective Estimation<br>
[paper](https://arxiv.org/abs/2211.13676) | [code](https://github.com/seungho-snu/SROOE)<br><br>

[2]N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2211.11436) | [code](https://github.com/rami0205/ngramswin)<br><br>

[1]Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild(野外鲁棒图像超分辨率的去噪扩散概率模型)<br>
[paper](https://arxiv.org/abs/2302.07864) | [project](https://sihyun.me/PVDM/)<br><br>

<br>

<br>

<a name="ImageRestoration"/>

###  图像复原/图像增强/图像重建(Image Restoration/Image Reconstruction)

[20]CherryPicker: Semantic Skeletonization and Topological Reconstruction of Cherry Trees<br>
[paper](https://arxiv.org/abs/2304.04708)<br><br>

[19]Generative Diffusion Prior for Unified Image Restoration and Enhancement<br>
[paper](https://arxiv.org/abs/2304.01247)<br><br>

[18]CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects<br>
[paper](https://arxiv.org/abs/2303.15782)<br><br>

[17]HyperThumbnail: Real-time 6K Image Rescaling with Rate-distortion Optimization<br>
[paper](https://arxiv.org/abs/2304.01064) | [code](https://github.com/abnervictor/hyperthumbnail)<br><br>

[16]Burstormer: Burst Image Restoration and Enhancement Transformer<br>
[paper](https://arxiv.org/abs/2304.01194)<br><br>

[15]Visual-Tactile Sensing for In-Hand Object Reconstruction<br>
[paper](https://arxiv.org/abs/2303.14498)<br><br>

[14]3D-Aware Multi-Class Image-to-Image Translation with NeRFs<br>
[paper](https://arxiv.org/abs/2303.15012) | [code](https://github.com/sen-mao/3di2i-translation)<br><br>

[13]CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not<br>
[paper](https://arxiv.org/abs/2303.13440)<br><br>

[12]Instant Volumetric Head Avatars<br>
[paper](https://arxiv.org/abs/2211.12499)<br><br>

[11]Contrastive Semi-supervised Learning for Underwater Image Restoration via Reliable Bank<br>
[paper](https://arxiv.org/abs/2303.09101) | [code](https://github.com/huang-shirui/semi-uir)<br><br>

[10]ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction<br>
[paper](https://arxiv.org/abs/2303.05938) | [code](https://github.com/zhengdiyu/arbitrary-hands-3d-reconstruction)<br><br>

[9]Masked Image Modeling with Local Multi-Scale Reconstruction<br>
[paper](https://arxiv.org/abs/2303.05251) | [code](https://github.com/huawei-noah/Efficient-Computing)<br><br>

[8]Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective<br>
[paper](https://arxiv.org/abs/2303.06859) | [code](https://github.com/lixinustc/Casual-IRDIL)<br><br>

[7]DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration<br>
[paper](https://arxiv.org/abs/2303.06885) <br><br>

[6]Robust Unsupervised StyleGAN Image Restoration<br>
[paper](https://arxiv.org/abs/2302.06733)<br><br>

[5]Raw Image Reconstruction with Learned Compact Metadata<br>
[paper](https://arxiv.org/abs/2302.12995)<br><br>

[4]Efficient and Explicit Modelling of Image Hierarchies for Image Restoration<br>
[paper](https://arxiv.org/abs/2303.00748) | [code](https://github.com/ofsoundof/GRL-Image-Restoration)<br><br>

[3]Imagic: Text-Based Real Image Editing with Diffusion Models<br>
[paper](https://arxiv.org/abs/2210.09276) | [project](https://imagic-editing.github.io/)<br><br>

[2]High-resolution image reconstruction with latent diffusion models from human brain activity<br>
[paper](https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2) | [project](https://sites.google.com/view/stablediffusion-with-brain/)<br><br>

[1]Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.10655)<br><br>

<br>

<br>


<a name="ISR"/> 

### 图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)

[2]Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior<br>
[paper](https://arxiv.org/abs/2303.15046) | [code](https://github.com/ykdai/BracketFlare)<br><br>

[1]LightPainter: Interactive Portrait Relighting with Freehand Scribble<br>
[paper](https://arxiv.org/abs/2303.12950)<br><br>

<br>



<a name="ImageDenoising"/> 

### 图像去噪/去模糊/去雨去雾(Image Denoising)

[12]RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors<br>
[paper](https://arxiv.org/abs/2304.03994) | [code](https://github.com/RQ-Wu/RIDCP_dehazing)<br><br>

[11]HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering<br>
[paper](https://arxiv.org/abs/2304.01686)<br><br>

[10]Real-time Controllable Denoising for Image and Video<br>
[paper](https://arxiv.org/abs/2303.16425)<br><br>

[9]LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising<br>
[paper](https://arxiv.org/abs/2304.00534) | [code](https://github.com/wang-xiaodingdd/lgbpn)<br><br>

[8]Curricular Contrastive Regularization for Physics-aware Single Image Dehazing<br>
[paper](https://arxiv.org/abs/2303.14218) | [code](https://github.com/yuzheng9/c2pnet)<br><br>

[7]Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising<br>
[paper](https://arxiv.org/abs/2303.14934) | [code](https://github.com/nagejacob/spatiallyadaptivessid)<br><br>

[6]Masked Image Training for Generalizable Deep Image Denoising<br>
[paper](https://arxiv.org/abs/2303.13132) | [code](https://github.com/haoyuc/maskeddenoising)<br><br>

[5]Learning A Sparse Transformer Network for Effective Image Deraining<br>
[paper](https://arxiv.org/abs/2303.11950) | [code](https://github.com/cschenxiang/drsformer)<br><br>

[4]Uncertainty-Aware Unsupervised Image Deblurring with Deep Residual Prior<br>
[paper](https://arxiv.org/abs/2210.05361)<br><br>

[3]Polarized Color Image Denoising using Pocoformer<br>
[paper](https://arxiv.org/abs/2207.00215)<br><br>

[2]Blur Interpolation Transformer for Real-World Motion from Blur<br>
[paper](https://arxiv.org/abs/2211.11423) | [code](https://github.com/zzh-tech/BiT)<br><br>

[1]Structured Kernel Estimation for Photon-Limited Deconvolution<br>
[paper](https://arxiv.org/abs/2303.03472) | [code](https://github.com/sanghviyashiitb/structured-kernel-cvpr23)<br><br>

<br>

<br>

<a name="ImageEdit"/> 

### 图像编辑/图像修复(Image Edit/Inpainting)

[6]SIEDOB: Semantic Image Editing by Disentangling Object and Background<br>
[paper](https://arxiv.org/abs/2303.13062) | [code](https://github.com/wuyangluo/siedob)<br><br>

[5]CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing<br>
[paper](https://arxiv.org/abs/2303.05031)<br><br>

[4]SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model<br>
[paper](https://arxiv.org/abs/2212.05034)<br><br>

[3]Interactive Cartoonization with Controllable Perceptual Factors<br>
[paper](https://arxiv.org/abs/2212.09555)<br><br>

[2]Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint<br>
[paper](https://arxiv.org/abs/2211.11448) | [code](https://github.com/kumapowerliu/clcae)<br><br>

[1]LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data<br>
[paper](https://arxiv.org/abs/2208.14889) | [code](https://github.com/KU-CVLAB/LANIT)<br><br>

<br>

<br>

<a name="ImageTranslation"/> 

### 图像翻译(Image Translation)

[1]Masked and Adaptive Transformer for Exemplar Based Image Translation<br>
[paper](https://arxiv.org/abs/2303.17123) | [code](https://github.com/aiart-hdu/matebit)<br><br>

<br>

<br>

<a name="IQA"/> 

### 图像质量评估(Image Quality Assessment)

[3]Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild<br>
[paper](https://arxiv.org/abs/2304.00451)<br><br>

[2]CR-FIQA: Face Image Quality Assessment by Learning Sample Relative Classifiability<br>
[paper](https://arxiv.org/abs/2112.06592) <br><br>

[1]Quality-aware Pre-trained Models for Blind Image Quality Assessment<br>
[paper](https://arxiv.org/abs/2303.00521)<br><br>

<br>

<a name="StyleTransfer"/> 

### 风格迁移(Style Transfer)

[4]CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer<br>
[paper](https://arxiv.org/abs/2303.17867)<br><br>

[3]Neural Preset for Color Style Transfer<br>
[paper](https://arxiv.org/abs/2303.13511) | [code](https://github.com/ZHKKKe/NeuralPreset)<br><br>

[2]StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2303.10598)<br><br>

[1]Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN<br>
[paper](https://arxiv.org/abs/2204.14079) | [code](https://github.com/LeeDongYeun/FixNoise)<br><br>

<br>

<br>

<a name="ImageRegistration"/> 

### 图像配准(Image Registration)

[1]Indescribable Multi-modal Spatial Evaluator<br>
[paper](https://arxiv.org/abs/2303.00369) | [code](https://github.com/Kid-Liet/IMSE/pulse)<br><br>

<br>

<br>

<a name="Face"/> 

## 人脸(Face)

<br>

<br>

<a name="FacialRecognition"/> 

### 人脸识别/检测(Facial Recognition/Detection)

[6]Gradient Attention Balance Network: Mitigating Face Recognition Racial Bias via Gradient Attention<br>
[paper](https://arxiv.org/abs/2304.02284)<br><br>

[5]Micron-BERT: BERT-based Facial Micro-Expression Recognition<br>
[paper](https://arxiv.org/abs/2304.03195) | [code](https://github.com/uark-cviu/micron-bert)<br><br>

[4]Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition<br>
[paper](https://arxiv.org/abs/2303.15818)<br><br>

[3]Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition<br>
[paper](https://arxiv.org/abs/2303.12512)<br><br>

[2]Local Region Perception and Relationship Learning Combined with Feature Fusion for Facial Action Unit Detection<br>
[paper](https://arxiv.org/abs/2303.08545)<br><br>

[1]Multi Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling<br>
[paper](https://arxiv.org/abs/2303.08419)<br><br>

<br>

<br>

<a name="FaceSynthesis"/> 

### 人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)

[13]GANHead: Towards Generative Animatable Neural Head Avatars<br>
[paper](https://arxiv.org/abs/2304.03950)<br><br>

[12]Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos<br>
[paper](https://arxiv.org/abs/2304.01436)<br><br>

[11]StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer<br>
[paper](https://arxiv.org/abs/2304.02744)<br><br>

[10]OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering<br>
[paper](https://arxiv.org/abs/2303.14662)<br><br>

[9]High-fidelity 3D Human Digitization from Single 2K Resolution Images<br>
[paper](https://arxiv.org/abs/2303.15108)<br><br>

[8]FaceLit: Neural 3D Relightable Faces<br>
[paper](https://arxiv.org/abs/2303.15437)<br><br>

[7]SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage<br>
[paper](https://arxiv.org/abs/2204.03648)<br><br>

[6]MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation<br>
[paper](https://arxiv.org/abs/2212.08062) | [code](https://github.com/Meta-Portrait/MetaPortrait)<br><br>

[5]NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images<br>
[paper](https://arxiv.org/abs/2303.14092)<br><br>

[4]Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D Images<br>
[paper](https://arxiv.org/abs/2303.10896)<br><br>

[3]Robust Model-based Face Reconstruction through Weakly-Supervised Outlier Segmentation<br>
[paper](https://arxiv.org/abs/2106.09614) | [code](https://github.com/unibas-gravis/Occlusion-Robust-MoFA)<br><br>

[2]A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images<br>
[paper](https://arxiv.org/abs/2302.14434)<br><br>

[1]MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation(MetaPortrait：具有快速个性化适应的身份保持谈话头像生成)<br>
[paper](https://arxiv.org/abs/2212.08062) | [code](https://github.com/Meta-Portrait/MetaPortrait)<br><br>

<br>

<br>

<a name="FaceAnti-Spoofing"/> 

### 人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)

[4]Hierarchical Fine-Grained Image Forgery Detection and Localization<br>
[paper](https://arxiv.org/abs/2303.17111)<br><br>

[3]Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment<br>
[paper](https://arxiv.org/abs/2303.13662)<br><br>

[2]Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization<br>
[paper](https://arxiv.org/abs/2210.14457) | [code](https://github.com/megvii-research/caddm)<br><br>

[1]Physical-World Optical Adversarial Attacks on 3D Face Recognition<br>
[paper](https://arxiv.org/abs/2205.13412)<br><br>

<br>


<br>

<a name="ObjectTracking"/> 

## 目标跟踪(Object Tracking)

[11]Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction<br>
[paper](https://arxiv.org/abs/2304.04298) | [code](https://github.com/viewsetting/unsupervised_sampling_promoting)<br><br>

[10]Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion<br>
[paper](https://arxiv.org/abs/2304.01893)<br><br>

[9]Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction<br>
[paper](https://arxiv.org/abs/2303.16005) | [code](https://github.com/colorfulfuture/gc-vrnn)<br><br>

[8]Visibility Aware Human-Object Interaction Tracking from Single RGB Camera<br>
[paper](https://arxiv.org/abs/2303.16479)<br><br>

[7]DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks<br>
[paper](https://arxiv.org/abs/2304.00571) | [code](https://github.com/jimmy-dq/dropmae)<br><br>

[6]MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking<br>
[paper](https://arxiv.org/abs/2303.10404)<br><br>

[5]Visual Prompt Multi-Modal Tracking<br>
[paper](https://arxiv.org/abs/2303.10826) | [code](https://github.com/jiawen-zhu/vipt)<br><br>

[4]Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking<br>
[paper](https://arxiv.org/abs/2203.14360v2) | [code](https://github.com/noahcao/OC_SORT)<br><br>

[3]Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation<br>
[paper](https://arxiv.org/abs/2302.14589) <br><br>

[2]Referring Multi-Object Tracking<br>
[paper](https://arxiv.org/abs/2303.03366) <br><br>

[1]Simple Cues Lead to a Strong Multi-Object Tracker<br>
[paper](https://arxiv.org/abs/2206.04656)<br><br>

<br>

<br>
<a name="ImageRetrieval"/> 

## 图像&视频检索/视频理解(Image&Video Retrieval/Video Understanding)

[17]Improving Image Recognition by Retrieving from Web-Scale Image-Text Data<br>
[paper](https://arxiv.org/abs/2304.05173)<br><br>

[16]System-status-aware Adaptive Network for Online Streaming Video Understanding<br>
[paper](https://arxiv.org/abs/2303.15742)<br><br>

[15]Hierarchical Video-Moment Retrieval and Step-Captioning<br>
[paper](https://arxiv.org/abs/2303.16406) | [code](https://github.com/j-min/HiREST)<br><br>

[14]Procedure-Aware Pretraining for Instructional Video Understanding<br>
[paper](https://arxiv.org/abs/2303.18230) | [code](https://github.com/salesforce/paprika)<br><br>

[13]Use Your Head: Improving Long-Tail Video Recognition<br>
[paper](https://arxiv.org/abs/2304.01143)<br><br>

[12]Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style<br>
[paper](https://arxiv.org/abs/2303.14348) | [code](https://github.com/buptlinfy/zse-sbir)<br><br>

[11]Selective Structured State-Spaces for Long-Form Video Understanding<br>
[paper](https://arxiv.org/abs/2303.14526)<br><br>

[10]Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?<br>
[paper](https://arxiv.org/abs/2301.00184) | [code](https://github.com/whwu95/Cap4Video)<br><br>

[9]NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory<br>
[paper](https://arxiv.org/abs/2301.00746)<br><br>

[8]Aligning Step-by-Step Instructional Diagrams to Video Demonstrations<br>
[paper](https://arxiv.org/abs/2303.13800)<br><br>

[7]Query-Dependent Video Representation for Moment Retrieval and Highlight Detection<br>
[paper](https://arxiv.org/abs/2303.13874)<br><br>

[6]Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval<br>
[paper](https://arxiv.org/abs/2303.12501) | [code](https://github.com/anosorae/irra)<br><br>

[5]Dual-path Adaptation from Image to Video Transformers<br>
[paper](https://arxiv.org/abs/2303.09857) | [code](https://github.com/park-jungin/dualpath)<br><br>

[4]Data-Free Sketch-Based Image Retrieval<br>
[paper](https://arxiv.org/abs/2303.07775)<br><br>

[3]DAA: A Delta Age AdaIN operation for age estimation via binary code transformer<br>
[paper](https://arxiv.org/abs/2303.07929)<br><br>

[2]VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval<br>
[paper](https://arxiv.org/abs/2211.12764) | [code](https://github.com/bighuang624/vop)<br><br>

[1]Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval<br>
[paper](https://arxiv.org/abs/2206.02082)<br><br>

<br>



<a name="ActionRecognition"/> 

### 行为识别/动作识别/检测/分割/定位(Action/Activity Recognition)

[16]Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition<br>
[paper](https://arxiv.org/abs/2303.15467)<br><br>

[15]STMixer: A One-Stage Sparse Action Detector<br>
[paper](https://arxiv.org/abs/2303.15879)<br><br>

[14]TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition<br>
[paper](https://arxiv.org/abs/2303.16268)<br><br>

[13]Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection<br>
[paper](https://arxiv.org/abs/2303.17285)<br><br>

[12]STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition<br>
[paper](https://arxiv.org/abs/2303.18177)<br><br>

[11]MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition<br>
[paper](https://arxiv.org/abs/2304.00946)<br><br>

[10]On the Benefits of 3D Pose and Tracking for Human Action Recognition<br>
[paper](https://arxiv.org/abs/2304.01199)<br><br>

[9]3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition<br>
[paper](https://arxiv.org/abs/2303.14474)<br><br>

[8]Box-Level Active Detection<br>
[paper](https://arxiv.org/abs/2303.13089)<br><br>

[7]Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition<br>
[paper](https://arxiv.org/abs/2303.10904)<br><br>

[6]Open Set Action Recognition via Multi-Label Evidential Learning<br>
[paper](https://arxiv.org/abs/2303.12698)<br><br>

[5]Video Test-Time Adaptation for Action Recognition<br>
[paper](https://arxiv.org/abs/2211.15393)<br><br>

[4]Post-Processing Temporal Action Detection<br>
[paper](https://arxiv.org/abs/2211.14924)<br><br>

[3]TriDet: Temporal Action Detection with Relative Boundary Modeling<br>
[paper](https://arxiv.org/abs/2303.07347) | [code](https://github.com/sssste/TriDet)<br><br>

[2]Learning Discriminative Representations for Skeleton Based Action Recognition<br>
[paper](https://arxiv.org/abs/2303.03729) <br><br>

[1]Continuous Sign Language Recognition with Correlation Network<br>
[paper](https://arxiv.org/abs/2303.03202) | [code](https://github.com/hulianyuyy/CorrNet)<br><br>

<br>

<a name="Re-Identification"/> 

### 行人重识别/检测(Re-Identification/Detection)

[7]Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification<br>
[paper](https://arxiv.org/abs/2304.04205) | [code](https://github.com/jiawei151/sgiel_vireid)<br><br>

[6]PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification<br>
[paper](https://arxiv.org/abs/2304.01537)<br><br>

[5]Large-scale Training Data Search for Object Re-identification<br>
[paper](https://arxiv.org/abs/2303.16186)<br><br>

[4]Adaptive Sparse Pairwise Loss for Object Re-Identification<br>
[paper](https://arxiv.org/abs/2303.18247) | [code](https://github.com/astaxanthin/adasp)<br><br>

[3]Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification<br>
[paper](https://arxiv.org/abs/2303.14481) | [code](https://github.com/zyk100/llcm)<br><br>

[2]TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification<br>
[paper](https://arxiv.org/abs/2303.06819) | [code](https://github.com/kali-hac/transg)<br><br>

[1]MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID<br>
[paper](https://arxiv.org/abs/2303.07065) | [code](https://github.com/vimar-gu/MSINet)<br><br>

<br>

<a name="VideoCaption"/> 

### 图像/视频字幕(Image/Video Caption)

[8]Model-Agnostic Gender Debiased Image Captioning<br>
[paper](https://arxiv.org/abs/2304.03693)<br><br>

[7]Cross-Domain Image Captioning with Discriminative Finetuning<br>
[paper](https://arxiv.org/abs/2304.01662) | [code](https://github.com/facebookresearch/EGG)<br><br>

[6]AutoAD: Movie Description in Context<br>
[paper](https://arxiv.org/abs/2303.16899) | [code](https://github.com/Soldelli/MAD)<br><br>

[5]Text with Knowledge Graph Augmented Transformer for Video Captioning<br>
[paper](https://arxiv.org/abs/2303.12423)<br><br>

[4]Dual-Stream Transformer for Generic Event Boundary Captioning<br>
[paper](https://arxiv.org/abs/2207.03038) | [code](https://github.com/gx77/dual-stream-transformer-for-generic-event-boundary-captioning)<br><br>

[3]ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing<br>
[paper](https://arxiv.org/abs/2303.02437) | [code](https://github.com/joeyz0z/conzic)<br><br>

[2]Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training<br>
[paper](https://arxiv.org/abs/2303.00040) <br><br>

[1]Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning<br>
[paper](https://arxiv.org/abs/2302.14115) | [code](https://antoyang.github.io/vid2seq.html)<br><br>

<br>


<a name="MedicalImaging"/> 

## 医学影像(Medical Imaging)

[15]Deep Prototypical-Parts Ease Morphological Kidney Stone Identification and are Competitively Robust to Photometric Perturbations<br>
[paper](https://arxiv.org/abs/2304.04077) | [code](https://github.com/danielf29/prototipical_parts)<br><br>

[14]Coherent Concept-based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis<br>
[paper](https://arxiv.org/abs/2304.04579) | [code](https://github.com/cristianopatricio/coherent-cbe-skin)<br><br>

[13]Topology-Guided Multi-Class Cell Context Generation for Digital Pathology<br>
[paper](https://arxiv.org/abs/2304.02255)<br><br>

[12]Fair Federated Medical Image Segmentation via Client Contribution Estimation<br>
[paper](https://arxiv.org/abs/2303.16520)<br><br>

[11]Directional Connectivity-based Segmentation of Medical Images<br>
[paper](https://arxiv.org/abs/2304.00145) | [code](https://github.com/zyun-y/dconnnet)<br><br>

[10]Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization<br>
[paper](https://arxiv.org/abs/2304.00212)<br><br>

[9]Label-Free Liver Tumor Segmentation<br>
[paper](https://arxiv.org/abs/2303.14869) | [code](https://github.com/mrgiovanni/synthetictumors)<br><br>

[8]Image Quality-aware Diagnosis via Meta-knowledge Co-embedding<br>
[paper](https://arxiv.org/abs/2303.15038)<br><br>

[7]RepMode: Learning to Re-parameterize Diverse Experts for Subcellular Structure Prediction<br>
[paper](https://arxiv.org/abs/2212.10066) | [code](https://github.com/Correr-Zhou/RepMode)<br><br>

[6]Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation<br>
[paper](https://arxiv.org/abs/2303.13090) | [code](https://github.com/hengcai-nju/desco)<br><br>

[5]Task-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification<br>
[paper](https://arxiv.org/abs/2303.08446)<br><br>

[4]Neuron Structure Modeling for Generalizable Remote Physiological Measurement<br>
[paper](https://arxiv.org/abs/2303.05955) | [code](https://github.com/lupaopao/nest)<br><br>

[3]Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses<br>
[paper](https://arxiv.org/abs/2303.08364) | [code](https://github.com/junbongjang/contour-tracking)<br><br>

[2]Deep Feature In-painting for Unsupervised Anomaly Detection in X-ray Images<br>
[paper](https://arxiv.org/pdf/2111.13495.pdf) | [code](https://github.com/tiangexiang/SQUID)<br><br>

[1]Label-Free Liver Tumor Segmentation<br>
[paper](https://arxiv.org/pdf/2210.14845.pdf) | [code](https://github.com/MrGiovanni/SyntheticTumors)<br><br>

<br>


<a name="TDR"/> 


## 文本检测/识别/理解(Text Detection/Recognition/Understanding)

[7]Towards Unified Scene Text Spotting based on Sequence Generation<br>
[paper](https://arxiv.org/abs/2304.03435)<br><br>

[6]Images Speak in Images: A Generalist Painter for In-Context Visual Learning<br>
[paper](https://arxiv.org/abs/2212.02499) | [code](https://github.com/baaivision/painter)<br><br>

[5]Context De-confounded Emotion Recognition<br>
[paper](https://arxiv.org/abs/2303.11921)<br><br>

[4]Joint Visual Grounding and Tracking with Natural Language Specification<br>
[paper](https://arxiv.org/abs/2303.12027)<br><br>

[3]Unifying Vision, Text, and Layout for Universal Document Processing<br>
[paper](https://arxiv.org/abs/2212.02623)<br><br>

[2]Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling<br>
[paper](https://arxiv.org/abs/2303.06949) <br><br>

[1]DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting<br>
[paper](https://arxiv.org/pdf/2211.10772v3.pdf) | [code](https://github.com/ViTAE-Transformer/DeepSolo)<br><br>

<br>



<a name="RSI"/> 

## 遥感图像(Remote Sensing Image)

<br>


<a name="GAN"/> 

## GAN/生成式/对抗式(GAN/Generative/Adversarial)

[7]Fine-Grained Face Swapping via Regional GAN Inversion<br>
[paper](https://arxiv.org/abs/2211.14068)<br><br>

[6]Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models<br>
[paper](https://arxiv.org/abs/2303.10774)<br><br>

[5]Graph Transformer GANs for Graph-Constrained House Generation<br>
[paper](https://arxiv.org/abs/2303.08225)<br><br>

[4]Improving GAN Training via Feature Space Shrinkage<br>
[paper](https://arxiv.org/abs/2303.01559) | [code](https://github.com/wentianzhang-ml/adaptivemix)<br><br>

[3]Adversarial Attack with Raindrops<br>
[paper](https://arxiv.org/abs/2302.14267) <br><br>

[2]T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations<br>
[paper](https://arxiv.org/abs/2301.06052) | [project](https://mael-zys.github.io/T2M-GPT/)<br><br>

[1]Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars<br>
[paper](https://arxiv.org/abs/2211.11208) | [project](https://mrtornado24.github.io/Next3D/)<br><br>


<br>

<a name="IGIS"/> 

## 图像生成/图像合成(Image Generation/Image Synthesis)

[30]Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation<br>
[paper](https://arxiv.org/abs/2304.01816)<br><br>

[29]Few-shot Semantic Image Synthesis with Class Affinity Transfer<br>
[paper](https://arxiv.org/abs/2304.02321)<br><br>

[28]Variational Distribution Learning for Unsupervised Text-to-Image Generation<br>
[paper](https://arxiv.org/abs/2303.16105)<br><br>

[27]HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images<br>
[paper](https://arxiv.org/abs/2303.16509)<br><br>

[26]LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation<br>
[paper](https://arxiv.org/abs/2303.17189) | [code](https://github.com/zgctroy/layoutdiffusion)<br><br>

[25]Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert<br>
[paper](https://arxiv.org/abs/2303.17480) | [code](https://github.com/sxjdwang/talklip)<br><br>

[24]Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation<br>
[paper](https://arxiv.org/abs/2303.14377)<br><br>

[23]Freestyle Layout-to-Image Synthesis<br>
[paper](https://arxiv.org/abs/2303.14412) | [code](https://github.com/essunny310/freestylenet)<br><br>

[22]All are Worth Words: A ViT Backbone for Diffusion Models<br>
[paper](https://arxiv.org/abs/2209.12152) | [code](https://github.com/baofff/U-ViT	)<br><br>

[21]Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars<br>
[paper](https://arxiv.org/abs/2211.11208) | [code](https://github.com/MrTornado24/FENeRF)<br><br>

[20]Shifted Diffusion for Text-to-image Generation<br>
[paper](https://arxiv.org/abs/2211.15388) | [code](https://github.com/drboog/Shifted_Diffusion)<br><br>

[19]Towards Practical Plug-and-Play Diffusion Models<br>
[paper](https://arxiv.org/abs/2212.05973)<br><br>

[18]Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis<br>
[paper](https://arxiv.org/abs/2303.14157)<br><br>

[17]Wavelet Diffusion Models are fast and scalable Image Generators<br>
[paper](https://arxiv.org/abs/2211.16152) | [code](https://github.com/vinairesearch/wavediff)<br><br>

[16]Learning 3D-aware Image Synthesis with Unknown Pose Distribution<br>
[paper](https://arxiv.org/abs/2301.07702)<br><br>

[15]Picture that Sketch: Photorealistic Image Generation from Abstract Sketches<br>
[paper](https://arxiv.org/abs/2303.11162)<br><br>

[14]3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process<br>
[paper](https://arxiv.org/abs/2303.10406) | [code](https://github.com/colorful-liyu/3dqd)<br><br>

[13]A Dynamic Multi-Scale Voxel Flow Network for Video Prediction<br>
[paper](https://arxiv.org/abs/2303.09875) | [code](https://github.com/megvii-research/CVPR2023-DMVFN)<br><br>

[12]Regularized Vector Quantization for Tokenized Image Synthesis<br>
[paper](https://arxiv.org/abs/2303.06424)<br><br>

[11]SpaText: Spatio-Textual Representation for Controllable Image Generation<br>
[paper](https://arxiv.org/abs/2211.14305)<br><br>

[10]Unifying Layout Generation with a Decoupled Diffusion Model<br>
[paper](https://arxiv.org/abs/2303.05049)<br><br>

[9]Scaling up GANs for Text-to-Image Synthesis<br>
[paper](https://arxiv.org/abs/2303.05511)<br><br>

[8]Inversion-Based Style Transfer with Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.13203) | [code](https://github.com/zyxelsa/InST)<br><br>

[7]Perspective Fields for Single Image Camera Calibration<br>
[paper](https://arxiv.org/abs/2212.03239)<br><br>

[6]VGFlow: Visibility guided Flow Network for Human Reposing<br>
[paper](https://arxiv.org/abs/2211.08540)<br><br>

[5]DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation<br>
[paper](https://arxiv.org/abs/2208.12242) | [code](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/ppdiffusers/examples/dreambooth)<br><br>

[4]Progressive Open Space Expansion for Open-Set Model Attribution<br>
[paper](https://arxiv.org/abs/2303.06877) | [code](https://github.com/tianyunyoung/pose)<br><br>

[3]Person Image Synthesis via Denoising Diffusion Model<br>
[paper](https://arxiv.org/abs/2211.12500) <br><br>

[2]Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models(使用预训练的 2D 扩散模型解决 3D 逆问题)<br>
[paper](https://arxiv.org/abs/2211.10655) <br><br>

[1]Parallel Diffusion Models of Operator and Image for Blind Inverse Problems(盲反问题算子和图像的并行扩散模型)<br>
[paper](https://arxiv.org/abs/2211.10656) <br><br>

<br>

<a name="3DVision"/> 

## 三维视觉(3D Vision)

[3]LinK: Linear Kernel for LiDAR-based 3D Perception<br>
[paper](https://arxiv.org/abs/2303.16094)<br><br>

[2]Learning a 3D Morphable Face Reflectance Model from Low-cost Data<br>
[paper](https://arxiv.org/abs/2303.11686) | [code](https://github.com/yxuhan/reflectancemm)<br><br>

[1]Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction<br>
[paper](https://arxiv.org/abs/2302.07817) | [code](https://github.com/wzzheng/tpvformer)<br><br>

<br>

<a name="3DPC"/> 

### 点云(Point Cloud)

[26]MEnsA: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds<br>
[paper](https://arxiv.org/abs/2304.01554) | [code](https://github.com/sinashish/mensa_mtda)<br><br>

[25]Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis<br>
[paper](https://arxiv.org/abs/2303.15493)<br><br>

[24]Self-positioning Point-based Transformer for Point Cloud Understanding<br>
[paper](https://arxiv.org/abs/2303.16450) | [code](https://github.com/mlvlab/spotr)<br><br>

[23]NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud<br>
[paper](https://arxiv.org/abs/2303.16465)<br><br>

[22]PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations<br>
[paper](https://arxiv.org/abs/2303.16958)<br><br>

[21]Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud Normal Estimation<br>
[paper](https://arxiv.org/abs/2303.17167) | [code](https://github.com/hikvision-research/3dvision)<br><br>

[20]Learning Human-to-Robot Handovers from Point Clouds<br>
[paper](https://arxiv.org/abs/2303.17592)<br><br>

[19]Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting<br>
[paper](https://arxiv.org/abs/2304.00467) | [code](https://github.com/whu-usi3dv/sghr)<br><br>

[18]Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors<br>
[paper](https://arxiv.org/abs/2303.14505)<br><br>

[17]NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation<br>
[paper](https://arxiv.org/abs/2303.15126) | [code](https://github.com/ispc-lab/neuralpci)<br><br>

[16]Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negatives and no False Positives<br>
[paper](https://arxiv.org/abs/2303.15385)<br><br>

[15]CLIP2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data<br>
[paper](https://arxiv.org/abs/2303.12417)<br><br>

[14]Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration<br>
[paper](https://arxiv.org/abs/2303.13290) | [code](https://github.com/gfmei/udpreg)<br><br>

[13]Deep Graph-based Spatial Consistency for Robust Non-rigid Point Cloud Registration<br>
[paper](https://arxiv.org/abs/2303.09950) | [code](https://github.com/qinzheng93/graphscnet)<br><br>

[12]Controllable Mesh Generation Through Sparse Latent Point Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.07938)<br><br>

[11]Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis<br>
[paper](https://arxiv.org/abs/2303.08134) | [code](https://github.com/zrrskywalker/point-nn)<br><br>

[10]Rotation-Invariant Transformer for Point Cloud Matching<br>
[paper](https://arxiv.org/abs/2303.08231)<br><br>

[9]GraVoS: Voxel Selection for 3D Point-Cloud Detection<br>
[paper](https://arxiv.org/abs/2208.08780)<br><br>

[8]DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets<br>
[paper](https://arxiv.org/abs/2301.06051) | [code](https://github.com/haiyang-w/dsvt)<br><br>

[7]PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees<br>
[paper](https://arxiv.org/abs/2303.01959)<br><br>

[6]ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud Completion<br>
[paper](https://arxiv.org/abs/2303.01979) | [code](https://github.com/sangminhong/acl-spc_pytorch)<br><br>

[5]DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization<br>
[paper](https://arxiv.org/abs/2212.06331)<br><br>

[4]Frequency-Modulated Point Cloud Rendering with Easy Editing<br>
[paper](https://arxiv.org/abs/2303.07596v1)<br><br>

[3]Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss<br>
[paper](https://arxiv.org/abs/2301.05709)<br><br>

[2]ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer<br>
[paper](https://arxiv.org/abs/2302.14435) | [code](https://github.com/I2-Multimedia-Lab/ProxyFormer)<br><br>

[1]Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting<br>
[paper](https://arxiv.org/abs/2302.13130) | [code](https://github.com/tarashakhurana/4d-occ-forecasting)<br><br>

<br>


<a name="3DReconstruction"/> 

### 三维重建(3D Reconstruction)

[28]Multi-View Azimuth Stereo via Tangent Space Consistency<br>
[paper](https://arxiv.org/abs/2303.16447) | [code](https://github.com/xucao-42/mvas)<br><br>

[27]3D Line Mapping Revisited<br>
[paper](https://arxiv.org/abs/2303.17504) | [code](https://github.com/cvg/limap)<br><br>

[26]PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters<br>
[paper](https://arxiv.org/abs/2303.14587) | [code](https://github.com/shuhongchen/panic3d-anime-reconstruction)<br><br>

[25]HexPlane: A Fast Representation for Dynamic Scenes<br>
[paper](https://arxiv.org/abs/2301.09632)<br><br>

[24]Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container<br>
[paper](https://arxiv.org/abs/2303.13805)<br><br>

[23]BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects<br>
[paper](https://arxiv.org/abs/2303.14158)<br><br>

[22]Structured 3D Features for Reconstructing Controllable Avatars<br>
[paper](https://arxiv.org/abs/2212.06820)<br><br>

[21]PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360∘<br>
[paper](https://arxiv.org/abs/2303.13071)<br><br>

[20]Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization<br>
[paper](https://arxiv.org/abs/2303.13232)<br><br>

[19]TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision<br>
[paper](https://arxiv.org/abs/2303.13273) | [code](https://github.com/plusmultiply/taps3d)<br><br>

[18]MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based Self-Supervised Pre-Training<br>
[paper](https://arxiv.org/abs/2303.13510) | [code](https://github.com/smartbot-pjlab/mv-jar)<br><br>

[17]PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision<br>
[paper](https://arxiv.org/abs/2303.09554)<br><br>

[16]SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation<br>
[paper](https://arxiv.org/abs/2212.04493) | [code](https://github.com/yccyenchicheng/SDFusion)<br><br>

[15]Masked Wavelet Representation for Compact Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2212.09069)<br><br>

[14]Decoupling Human and Camera Motion from Videos in the Wild<br>
[paper](https://arxiv.org/abs/2302.12827)<br><br>

[13]Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction<br>
[paper](https://arxiv.org/abs/2303.05937)<br><br>

[12]NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images<br>
[paper](https://arxiv.org/abs/2303.07653)<br><br>

[11]Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion<br>
[paper](https://arxiv.org/abs/2211.11674) | [code](https://github.com/google-research/nerf-from-image)<br><br>

[10]MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices<br>
[paper](https://arxiv.org/abs/2303.01932) | [code](https://github.com/ActiveVisionLab/MobileBrick)<br><br>

[9]Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly<br>
[paper](https://arxiv.org/abs/2303.01999)<br><br>

[8]NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction<br>
[paper](https://arxiv.org/abs/2303.02375)<br><br>

[7]HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling<br>
[paper](https://arxiv.org/abs/2303.02700)<br><br>

[6]MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision<br>
[paper](https://arxiv.org/abs/2303.03315)<br><br>

[4]Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness<br>
[paper](https://arxiv.org/abs/2303.00971) | [code](https://github.com/zhijieshen-bjtu/DOPNet)<br><br>

[3]Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes<br>
[paper](https://arxiv.org/abs/2302.14348) | [code](https://github.com/jyunlee/Im2Hands)<br><br>

[2]ECON: Explicit Clothed humans Obtained from Normals<br>
[paper](https://arxiv.org/abs/2212.07422) | [code](https://github.com/YuliangXiu/ECON)<br><br>

[1]Structured 3D Features for Reconstructing Relightable and Animatable Avatars<br>
[paper](https://arxiv.org/abs/2212.06820) | [project](https://enriccorona.github.io/s3f/)<br><br>

<br>

<a name="NeRF"/> 

### 场景重建/视图合成/新视角合成(Novel View Synthesis)

[51]Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field<br>
[paper](https://arxiv.org/abs/2304.03526)<br><br>

[50]POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo<br>
[paper](https://arxiv.org/abs/2304.04038) | [code](https://github.com/lixiny/poem)<br><br>

[49]Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos<br>
[paper](https://arxiv.org/abs/2304.04452)<br><br>

[48]Neural Lens Modeling<br>
[paper](https://arxiv.org/abs/2304.04848)<br><br>

[47]One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field<br>
[paper](https://arxiv.org/abs/2304.05097)<br><br>

[46]MonoHuman: Animatable Human Neural Field from Monocular Video<br>
[paper](https://arxiv.org/abs/2304.02001)<br><br>

[45]GINA-3D: Learning to Generate Implicit Neural Assets in the Wild<br>
[paper](https://arxiv.org/abs/2304.02163)<br><br>

[44]Neural Fields meet Explicit Geometric Representation for Inverse Rendering of Urban Scenes<br>
[paper](https://arxiv.org/abs/2304.03266)<br><br>

[43]F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories<br>
[paper](https://arxiv.org/abs/2303.15951)<br><br>

[42]NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination<br>
[paper](https://arxiv.org/abs/2303.16617)<br><br>

[41]Enhanced Stable View Synthesis<br>
[paper](https://arxiv.org/abs/2303.17094)<br><br>

[40]Consistent View Synthesis with Pose-Guided Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.17598)<br><br>

[39]NeRF-Supervised Deep Stereo<br>
[paper](https://arxiv.org/abs/2303.17603) | [code](https://github.com/fabiotosi92/nerf-supervised-deep-stereo)<br><br>

[38]Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations<br>
[paper](https://arxiv.org/abs/2303.18139)<br><br>

[37]DyLiN: Making Light Field Networks Dynamic<br>
[paper](https://arxiv.org/abs/2303.14243)<br><br>

[36]FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views<br>
[paper](https://arxiv.org/abs/2303.14368)<br><br>

[35]NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects<br>
[paper](https://arxiv.org/abs/2303.14435) | [code](https://github.com/jokeryan/nerf-ds)<br><br>

[34]SUDS: Scalable Urban Dynamic Scenes<br>
[paper](https://arxiv.org/abs/2303.14536)<br><br>

[33]JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2303.15427)<br><br>

[32]Magic3D: High-Resolution Text-to-3D Content Creation<br>
[paper](https://arxiv.org/abs/2211.10440)<br><br>

[31]DiffRF: Rendering-Guided 3D Radiance Field Diffusion<br>
[paper](https://arxiv.org/abs/2212.01206)<br><br>

[30]Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization<br>
[paper](https://arxiv.org/abs/2212.02766) | [code](https://github.com/dvlab-research/ref-npr)<br><br>

[29]Interactive Segmentation of Radiance Fields <br>
[paper](https://arxiv.org/abs/2212.13545)<br><br>

[28]MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying Lighting Estimation<br>
[paper](https://arxiv.org/abs/2303.12368)<br><br>

[27]GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images<br>
[paper](https://arxiv.org/abs/2303.13777)<br><br>

[26]Progressively Optimized Local Radiance Fields for Robust View Synthesis<br>
[paper](https://arxiv.org/abs/2303.13791)<br><br>

[25]ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field<br>
[paper](https://arxiv.org/abs/2303.13817)<br><br>

[24]HandNeRF: Neural Radiance Fields for Animatable Interacting Hands<br>
[paper](https://arxiv.org/abs/2303.13825)<br><br>

[23]Grid-guided Neural Radiance Fields for Large Urban Scenes<br>
[paper](https://arxiv.org/abs/2303.14001)<br><br>

[22]EventNeRF: Neural Radiance Fields from a Single Colour Event Camera<br>
[paper](https://arxiv.org/abs/2206.11896)<br><br>

[21]SPARF: Neural Radiance Fields from Sparse and Noisy Poses<br>
[paper](https://arxiv.org/abs/2211.11738)<br><br>

[20]RUST: Latent Neural Scene Representations from Unposed Imagery<br>
[paper](https://arxiv.org/abs/2211.14306)<br><br>

[19]SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field<br>
[paper](https://arxiv.org/abs/2303.13277)<br><br>

[18]ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision<br>
[paper](https://arxiv.org/abs/2211.14086) | [code](https://github.com/gerwang/shadowneus)<br><br>

[17]Balanced Spherical Grid for Egocentric View Synthesis<br>
[paper](https://arxiv.org/abs/2303.12408) | [code](https://github.com/changwoonchoi/EgoNeRF)<br><br>

[16]Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention<br>
[paper](https://arxiv.org/abs/2303.13014)<br><br>

[15]MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures<br>
[paper](https://arxiv.org/abs/2208.00277) | [code](https://github.com/google-research/jax3d)<br><br>

[14]Robust Dynamic Radiance Fields<br>
[paper](https://arxiv.org/abs/2301.02239)<br><br>

[13]I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs<br>
[paper](https://arxiv.org/abs/2303.07634)<br><br>

[12]Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image<br>
[paper](https://arxiv.org/abs/2211.13901)<br><br>

[11]Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision<br>
[paper](https://arxiv.org/abs/2303.03361)<br><br>

[10]Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2211.11505)<br><br>

[9]DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors<br>
[paper](https://arxiv.org/abs/2211.12046) | [code](https://github.com/dogyoonlee/dp-nerf)<br><br>

[8]SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields<br>
[paper](https://arxiv.org/abs/2211.12254)<br><br>

[7]3D Video Loops from Asynchronous Input<br>
[paper](https://arxiv.org/abs/2303.05312) | [code](https://github.com/limacv/VideoLoop3D)<br><br>

[6]NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer<br>
[paper](https://arxiv.org/abs/2303.06919) | [code](https://t.co/uNiTd9ujCv)<br><br>

[5]NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation<br>
[paper](https://arxiv.org/abs/2212.14710) <br><br>

[4]Renderable Neural Radiance Map for Visual Navigation<br>
[paper](https://arxiv.org/abs/2303.00304)<br><br>

[3]Real-Time Neural Light Field on Mobile Devices<br>
[paper](https://arxiv.org/abs/2212.08057) | [project](https://snap-research.github.io/MobileR2L/)<br><br>

[2]Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures<br>
[paper](https://arxiv.org/abs/2211.07600) | [code](https://github.com/eladrich/latent-nerf)<br><br>

[1]NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior<br>
[paper](https://arxiv.org/abs/2212.07388) | [project](https://nope-nerf.active.vision/)<br><br>

<br>

<a name="ModelCompression"/> 

## 模型压缩(Model Compression)

[1]Neural Video Compression with Diverse Contexts	<br>
[paper](http://arxiv.org/abs/2302.14402) | [code](https://github.com/microsoft/dcvc)<br><br>

<br>

<a name="KnowledgeDistillation"/> 

### 知识蒸馏(Knowledge Distillation)

[7]Supervised Masked Knowledge Distillation for Few-Shot Transformers<br>
[paper](https://arxiv.org/abs/2303.15466) | [code](https://github.com/hl-hanlin/smkd)<br><br>

[6]DisWOT: Student Architecture Search for Distillation WithOut Training<br>
[paper](https://arxiv.org/abs/2303.15678)<br><br>

[5]KD-DLGAN: Data Limited Image Generation via Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2303.17158)<br><br>

[4]Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2303.14666)<br><br>

[3]Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2302.14290) <br><br>

[2]Generic-to-Specific Distillation of Masked Autoencoders<br>
[paper](https://arxiv.org/abs/2302.14771) | [code](https://github.com/pengzhiliang/G2SD)<br><br>

[1]CLIPPING: Distilling CLIP-based Models for Video-Language Understanding(CLIPPING：为视频语言理解提炼基于 CLIP 的模型)<br>
[paper](https://openreview.net/forum?id=aqIvCsRsYt) <br><br>

<br>

<a name="Pruning"/> 

### 剪枝(Pruning)

[2]CP3: Channel Pruning Plug-in for Point-based Networks<br>
[paper](https://arxiv.org/abs/2303.13097)<br><br>

[1]DepGraph: Towards Any Structural Pruning<br>
[paper](https://arxiv.org/abs/2301.12900) | [code](https://github.com/VainF/Torch-Pruning)<br><br>

<br>

<a name="Quantization"/> 

### 量化(Quantization)

[4]Hard Sample Matters a Lot in Zero-Shot Quantization<br>
[paper](https://arxiv.org/abs/2303.13826)<br><br>

[3]Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective<br>
[paper](https://arxiv.org/abs/2303.11906)<br><br>

[2]Post-training Quantization on Diffusion Models<br>
[paper](https://arxiv.org/abs/2211.15736) | [code](https://github.com/42shawn/ptq4dm)<br><br>

[1]Adaptive Data-Free Quantization<br>
[paper](https://arxiv.org/abs/2303.06869) | [code](https://github.com/hfutqian/AdaDFQ)<br><br>

<br>

<a name="NNS"/> 

## 神经网络结构设计(Neural Network Structure Design)

[9]SMPConv: Self-moving Point Representations for Continuous Convolution<br>
[paper](https://arxiv.org/abs/2304.02330) | [code](https://github.com/sangnekim/smpconv)<br><br>

[8]Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection<br>
[paper](https://arxiv.org/abs/2303.14404) | [code](https://github.com/akhtarvision/bpc_calibration)<br><br>

[7]Compacting Binary Neural Networks by Sparse Kernel Selection<br>
[paper](https://arxiv.org/abs/2303.14470)<br><br>

[6]LINe: Out-of-Distribution Detection by Leveraging Important Neurons<br>
[paper](https://arxiv.org/abs/2303.13995)<br><br>

[5]Towards Scalable Neural Representation for Diverse Videos<br>
[paper](https://arxiv.org/abs/2303.14124)<br><br>

[4]Boundary Unlearning<br>
[paper](https://arxiv.org/abs/2303.11570)<br><br>

[3]Equiangular Basis Vectors<br>
[paper](https://arxiv.org/abs/2303.11637) | [code](https://github.com/njust-vipgroup/equiangular-basis-vectors)<br><br>

[2]LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs<br>
[paper](https://arxiv.org/abs/2206.10555) | [code](https://github.com/dvlab-research/largekernel3d)<br><br>

[1]Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks<br>
[paper](https://arxiv.org/abs/2303.03667) | [code](https://github.com/JierunChen/FasterNet)<br><br>

<br>

<a name="CNN"/> 

### CNN

[6]VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution<br>
[paper](https://arxiv.org/abs/2304.01434) | [code](https://github.com/jaeill/CVPR23-VNE)<br><br>

[5]Randomized Adversarial Training via Taylor Expansion<br>
[paper](https://arxiv.org/abs/2303.10653) | [code](https://github.com/alexkael/randomized-adversarial-training)<br><br>

[4]Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations<br>
[paper](https://arxiv.org/abs/2303.08085) | [code](https://github.com/hmichaeli/alias_free_convnets)<br><br>

[3]DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network<br>
[paper](https://arxiv.org/abs/2303.02165) | [code](https://github.com/alibaba/lightweight-neural-architecture-search)<br><br>

[2]Demystify Transformers & Convolutions in Modern Image Deep Networks<br>
[paper](https://arxiv.org/abs/2211.05781) | [code](https://github.com/OpenGVLab/STM-Evaluation)<br><br>

[1]InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions<br>
[paper](https://arxiv.org/abs/2211.05778) | [code](https://github.com/OpenGVLab/InternImage)<br><br>


<br>

<a name="Transformer"/> 

### Transformer

[24]Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention<br>
[paper](https://arxiv.org/abs/2304.04237) | [code](https://github.com/leaplabthu/slide-transformer)<br><br>

[23]METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens<br>
[paper](https://arxiv.org/abs/2304.02211)<br><br>

[22]MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection<br>
[paper](https://arxiv.org/abs/2304.02767)<br><br>

[21]Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention<br>
[paper](https://arxiv.org/abs/2304.03282) | [code](https://github.com/dingmyu/dependencyvit)<br><br>

[20]Learning Expressive Prompting With Residuals for Vision Transformers<br>
[paper](https://arxiv.org/abs/2303.15591)<br><br>

[19]Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization<br>
[paper](https://arxiv.org/abs/2303.15754)<br><br>

[18]One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer<br>
[paper](https://arxiv.org/abs/2303.16160) | [code](https://github.com/IDEA-Research/OSX)<br><br>

[17]Generalized Relation Modeling for Transformer Tracking<br>
[paper](https://arxiv.org/abs/2303.16580) | [code](https://github.com/little-podi/grm)<br><br>

[16]Learning Anchor Transformations for 3D Garment Animation<br>
[paper](https://arxiv.org/abs/2304.00761)<br><br>

[15]CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection<br>
[paper](https://arxiv.org/abs/2301.01970) | [code](https://github.com/xiaomabufei/CAT)<br><br>

[14]Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers<br>
[paper](https://arxiv.org/abs/2303.13755)<br><br>

[13]POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery<br>
[paper](https://arxiv.org/abs/2303.13357)<br><br>

[12]FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER<br>
[paper](https://arxiv.org/abs/2205.15448)<br><br>

[11]Spherical Transformer for LiDAR-based 3D Recognition<br>
[paper](https://arxiv.org/abs/2303.12766) | [code](https://github.com/dvlab-research/sphereformer)<br><br>

[10]MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models<br>
[paper]() | [code](https://github.com/mlvlab/MELTR)<br><br>

[9]Top-Down Visual Attention from Analysis by Synthesis<br>
[paper](https://arxiv.org/abs/2303.13043)<br><br>

[8]BiFormer: Vision Transformer with Bi-Level Routing Attention<br>
[paper](https://arxiv.org/abs/2303.08810) | [code](https://github.com/rayleizhu/biformer)<br><br>

[7]Making Vision Transformers Efficient from A Token Sparsification View<br>
[paper](https://arxiv.org/abs/2303.08685)<br><br>

[6]Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves<br>
[paper](https://arxiv.org/abs/2303.01112)<br><br>

[5]Learning Imbalanced Data with Vision Transformers<br>
[paper](https://arxiv.org/abs/2212.02015) | [code](https://github.com/xuzhengzhuo/livt)<br><br>

[4]SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency<br>
[paper](https://arxiv.org/abs/2211.02006)<br><br>

[3]Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers<br>
[paper](https://arxiv.org/abs/2205.12551) | [code](https://github.com/yhlleo/mjp)<br><br>

[2]Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR<br>
[paper](https://arxiv.org/abs/2303.07335) | [code](https://github.com/IDEA-Research/Lite-DETR)<br><br>

[1]Integrally Pre-Trained Transformer Pyramid Networks<br>
[paper](https://arxiv.org/abs/2211.12735) | [code](https://github.com/sunsmarterjie/iTPN)<br><br>

<br>

<a name="GNN"/> 

### 图神经网络(GNN)

[4]Adversarially Robust Neural Architecture Search for Graph Neural Networks<br>
[paper](https://arxiv.org/abs/2304.04168)<br><br>

[3]Mind the Label Shift of Augmentation-based Graph OOD Generalization<br>
[paper](https://arxiv.org/abs/2303.14859)<br><br>

[2]Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks<br>
[paper](https://arxiv.org/abs/2303.06199)<br><br>

[1]From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm<br>
[paper](https://arxiv.org/abs/2211.11761)<br><br>


<br>

<a name="NAS"/> 

### 神经网络架构搜索(NAS)

[3]Polynomial Implicit Neural Representations For Large Diverse Datasets<br>
[paper](https://arxiv.org/abs/2303.11424) | [code](https://github.com/rajhans0/poly_inr)<br><br>

[2]PA&DA: Jointly Sampling PAth and DAta for Consistent NAS<br>
[paper](https://arxiv.org/abs/2302.14772) | [code](https://github.com/ShunLu91/PA-DA)<br><br>

[1]Stitchable Neural Networks(可缝合神经网络)<br>
[paper](https://arxiv.org/abs/2302.06586) | [code](https://github.com/ziplab/SN-Net)<br><br>

<br>

<a name="MLP"/> 

### MLP

[1]ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization<br>
[paper](https://arxiv.org/abs/2303.11674) | [code](https://github.com/lingeringlight/aloft)<br><br>

<br>

<a name="M AE"/> 

### MAE

[1]Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders
[paper](https://arxiv.org/abs/2212.06785) | [code](https://github.com/ZrrSkywalker/I2P-MAE)<br><br>



<br>

<a name="DataProcessing"/> 

## 数据处理(Data Processing)

[1]TINC: Tree-structured Implicit Neural Compression<br>
[paper](https://arxiv.org/abs/2211.06689) | [code](https://github.com/richealyoung/tinc)<br><br>

<br>

<a name="DataAugmentation"/> 

### 数据增广(Data Augmentation)





<br>

<a name="BatchNormalization"/> 

### 归一化/正则化(Batch Normalization)

[2]Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling<br>
[paper](https://arxiv.org/abs/2304.03937)<br><br>

[1]Masked Images Are Counterfactual Samples for Robust Fine-tuning<br>
[paper](https://arxiv.org/abs/2303.03052)<br><br>

<br>

<a name="ImageClustering"/> 

### 图像聚类(Image Clustering)

[2]DivClust: Controlling Diversity in Deep Clustering<br>
[paper](https://arxiv.org/abs/2304.01042)<br><br>

[1]On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering<br>
[paper](https://arxiv.org/abs/2303.09877) | [code](https://github.com/danieltrosten/deepmvc)<br><br>

<br>


<a name="ImageCompression"/> 

### 图像压缩(Image Compression)

[2]Learned Image Compression with Mixed Transformer-CNN Architectures<br>
[paper](https://arxiv.org/abs/2303.14978) | [code](https://github.com/jmliu206/lic_tcm)<br><br>

[1]Context-Based Trit-Plane Coding for Progressive Image Compression<br>
[paper](https://arxiv.org/abs/2303.05715) | [code](https://github.com/seungminjeon-github/CTC)<br><br>

<br>

<a name="ModelTraining"/> 

## 模型训练/泛化(Model Training/Generalization)

[25]Improved Test-Time Adaptation for Domain Generalization<br>
[paper](https://arxiv.org/abs/2304.04494)<br><br>

[24]Re-thinking Model Inversion Attacks Against Deep Neural Networks<br>
[paper](https://arxiv.org/abs/2304.01669)<br><br>

[23]Regularize implicit neural representation by itself<br>
[paper](https://arxiv.org/abs/2303.15484)<br><br>

[22]Improving the Transferability of Adversarial Samples by Path-Augmented Method<br>
[paper](https://arxiv.org/abs/2303.15735)<br><br>

[21]Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency<br>
[paper](https://arxiv.org/abs/2303.18191) | [code](https://github.com/cgcl-codes/teco)<br><br>

[20]Progressive Random Convolutions for Single Domain Generalization<br>
[paper](https://arxiv.org/abs/2304.00424)<br><br>

[19]Tunable Convolutions with Parametric Multi-Loss Optimization<br>
[paper](https://arxiv.org/abs/2304.00898)<br><br>

[18]Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm<br>
[paper](https://arxiv.org/abs/2303.14382) | [code](https://github.com/yichen928/activeft)<br><br>

[17]CFA: Class-wise Calibrated Fair Adversarial Training<br>
[paper](https://arxiv.org/abs/2303.14460) | [code](https://github.com/pku-ml/cfa)<br><br>

[16]Generalist: Decoupling Natural and Robust Generalization<br>
[paper](https://arxiv.org/abs/2303.13813)<br><br>

[15]Feature Separation and Recalibration for Adversarial Robustness<br>
[paper](https://arxiv.org/abs/2303.13846)<br><br>

[14]Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck<br>
[paper](https://arxiv.org/abs/2303.14096)<br><br>

[13]FlexiViT: One Model for All Patch Sizes<br>
[paper](https://arxiv.org/abs/2212.08013) | [code](https://github.com/google-research/big_vision)<br><br>

[12]Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization<br>
[paper](https://arxiv.org/abs/2303.13087) | [code](https://github.com/zhuohuangai/sharpdro)<br><br>

[11]Improving Generalization with Domain Convex Game<br>
[paper](https://arxiv.org/abs/2303.13297)<br><br>

[10]TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization<br>
[paper](https://arxiv.org/abs/2303.11135) | [code](https://github.com/ziquanliu/cvpr2023-twins)<br><br>

[9]An Extended Study of Human-like Behavior under Adversarial Training<br>
[paper](https://arxiv.org/abs/2303.12669)<br><br>

[8]Sharpness-Aware Gradient Matching for Domain Generalization<br>
[paper](https://arxiv.org/abs/2303.10353) | [code](https://github.com/wang-pengfei/sagm)<br><br>

[7]HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining<br>
[paper](https://arxiv.org/abs/2303.05675)<br><br>

[6]Universal Instance Perception as Object Discovery and Retrieval<br>
[paper](https://arxiv.org/abs/2303.06674) | [code](https://github.com/MasterBin-IIAU/UNINEXT)<br><br>

[5]Practical Network Acceleration with Tiny Sets<br>
[paper]([arxiv.org](https://arxiv.org/abs/2202.07861)) | [code]([github.com](https://github.com/doctorkey/practise))<br><br>

[4]Towards Bridging the Performance Gaps of Joint Energy-based Models<br>
[paper](https://arxiv.org/abs/2209.07959v2) | [code](https://github.com/sndnyang/sadajem)<br><br>

[3]DropKey<br>
[paper](https://arxiv.org/abs/2208.02646) <br><br>

[2]Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization<br>
[paper](https://arxiv.org/abs/2303.03108)<br><br>

[1]DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks<br>
[paper](https://arxiv.org/abs/2302.14685)<br><br>

<br>

<a name="NoisyLabel"/> 

### 噪声标签(Noisy Label)

[2]Fine-Grained Classification with Noisy Labels<br>
[paper](https://arxiv.org/abs/2303.02404)<br><br>

[1]Combating noisy labels in object detection datasets<br>
[paper](https://arxiv.org/abs/2211.13993)<br><br>



<br>

<a name="Long-Tailed"/> 

### 长尾分布(Long-Tailed Distribution)

[3]Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation<br>
[paper](https://arxiv.org/abs/2304.01279) | [code](https://github.com/jinyan-06/shike)<br><br>

[2]SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail<br>
[paper](https://arxiv.org/abs/2304.00101)<br><br>

[1]Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification<br>
[paper](https://arxiv.org/abs/2303.12307)<br><br>


<br>

<a name="matching"/> 


## 图像特征提取与匹配(Image feature extraction and matching)

[7]CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes<br>
[paper](https://arxiv.org/abs/2303.16202)<br><br>

[6]Adaptive Spot-Guided Transformer for Consistent Local Feature Matching<br>
[paper](https://arxiv.org/abs/2303.16624)<br><br>

[5]PMatch: Paired Masked Image Modeling for Dense Geometric Matching<br>
[paper](https://arxiv.org/abs/2303.17342)<br><br>

[4]Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints<br>
[paper](https://arxiv.org/abs/2304.00583)<br><br>

[3]Referring Image Matting<br>
[paper](https://arxiv.org/abs/2206.05149) | [code](https://github.com/jizhizili/rim)<br><br>

[2]Iterative Geometry Encoding Volume for Stereo Matching<br>
[paper](https://arxiv.org/abs/2303.06615) | [code](https://github.com/gangweix/igev)<br><br>

[1]Modality-Agnostic Debiasing for Single Domain Generalization<br>
[paper](https://arxiv.org/abs/2303.07123) <br><br>

<br>

<a name="VisualRL"/> 

## 视觉表征学习(Visual Representation Learning)

[17]HNeRV: A Hybrid Neural Representation for Videos<br>
[paper](https://arxiv.org/abs/2304.02633) | [code](https://github.com/haochen-rye/hnerv)<br><br>

[16]Learning Rotation-Equivariant Features for Visual Correspondence<br>
[paper](https://arxiv.org/abs/2303.15472)<br><br>

[15]Mixed Autoencoder for Self-supervised Visual Representation Learning<br>
[paper](https://arxiv.org/abs/2303.17152)<br><br>

[14]Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations<br>
[paper](https://arxiv.org/abs/2303.17839)<br><br>

[13]Multi-Modal Representation Learning with Text-Driven Soft Masks<br>
[paper](https://arxiv.org/abs/2304.00719)<br><br>

[12]Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning<br>
[paper](https://arxiv.org/abs/2303.14191)<br><br>

[11]CrOC: Cross-View Online Clustering for Dense Visual Representation Learning<br>
[paper](https://arxiv.org/abs/2303.13245) | [code](https://github.com/stegmuel/croc)<br><br>

[10]Masked Motion Encoding for Self-Supervised Video Representation Learning<br>
[paper](https://arxiv.org/abs/2210.06096) | [code](https://github.com/xinyusun/mme)<br><br>

[9]Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos<br>
[paper](https://arxiv.org/abs/2303.12370) | [code](https://github.com/svip-lab/weaksvr)<br><br>

[8]MARLIN: Masked Autoencoder for facial video Representation LearnINg<br>
[paper](https://arxiv.org/abs/2211.06627) | [code](https://github.com/ControlNet/MARLIN)<br><br>

[7]Hierarchical discriminative learning improves visual representations of biomedical microscopy<br>
[paper](https://arxiv.org/abs/2303.01605)<br><br>

[6]Fine-tuned CLIP Models are Efficient Video Learners<br>
[paper](https://arxiv.org/abs/2212.03640) | [code](https://github.com/muzairkhattak/vifi-clip)<br><br>

[5]Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning<br>
[paper](https://arxiv.org/abs/2212.04500) | [code](https://github.com/ruiwang2021/mvd)<br><br>

[4]Open-Set Representation Learning through Combinatorial Embedding<br>
[paper](https://arxiv.org/abs/2106.15278	)<br><br>

[3]NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction<br>
[paper](https://arxiv.org/abs/2211.08024)<br><br>

[2]Stare at What You See: Masked Image Modeling without Reconstruction<br>
[paper](https://arxiv.org/abs/2211.08887) | [code](https://github.com/openperceptionx/maskalign)<br><br>

[1]Switchable Representation Learning Framework with Self-compatibility<br>
[paper](https://arxiv.org/abs/2206.08289)<br><br>

<br>

<a name="ModelEvaluation"/> 

## 模型评估(Model Evaluation)

[3]ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing<br>
[paper](https://arxiv.org/abs/2303.17096) | [code](https://github.com/alibaba/easyrobust/tree/main/benchmarks/imagenet-e)<br><br>

[2]Physically Adversarial Infrared Patches with Learnable Shapes and Locations<br>
[paper](https://arxiv.org/abs/2303.13868)<br><br>

[1]TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets<br>
[paper](https://arxiv.org/abs/2303.05762) | [code](https://github.com/chenweixin107/trojdiff)<br><br>

<br>

<a name="MMLearning"/> 

## 多模态学习(Multi-Modal Learning)

[18]Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting<br>
[paper](https://arxiv.org/abs/2304.03307) | [code](https://github.com/talalwasim/vita-clip)<br><br>

[17]Detecting and Grounding Multi-Modal Media Manipulation<br>
[paper](https://arxiv.org/abs/2304.02556) | [code](https://github.com/rshaojimmy/multimodal-deepfake)<br><br>

[16]Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-commerce<br>
[paper](https://arxiv.org/abs/2304.02853)<br><br>

[15]Quantum Multi-Model Fitting<br>
[paper](https://arxiv.org/abs/2303.15444) | [code](https://github.com/farinamatteo/qmmf)<br><br>

[14]Towards Flexible Multi-modal Document Models<br>
[paper](https://arxiv.org/abs/2303.18248)<br><br>

[13]CLIP2Scene: Towards Label-Efficient 3D Scene Understanding by CLIP<br>
[paper]( https://arxiv.org/abs/2301.04926) | [code](https://github.com/runnanchen/CLIP2Scene)<br><br>

[12]MaPLe: Multi-modal Prompt Learning<br>
[paper](https://arxiv.org/abs/2210.03117) | [code](https://github.com/muzairkhattak/multimodal-prompt-learning)<br><br>

[11]Decoupled Multimodal Distilling for Emotion Recognition<br>
[paper](https://arxiv.org/abs/2303.13802)<br><br>

[10]MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation<br>
[paper](https://arxiv.org/abs/2212.09478) | [code](https://github.com/researchmm/mm-diffusion)<br><br>

[9]BiCro: Noisy Correspondence Rectification for Multi-modality Data via Bi-directional Cross-modal Similarity Consistency<br>
[paper](https://arxiv.org/abs/2303.12419) | [code](https://github.com/xu5zhao/bicro)<br><br>

[8]Mutilmodal Feature Extraction and Attention-based Fusion for Emotion Estimation in Videos<br>
[paper](https://arxiv.org/abs/2303.10421) | [code](https://github.com/xkwangcn/abaw-5th-rt-iai)<br><br>

[7]Emotional Reaction Intensity Estimation Based on Multimodal Data<br>
[paper](https://arxiv.org/abs/2303.09167)<br><br>

[6]Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers<br>
[paper](https://arxiv.org/abs/2303.09164)<br><br>

[5]Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning<br>
[paper](https://arxiv.org/abs/2303.05952)<br><br>

[4]Multimodal Prompting with Missing Modalities for Visual Recognition<br>
[paper](https://arxiv.org/abs/2303.03369) | [code](https://github.com/yilunlee/missing_aware_prompts)<br><br>

[3]Align and Attend: Multimodal Summarization with Dual Contrastive Losses<br>
[paper](https://arxiv.org/abs/2303.07284) | [code](https://boheumd.github.io/A2Summ/)<br><br>

[2]Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information(通过最大化多模态互信息实现一体化预训练)<br>
[paper](https://arxiv.org/abs/2211.09807) | [code](https://github.com/OpenGVLab/M3I-Pretraining)<br><br>

[1]Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks(Uni-Perceiver v2：用于大规模视觉和视觉语言任务的通才模型)<br>
[paper](https://arxiv.org/abs/2211.09808) | [code](https://github.com/fundamentalvision/Uni-Perceiver)<br><br>


<br>

<a name="Audio-VisualLearning"/> 

### 视听学习(Audio-visual Learning)

[11]Fine-grained Audible Video Description<br>
[paper](https://arxiv.org/abs/2303.15616)<br><br>

[10]Language-Guided Audio-Visual Source Separation via Trimodal Consistency<br>
[paper](https://arxiv.org/abs/2303.16342)<br><br>

[9]Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos<br>
[paper](https://arxiv.org/abs/2303.16897)<br><br>

[8]Audio-Visual Grouping Network for Sound Localization from Mixtures<br>
[paper](https://arxiv.org/abs/2303.17056) | [code](https://github.com/stonemo/avgn)<br><br>

[7]Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment<br>
[paper](https://arxiv.org/abs/2303.17490)<br><br>

[6]Egocentric Audio-Visual Object Localization<br>
[paper](https://arxiv.org/abs/2303.13471) | [code](https://github.com/wikichao/ego-av-loc)<br><br>

[5]Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning<br>
[paper](https://arxiv.org/abs/2303.11302)<br><br>

[4]Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline<br>
[paper](https://arxiv.org/abs/2303.12930)<br><br>

[3]Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring<br>
[paper](https://arxiv.org/abs/2303.08536) | [code](https://github.com/joannahong/av-relscore)<br><br>

[2]CASP-Net: Rethinking Video Saliency Prediction from an Audio-VisualConsistency Perceptual Perspective<br>
[paper](https://arxiv.org/abs/2303.06357) | [code](https://arxiv.org/abs/2303.06357)<br><br>

[1]A Light Weight Model for Active Speaker Detection<br>
[paper](https://arxiv.org/abs/2303.04439) | [code](https://github.com/junhua-liao/light-asd)<br><br>


<br>

<a name="VLRL"/> 

### 视觉-语言（Vision-language）

[30]CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model<br>
[paper](https://arxiv.org/abs/2304.04231) | [code](https://github.com/dk-liang/crowdclip)<br><br>

[29]Improving Vision-and-Language Navigation by Generating Future-View Image Semantics<br>
[paper](https://arxiv.org/abs/2304.04907)<br><br>

[28]Learning to Name Classes for Vision and Language Models<br>
[paper](https://arxiv.org/abs/2304.01830)<br><br>

[27]VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision<br>
[paper](https://arxiv.org/abs/2304.03135) | [code](https://github.com/lmy98129/vlpd)<br><br>

[26]HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models<br>
[paper](https://arxiv.org/abs/2303.15786) | [code](https://github.com/artanic30/hoiclip)<br><br>

[25]KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation<br>
[paper](https://arxiv.org/abs/2303.15796) | [code](https://github.com/xiangyangli20/kerm)<br><br>

[24]PosterLayout: A New Benchmark and Approach for Content-aware Visual-Textual Presentation Layout<br>
[paper](https://arxiv.org/abs/2303.15937) | [code](https://github.com/pku-icst-mipl/posterlayout-cvpr2023)<br><br>

[23]SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision<br>
[paper](https://arxiv.org/abs/2303.17200)<br><br>

[22]VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining<br>
[paper](https://arxiv.org/abs/2303.14302)<br><br>

[21]Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning<br>
[paper](https://arxiv.org/abs/2303.14369) | [code](https://github.com/jpthu17/HBI)<br><br>

[20]IFSeg: Image-free Semantic Segmentation via Vision-Language Model<br>
[paper](https://arxiv.org/abs/2303.14396) | [code](https://github.com/alinlab/ifseg)<br><br>

[19]Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective<br>
[paper](https://arxiv.org/abs/2303.14968) | [code](https://github.com/zwx8981/liqe)<br><br>

[18]MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model<br>
[paper](https://arxiv.org/abs/2210.05335) | [code](https://github.com/iigroup/map)<br><br>

[17]Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning<br>
[paper](https://arxiv.org/abs/2211.13437)<br><br>

[16]Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models<br>
[paper](https://arxiv.org/abs/2301.00182) | [code](https://github.com/whwu95/BIKE)<br><br>

[15]Test of Time: Instilling Video-Language Models with a Sense of Time<br>
[paper](https://arxiv.org/abs/2301.02074) | [code](https://github.com/bpiyush/TestOfTime)<br><br>

[14]Accelerating Vision-Language Pretraining with Free Language Modeling<br>
[paper](https://arxiv.org/abs/2303.14038)<br><br>

[13]Task Residual for Tuning Vision-Language Models<br>
[paper](https://arxiv.org/abs/2211.10277) | [code](https://github.com/geekyutao/taskres)<br><br>

[12]MAGVLT: Masked Generative Vision-and-Language Transformer<br>
[paper](https://arxiv.org/abs/2303.12208)<br><br>

[11]Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding<br>
[paper](https://arxiv.org/abs/2303.12513) | [code](https://github.com/TAU-VAILab/isbertblind)<br><br>

[10]Lana: A Language-Capable Navigator for Instruction Following and Generation<br>
[paper](https://arxiv.org/abs/2303.08409) | [code](https://github.com/wxh1996/lana-vln)<br><br>

[9]FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks<br>
[paper](https://arxiv.org/abs/2303.02483) | [code](https://github.com/brandonhanx/fame-vil)<br><br>

[8]Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding<br>
[paper](https://arxiv.org/abs/2303.04077)<br><br>

[7]Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing<br>
[paper](https://arxiv.org/abs/2301.04558)<br><br>

[6]Connecting Vision and Language with Video Localized Narratives<br>
[paper](https://arxiv.org/abs/2302.11217) | [code](https://github.com/google/video-localized-narratives)<br><br>

[5]Policy Adaptation from Foundation Model Feedback<br>
[paper](https://arxiv.org/abs/2212.07398)<br><br>

[4]Open-vocabulary Attribute Detection<br>
[paper](https://arxiv.org/abs/2211.12914)<br><br>

[3]Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training<br>
[paper](https://arxiv.org/abs/2303.00040) <br><br>

[2]Turning a CLIP Model into a Scene Text Detector<br>
[paper](https://arxiv.org/abs/2302.14338) | [code](https://github.com/wenwenyu/TCM)<br><br>

[1]GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods<br>
[paper](https://arxiv.org/abs/2301.01893) <br><br>


<br>
<a name="Vision-basedPrediction"/> 

## 视觉预测(Vision-based Prediction)

[4]TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.09998)<br><br>

[3]Intention-Conditioned Long-Term Human Egocentric Action Forecasting<br>
[paper](https://arxiv.org/abs/2207.12080) | [code](https://github.com/Evm7/ego4dlta-icvae)<br><br>

[2]Computational Choreography using Human Motion Synthesis<br>
[paper](https://arxiv.org/abs/2210.04366) <br><br>

[1]IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction<br>
[paper](https://arxiv.org/abs/2303.00575) <br><br>



<br>
<a name="Dataset"/> 

## 数据集(Dataset)

[21]Uncurated Image-Text Datasets: Shedding Light on Demographic Bias<br>
[paper](https://arxiv.org/abs/2304.02828) | [code](https://github.com/noagarcia/phase)<br><br>

[20]CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions<br>
[paper](https://arxiv.org/abs/2303.17948)<br><br>

[19]CelebV-Text: A Large-Scale Facial Text-Video Dataset<br>
[paper](https://arxiv.org/abs/2303.14717) | [code](https://github.com/CelebV-Text/CelebV-Text)<br><br>

[18]On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks<br>
[paper](https://arxiv.org/abs/2303.14840) | [code](https://github.com/junggy/hammer-dataset)<br><br>

[17]Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method<br>
[paper](https://arxiv.org/abs/2303.15166) | [code](https://github.com/dreemurr-t/baid)<br><br>

[16]Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding<br>
[paper](https://arxiv.org/abs/2303.15417) | [code](https://github.com/jaehakim97/blurhand_release)<br><br>

[15]GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts<br>
[paper](https://arxiv.org/abs/2211.05272)<br><br>

[14]ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data<br>
[paper](https://arxiv.org/abs/2303.13885)<br><br>

[13]Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts<br>
[paper](https://arxiv.org/abs/2303.14152)<br><br>

[12]A Bag-of-Prototypes Representation for Dataset-Level Applications<br>
[paper](https://arxiv.org/abs/2303.13251)<br><br>

[11]Music-Driven Group Choreography<br>
[paper](https://arxiv.org/abs/2303.12337)<br><br>

[10]RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset<br>
[paper](https://arxiv.org/abs/2303.12564)<br><br>

[9]Backdoor Defense via Adaptively Splitting Poisoned Dataset<br>
[paper](https://arxiv.org/abs/2303.12993) | [code](https://github.com/kuofenggao/asd)<br><br>

[8]Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and Degradation Models<br>
[paper](https://arxiv.org/abs/2303.13031) | [code](https://github.com/andreguo/hdrtvdm)<br><br>

[7]SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments<br>
[paper](https://arxiv.org/abs/2303.09095) | [code](https://github.com/climbingdaily/SLOPER4D)<br><br>

[6]A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others<br>
[paper](https://arxiv.org/abs/2212.04825) | [code](https://github.com/facebookresearch/Whac-A-Mole)<br><br>

[5]MVImgNet: A Large-scale Dataset of Multi-view Images<br>
[paper](https://arxiv.org/abs/2303.06042)<br><br>

[4]Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo<br>
[paper](https://arxiv.org/abs/2303.01943)<br><br>

[3]CUDA: Convolution-based Unlearnable Datasets<br>
[paper](https://arxiv.org/abs/2303.04278)<br><br>

[2]V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception<br>
[paper](http://arxiv.org/abs/2303.07601)<br><br>

[1]Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes<br>
[paper](https://arxiv.org/abs/2303.02760)<br><br>



<br>

<a name="ActiveLearning"/> 

## 主动学习(Active Learning)



<br>

<a name="Few-shotLearning"/> 

## 小样本学习/零样本学习(Few-shot Learning/Zero-shot Learning)

[15]Zero-shot Generative Model Adaptation via Image-specific Prompt Learning<br>
[paper](https://arxiv.org/abs/2304.03119)<br><br>

[14]Zero-shot Model Diagnosis<br>
[paper](https://arxiv.org/abs/2303.15441)<br><br>

[13]AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR<br>
[paper](https://arxiv.org/abs/2303.16501)<br><br>

[12]Hierarchical Dense Correlation Distillation for Few-Shot Segmentation<br>
[paper](https://arxiv.org/abs/2303.14652)<br><br>

[11]ZBS: Zero-shot Background Subtraction via Instance-level Background Modeling and Foreground Selection<br>
[paper](https://arxiv.org/abs/2303.14679) | [code](https://github.com/casia-iva-lab/zbs)<br><br>

[10]Learning Attention as Disentangler for Compositional Zero-shot Learning<br>
[paper](https://arxiv.org/abs/2303.15111) | [code](https://github.com/haoosz/ade-czsl)<br><br>

[9]Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning<br>
[paper](https://arxiv.org/abs/2303.15322) | [code](https://github.com/manliucoder/psvma)<br><br>

[8]CF-Font: Content Fusion for Few-shot Font Generation<br>
[paper](https://arxiv.org/abs/2303.14017)<br><br>

[7]DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection<br>
[paper](https://arxiv.org/abs/2303.09674) | [code](https://github.com/phoenix-v/digeo)<br><br>

[6]Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings<br>
[paper](https://arxiv.org/abs/2303.09352) | [code](https://github.com/uitml/nohub)<br><br>

[5]Bi-directional Distribution Alignment for Transductive Zero-Shot Learning<br>
[paper](https://arxiv.org/abs/2303.08698) | [code](https://github.com/zhicaiwww/bi-vaegan)<br><br>

[4]Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation<br>
[paper](https://arxiv.org/abs/2303.01311)<br><br>

[3]Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners<br>
[paper](https://arxiv.org/abs/2303.02151) | [code](https://github.com/zrrskywalker/cafo)<br><br>

[2]NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging<br>
[paper](https://arxiv.org/abs/2303.04958)<br><br>

[1]FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization<br>
[paper](http://arxiv.org/abs/2303.07418) | [code](https://github.com/jiawei-yang/freenerf)<br><br>

<br>

<a name="ContinualLearning"/> 

## 持续学习(Continual Learning/Life-long Learning)

[7]Asynchronous Federated Continual Learning<br>
[paper](https://arxiv.org/abs/2304.03626) | [code](https://github.com/lttm/fedspace)<br><br>

[6]Exploring Data Geometry for Continual Learning<br>
[paper](https://arxiv.org/abs/2304.03931)<br><br>

[5]Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning<br>
[paper](https://arxiv.org/abs/2304.05288) | [code](https://github.com/wenjinw/par)<br><br>

[4]Online Distillation with Continual Learning for Cyclic Domain Shifts<br>
[paper](https://arxiv.org/abs/2304.01239)<br><br>

[3]Preserving Linear Separability in Continual Learning by Backward Feature Projection<br>
[paper](https://arxiv.org/abs/2303.14595)<br><br>

[2]Computationally Budgeted Continual Learning: What Does Matter?<br>
[paper](https://arxiv.org/abs/2303.11165) | [code](https://github.com/drimpossible/budgetcl)<br><br>

[1]Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning<br>
[paper](https://arxiv.org/abs/2303.09483) | [code](https://github.com/kim-sanghwan/ancl)<br><br>

<br>

<a name="SG"/> 

## 场景图(Scene Graph)

[2]Devil's on the Edges: Selective Quad Attention for Scene Graph Generation<br>
[paper](https://arxiv.org/abs/2304.03495)<br><br>

[1]Probabilistic Debiasing of Scene Graphs<br>
[paper](https://arxiv.org/abs/2211.06444) | [code](https://github.com/bashirulazam/within-triplet-debias)<br><br>

<br>

<a name="SGG"/> 

### 场景图生成(Scene Graph Generation)

[1]Prototype-based Embedding Network for Scene Graph Generation<br>
[paper](https://paperswithcode.com/paper/prototype-based-embedding-network-for-scene) <br><br>

<br>

<a name="SGP"/> 

### 场景图预测(Scene Graph Prediction)

[1]VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud<br>
[paper](https://arxiv.org/abs/2303.14408) | [code](https://github.com/wz7in/cvpr2023-vlsat)<br><br>

<br>

<a name="SGU"/> 

### 场景图理解(Scene Graph Understanding)

[2]SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text<br>
[paper](https://arxiv.org/abs/2204.11964)<br><br>

[1]PLA: Language-Driven Open-Vocabulary 3D Scene Understanding<br>
[paper](https://arxiv.org/abs/2211.16312) | [code](https://github.com/cvmi-lab/pla)<br><br>

<br>

<a name="VisualLocalization"/> 

## 视觉定位/位姿估计(Visual Localization/Pose Estimation)

[7]OrienterNet: Visual Localization in 2D Public Maps with Neural Matching<br>
[paper](https://arxiv.org/abs/2304.02009)<br><br>

[6]Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention<br>
[paper](https://arxiv.org/abs/2303.15274)<br><br>

[5]Human Pose as Compositional Tokens<br>
[paper](https://arxiv.org/abs/2303.11638)<br><br>

[4]Data-efficient Large Scale Place Recognition with Graded Similarity Supervision<br>
[paper](https://arxiv.org/abs/2303.11739) | [code](https://github.com/marialeyvallina/generalized_contrastive_loss)<br><br>

[3]PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers<br>
[paper](https://arxiv.org/abs/2303.09187)<br><br>

[2]StructVPR: Distill Structural Knowledge with Weighting Samples for Visual Place Recognition<br>
[paper](https://arxiv.org/abs/2212.00937)<br><br>

[1]PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow<br>
[paper](https://arxiv.org/abs/2303.02595) <br><br>



<br>

<a name="VisualReasoning"/> 

## 视觉推理/视觉问答(Visual Reasoning/VQA)

[9]Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering<br>
[paper](https://arxiv.org/abs/2304.03754)<br><br>

[8]MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos<br>
[paper](https://arxiv.org/abs/2303.14933) | [code](https://github.com/zzc-1998/md-vqa)<br><br>

[7]3D Concept Learning and Reasoning from Multi-View Images<br>
[paper](https://arxiv.org/abs/2303.11327)<br><br>

[6]Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices<br>
[paper](https://arxiv.org/abs/2303.11730) | [code](https://github.com/xu-jingyi/algebraicmr)<br><br>

[5]Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning<br>
[paper](https://arxiv.org/abs/2303.10482) | [code](https://github.com/szzexpoi/poem)<br><br>

[4]Generative Bias for Robust Visual Question Answering<br>
[paper](https://arxiv.org/abs/2208.00690)<br><br>

[3]MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering<br>
[paper](https://arxiv.org/abs/2303.01239) | [code](https://github.com/jingjing12110/mixphm)<br><br>

[2]Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering<br>
[paper](https://arxiv.org/abs/2303.01903) | [code](https://github.com/milvlg/prophet)<br><br>

[1]From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models<br>
[paper](https://arxiv.org/abs/2212.10846) | [code](https://github.com/salesforce/lavis)<br><br>

<br>

<a name="ImageClassification"/> 

## 图像分类(Image Classification)

[4]Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments<br>
[paper](https://arxiv.org/abs/2210.16046)<br><br>

[3]Semantic Prompt for Few-Shot Image Recognition<br>
[paper](https://arxiv.org/abs/2303.14123)<br><br>

[2]Boosting Verified Training for Robust Image Classifications via Abstraction<br>
[paper](https://arxiv.org/abs/2303.11552) | [code](https://github.com/zhangzhaodi233/abscert)<br><br>

[1]I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification(I2MVFormer：用于零样本图像分类的大型语言模型生成的多视图文档监督)<br>
[paper](https://arxiv.org/abs/2212.02291)<br><br>

<br>

<a name="domain"/> 

## 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)

[17]DATE: Domain Adaptive Product Seeker for E-commerce<br>
[paper](https://arxiv.org/abs/2304.03669)<br><br>

[16]Modernizing Old Photos Using Multiple References via Photorealistic Style Transfer<br>
[paper](https://arxiv.org/abs/2304.04461)<br><br>

[15]GeoNet: Benchmarking Unsupervised Adaptation across Geographies<br>
[paper](https://arxiv.org/abs/2303.15443)<br><br>

[14]C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation<br>
[paper](https://arxiv.org/abs/2303.17132)<br><br>

[13]AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation<br>
[paper](https://arxiv.org/abs/2304.01110) | [code](https://github.com/gzaraunitn/autolabel)<br><br>

[12]BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning<br>
[paper](https://arxiv.org/abs/2303.14773) | [code](https://github.com/changdaeoh/blackvip)<br><br>

[11]Deep Frequency Filtering for Domain Generalization<br>
[paper](https://arxiv.org/abs/2203.12198)<br><br>

[10]Semi-Supervised Domain Adaptation with Source Label Adaptation<br>
[paper](https://arxiv.org/abs/2302.02335) | [code](https://github.com/chu0802/sla)<br><br>

[9]Unsupervised Continual Semantic Adaptation through Neural Rendering<br>
[paper](https://arxiv.org/abs/2211.13969)<br><br>

[8]MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation<br>
[paper](https://arxiv.org/abs/2212.01322) | [code](https://github.com/lhoyer/mic)<br><br>

[7]Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective<br>
[paper](https://arxiv.org/abs/2303.13434)<br><br>

[6]Manipulating Transfer Learning for Property Inference<br>
[paper](https://arxiv.org/abs/2303.11643) | [code](https://github.com/yulongt23/transfer-inference)<br><br>

[5]Trainable Projected Gradient Method for Robust Fine-tuning<br>
[paper](https://arxiv.org/abs/2303.10720)<br><br>

[4]DA-DETR: Domain Adaptive Detection Transformer with Information Fusion<br>
[paper](https://arxiv.org/abs/2103.17084)<br><br>

[3]Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection<br>
[paper](https://arxiv.org/abs/2203.15793) | [code](https://github.com/vibashan/irg-sfda)<br><br>

[2]Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation<br>
[paper](https://arxiv.org/abs/2303.03770) | [code](https://github.com/mattialitrico/guiding-pseudo-labels-with-uncertainty-estimation-for-source-free-unsupervised-domain-adaptation)<br><br>

[1]Adaptive Assignment for Geometry Aware Local Feature Matching<br>
[paper](https://arxiv.org/abs/2207.08427)<br><br>

<br>

<a name="MetricLearning"/> 

## 度量学习(Metric Learning)



<br>

<a name="ContrastiveLearning"/> 

## 对比学习(Contrastive Learning)

[11]FEND: A Future Enhanced Distribution-Aware Contrastive Learning Framework for Long-tail Trajectory Prediction<br>
[paper](https://arxiv.org/abs/2303.16574)<br><br>

[10]Dynamic Conceptional Contrastive Learning for Generalized Category Discovery<br>
[paper](https://arxiv.org/abs/2303.17393) | [code](https://github.com/tpcd/dccl)<br><br>

[9]Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens<br>
[paper](https://arxiv.org/abs/2303.14865)<br><br>

[8]PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery<br>
[paper](https://arxiv.org/abs/2212.05590) | [code](https://github.com/sheng-eatamath/promptcal)<br><br>

[7]Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data<br>
[paper](https://arxiv.org/abs/2303.14080)<br><br>

[6]Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss<br>
[paper](https://arxiv.org/abs/2301.05709)<br><br>

[5]Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation<br>
[paper](https://arxiv.org/abs/2303.12112) | [code](https://github.com/aimagelab/pacscore)<br><br>

[4]MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset<br>
[paper](https://arxiv.org/abs/2303.12756) | [code](https://github.com/MrChenFeng/MaskCon_CVPR2023)<br><br>

[3]CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning<br>
[paper](https://arxiv.org/abs/2303.12793) | [code](https://github.com/FangyunWei/SLRT)<br><br>

[2]Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation<br>
[paper](https://arxiv.org/abs/2303.10323) | [code](https://github.com/mlii0117/dcl)<br><br>

[1]Twin Contrastive Learning with Noisy Labels<br>
[paper](https://arxiv.org/abs/2303.06930) | [code](https://github.com/Hzzone/TCL)<br><br>

<br>

<a name="IncrementalLearning"/> 

## 增量学习(Incremental Learning)

[5]PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning<br>
[paper](https://arxiv.org/abs/2304.04408)<br><br>

[4]On the Stability-Plasticity Dilemma of Class-Incremental Learning<br>
[paper](https://arxiv.org/abs/2304.01663)<br><br>

[3]Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning<br>
[paper](https://arxiv.org/abs/2304.00426) | [code](https://github.com/zysong0113/savc)<br><br>

[2]Class-Incremental Exemplar Compression for Class-Incremental Learning<br>
[paper](https://arxiv.org/abs/2303.14042)<br><br>

[1]Dense Network Expansion for Class Incremental Learning<br>
[paper](https://arxiv.org/abs/2303.12696)<br><br>

<br>

<a name="RL"/> 

## 强化学习(Reinforcement Learning)

[4]Reinforcement Learning-Based Black-Box Model Inversion Attacks<br>
[paper](https://arxiv.org/abs/2304.04625)<br><br>

[3]Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction<br>
[paper](https://arxiv.org/abs/2301.10034) | [code](https://github.com/craftjarvis/mc-controller)<br><br>

[2]ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals<br>
[paper](https://arxiv.org/abs/2303.12071)<br><br>

[1]EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning<br>
[paper](https://arxiv.org/abs/2303.10876) | [code](https://github.com/mediabrain-sjtu/eqmotion)<br><br>

<br>

<a name="MetaLearning"/> 

## 元学习(Meta Learning)

[4]Meta-causal Learning for Single Domain Generalization<br>
[paper](https://arxiv.org/abs/2304.03709)<br><br>

[3]Meta Compositional Referring Expression Segmentation<br>
[paper](https://arxiv.org/abs/2304.04415)<br><br>

[2]Meta-Learning with a Geometry-Adaptive Preconditioner<br>
[paper](https://arxiv.org/abs/2304.01552) | [code](https://github.com/suhyun777/cvpr23-gap)<br><br>

[1]A Meta-Learning Approach to Predicting Performance and Data Requirements<br>
[paper](https://arxiv.org/abs/2303.01598)<br><br>

<br>

<a name="Robotic"/> 

## 机器人(Robotic)

[2]Efficient Map Sparsification Based on 2D and 3D Discretized Grids<br>
[paper](https://arxiv.org/abs/2303.10882)<br><br>

[1]PyPose: A Library for Robot Learning with Physics-based Optimization(PyPose：基于物理优化的机器人学习库)<br>
[paper](https://arxiv.org/abs/2209.15428) | [code](https://pypose.org/)<br><br>


<br>

<a name="self-supervisedlearning"/> 

## 半监督学习/弱监督学习/无监督学习/自监督学习(Self-supervised Learning/Semi-supervised Learning)

[29]Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model<br>
[paper](https://arxiv.org/abs/2304.03572)<br><br>

[28]Token Boosting for Robust Self-Supervised Visual Transformer Pre-training<br>
[paper](https://arxiv.org/abs/2304.04175)<br><br>

[27]SOOD: Towards Semi-Supervised Oriented Object Detection<br>
[paper](https://arxiv.org/abs/2304.04515) | [code](https://github.com/hamperdredes/sood)<br><br>

[26]Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2304.01482) | [code](https://github.com/ucdvision/patchsearch)<br><br>

[25]Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks<br>
[paper](https://arxiv.org/abs/2303.17602) | [code](https://github.com/tinyvision/SOLIDER)<br><br>

[24]Siamese DETR<br>
[paper](https://arxiv.org/abs/2303.18144)<br><br>

[23]HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions<br>
[paper](https://arxiv.org/abs/2304.00387)<br><br>

[22]Detecting Backdoors in Pre-trained Encoders<br>
[paper](https://arxiv.org/abs/2303.15180) | [code](https://github.com/giantseaweed/decree)<br><br>

[21]Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders<br>
[paper](https://arxiv.org/abs/2201.07513)<br><br>

[20]Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2303.01276) | [code](https://github.com/xiaoyao3302/CCVC)<br><br>

[19]ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning<br>
[paper](https://arxiv.org/abs/2303.13556)<br><br>

[18]Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels<br>
[paper](https://arxiv.org/abs/2303.13223)<br><br>

[17]Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching<br>
[paper](https://arxiv.org/abs/2303.10971) | [code](https://github.com/dongliangcao/Self-Supervised-Multimodal-Shape-Matching)<br><br>

[16]Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data<br>
[paper](https://arxiv.org/abs/2303.11066)<br><br>

[15]Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2303.11101)<br><br>

[14]Correlational Image Modeling for Self-Supervised Visual Pre-Training<br>
[paper](https://arxiv.org/abs/2303.12670)<br><br>

[13]Extracting Class Activation Maps from Non-Discriminative Features as well<br>
[paper](https://arxiv.org/abs/2303.10334) | [code](https://github.com/zhaozhengchen/lpcam)<br><br>

[12]TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation<br>
[paper](https://arxiv.org/abs/2303.09870) | [code](https://github.com/devavrattomar/tesla)<br><br>

[11]LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding<br>
[paper](https://arxiv.org/abs/2303.09665)<br><br>

[10]MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection<br>
[paper](https://arxiv.org/abs/2303.09061) | [code](https://github.com/lliuz/mixteacher)<br><br>

[9]Semi-supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination<br>
[paper](https://arxiv.org/abs/2303.06380)<br><br>

[8]Non-Contrastive Unsupervised Learning of Physiological Signals from Video<br>
[paper](https://arxiv.org/abs/2303.07944)<br><br>

[7]Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems<br>
[paper](https://arxiv.org/abs/2303.01669) | [code](https://github.com/ganperf/lcr)<br><br>

[6]Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models<br>
[paper](https://arxiv.org/abs/2303.01869)<br><br>

[5]The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training<br>
[paper]([arxiv.org](https://arxiv.org/abs/2205.12502)) | [code](https://github.com/gicheonkang/gst-visdial)<br><br>

[4]Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2303.06870) | [code](https://github.com/megvii-research/US3L-CVPR2023)<br><br>

[3]Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors<br>
[paper](https://arxiv.org/abs/2302.14746) <br><br>

[2]Siamese Image Modeling for Self-Supervised Vision Representation Learning<br>
[paper](https://arxiv.org/abs/2206.01204) | [code](https://github.com/fundamentalvision/Siamese-Image-Modeling)<br><br>

[1]Cut and Learn for Unsupervised Object Detection and Instance Segmentation<br>
[paper](https://arxiv.org/abs/2301.11320) | [project](http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/)<br><br>

<br>

<a name="interpretability"/> 

## 神经网络可解释性(Neural Network Interpretability)

[6]Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning<br>
[paper](https://arxiv.org/abs/2304.04824)<br><br>

[5]Are Data-driven Explanations Robust against Out-of-distribution Data?<br>
[paper](https://arxiv.org/abs/2303.16390)<br><br>

[4]IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients<br>
[paper](https://arxiv.org/abs/2303.14242) | [code](https://github.com/yangruo1226/idgi)<br><br>

[3]OCTET: Object-aware Counterfactual Explanations<br>
[paper](https://arxiv.org/abs/2211.12380) | [code](https://github.com/valeoai/octet)<br><br>

[2]Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis<br>
[paper](https://arxiv.org/abs/2202.07728)<br><br>

[1]SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries(SplineCam：深度网络几何和决策边界的精确可视化和表征)<br>
[paper](https://arxiv.org/abs/2302.12828) | [code](https://github.com/AhmedImtiazPrio/SplineCAM)<br><br>

<br>

<a name="CrowdCounting"/> 


## 图像计数(Image Counting)

[2]Density Map Distillation for Incremental Object Counting<br>
[paper](https://arxiv.org/abs/2304.05255)<br><br>

[1]Zero-shot Object Counting<br>
[paper](https://arxiv.org/abs/2303.02001)<br><br>

<br>

<a name="federatedlearning"/> 


## 联邦学习(Federated Learning)

[4]The Resource Problem of Using Linear Layer Leakage Attack in Federated Learning<br>
[paper](https://arxiv.org/abs/2303.14868)<br><br>

[3]Make Landscape Flatter in Differentially Private Federated Learning<br>
[paper](https://arxiv.org/abs/2303.11242)<br><br>

[2]STDLens: Model Hijacking-resilient Federated Learning for Object Detection<br>
[paper](https://arxiv.org/abs/2303.11511) | [code](https://github.com/git-disl/stdlens)<br><br>

[1]Re-thinking Federated Active Learning based on Inter-class Diversity<br>
[paper](https://arxiv.org/abs/2303.12317) | [code](https://github.com/raymin0223/logo)<br><br>

<br>

<a name="automatic driving"/> 


## 自动驾驶(automatic driving)


[1]BEVFormer v2: Adapting Modern Image Backbones to Bird’s-Eye-View Recognition via Perspective Supervision(BEVFormer v2：通过透视监督使现代图像主干适应鸟瞰图识别)<br>
[paper](https://arxiv.org/abs/2211.10439)<br><br>

<br>

<a name="100"/> 

## 其他

[74]Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification<br>
[paper](https://arxiv.org/abs/2304.01804) | [code](https://github.com/youngwk/bridgegapexplanationpamc)<br><br>

[73]Knowledge Combination to Learn Rotated Detection Without Rotated Annotation<br>
[paper](https://arxiv.org/abs/2304.02199)<br><br>

[72]CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition<br>
[paper](https://arxiv.org/abs/2304.03167)<br><br>

[71]DC2: Dual-Camera Defocus Control by Learning to Refocus<br>
[paper](https://arxiv.org/abs/2304.03285)<br><br>

[70]Scalable, Detailed and Mask-Free Universal Photometric Stereo<br>
[paper](https://arxiv.org/abs/2303.15724) | [code](https://github.com/satoshi-ikehata/sdm-unips-cvpr2023)<br><br>

[69]DiffCollage: Parallel Generation of Large Content with Diffusion Models<br>
[paper](https://arxiv.org/abs/2303.17076)<br><br>

[68]Why is the winner the best?<br>
[paper](https://arxiv.org/abs/2303.17719)<br><br>

[67]UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning<br>
[paper](https://arxiv.org/abs/2304.00464)<br><br>

[66]HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion<br>
[paper](https://arxiv.org/abs/2304.00932) | [code](https://github.com/sijieaaa/hypliloc)<br><br>

[65]Neural Volumetric Memory for Visual Locomotion Control<br>
[paper](https://arxiv.org/abs/2304.01201)<br><br>

[64]DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality<br>
[paper](https://arxiv.org/abs/2303.14585) | [code](https://github.com/yizhiwang96/deepvecfont-v2)<br><br>

[63]PDPP:Projected Diffusion for Procedure Planning in Instructional Videos<br>
[paper](https://arxiv.org/abs/2303.14676)<br><br>

[62]Disentangling Writer and Character Styles for Handwriting Generation<br>
[paper](https://arxiv.org/abs/2303.14736) | [code](https://github.com/dailenson/sdt)<br><br>

[61]Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation<br>
[paper](https://arxiv.org/abs/2303.14926)<br><br>

[60]DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering<br>
[paper](https://arxiv.org/abs/2303.15101) | [code](https://github.com/lmozart/cvpr2023-dani-net)<br><br>

[59]Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph<br>
[paper](https://arxiv.org/abs/2303.15266) | [code](https://github.com/zhourixin/bronze-ding)<br><br>

[58]Handwritten Text Generation from Visual Archetypes<br>
[paper](https://arxiv.org/abs/2303.15269) | [code](https://github.com/aimagelab/vatr)<br><br>

[57]Level-S2fM: Structure from Motion on Neural Level Set of Implicit Surfaces<br>
[paper](https://arxiv.org/abs/2211.12018)<br><br>

[56]FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network<br>
[paper](https://arxiv.org/abs/2211.15069)<br><br>

[55]ARO-Net: Learning Implicit Fields from Anchored Radial Observations<br>
[paper](https://arxiv.org/abs/2212.10275) | [code](https://github.com/yizhiwang96/aro-net)<br><br>

[54]Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects<br>
[paper](https://arxiv.org/abs/2303.13769)<br><br>

[53]Robust Test-Time Adaptation in Dynamic Scenarios<br>
[paper](https://arxiv.org/abs/2303.13899)<br><br>

[52]LayoutFormer++: Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction<br>
[paper](https://arxiv.org/abs/2207.10660)<br><br>

[51]Doubly Right Object Recognition: A Why Prompt for Visual Rationales<br>
[paper](https://arxiv.org/abs/2212.06202)<br><br>

[50]CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching<br>
[paper](https://arxiv.org/abs/2303.13076)<br><br>

[49]Marching-Primitives: Shape Abstraction from Signed Distance Function<br>
[paper](https://arxiv.org/abs/2303.13190)<br><br>

[48]Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery<br>
[paper](https://arxiv.org/abs/2210.03591) | [code](https://github.com/fanzhichen/ncd-iic)<br><br>

[47]ActMAD: Activation Matching to Align Distributions for Test-Time-Training<br>
[paper](https://arxiv.org/abs/2211.12870) | [code](https://github.com/jmiemirza/actmad)<br><br>

[46]Robust Mean Teacher for Continual and Gradual Test-Time Adaptation<br>
[paper](https://arxiv.org/abs/2211.13081) | [code](https://github.com/mariodoebler/test-time-adaptation)<br><br>

[45]Planning-oriented Autonomous Driving<br>
[paper](https://arxiv.org/abs/2212.10156) | [code](https://github.com/opendrivelab/uniad)<br><br>

[44]Explicit Visual Prompting for Low-Level Structure Segmentations<br>
[paper](https://arxiv.org/abs/2303.10883) | [code](https://github.com/nifangbaage/explicit-visual-prompt)<br><br>

[43]Leapfrog Diffusion Model for Stochastic Trajectory Prediction<br>
[paper](https://arxiv.org/abs/2303.10895) | [code](https://github.com/mediabrain-sjtu/led)<br><br>

[42]Feature Alignment and Uniformity for Test Time Adaptation<br>
[paper](https://arxiv.org/abs/2303.10902)<br><br>

[41]Attribute-preserving Face Dataset Anonymization via Latent Code Optimization<br>
[paper](https://arxiv.org/abs/2303.11296) | [code](https://github.com/chi0tzp/falco)<br><br>

[40]Fix the Noise: Disentangling Source Feature for Controllable Domain Translation<br>
[paper](https://arxiv.org/abs/2303.11545) | [code](https://github.com/LeeDongYeun/FixNoise)<br><br>

[39]Effective Ambiguity Attack Against Passport-based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution<br>
[paper](https://arxiv.org/abs/2303.11595)<br><br>

[38]Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark<br>
[paper](https://arxiv.org/abs/2303.11642) | [code](https://github.com/myniuuu/vcsd)<br><br>

[37]Learning a Depth Covariance Function<br>
[paper](https://arxiv.org/abs/2303.12157)<br><br>

[36]VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions<br>
[paper](https://arxiv.org/abs/2303.12675)<br><br>

[35]Dense Distinct Query for End-to-End Object Detection<br>
[paper](https://arxiv.org/abs/2303.12776) | [code](https://github.com/jshilong/ddq)<br><br>

[34]Facial Affective Analysis based on MAE and Multi-modal Information for 5th ABAW Competition<br>
[paper](https://arxiv.org/abs/2303.10849)<br><br>

[33]Partial Network Cloning<br>
[paper](https://arxiv.org/abs/2303.10597) | [code](https://github.com/jngwenye/pncloning)<br><br>

[32]Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection<br>
[paper](https://arxiv.org/abs/2303.10449) | [code](https://github.com/lufan31/et-ood)<br><br>

[31]Adversarial Counterfactual Visual Explanations<br>
[paper](https://arxiv.org/abs/2303.09962) | [code](https://github.com/guillaumejs2403/ace)<br><br>

[3-]A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation<br>
[paper](https://arxiv.org/abs/2303.09165) | [code](https://github.com/huitangtang/on_the_utility_of_synthetic_data)<br><br>

[29]Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation<br>
[paper](https://arxiv.org/abs/2303.09119) | [code](https://github.com/advocate99/diffgesture)<br><br>

[28]Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry<br>
[paper](https://arxiv.org/abs/2303.08658) | [code](https://github.com/kebii/r2et)<br><br>

[27]Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations<br>
[paper](https://arxiv.org/abs/2202.04235) | [code](https://github.com/twweeb/composite-adv)<br><br>

[26]Backdoor Defense via Deconfounded Representation Learning<br>
[paper](https://arxiv.org/abs/2303.06818) | [code](https://github.com/zaixizhang/cbd)<br><br>

[25]Label Information Bottleneck for Label Enhancement<br>
[paper](https://arxiv.org/abs/2303.06836)<br><br>

[24]LayoutDM: Discrete Diffusion Model for Controllable Layout Generation<br>
[paper](https://arxiv.org/abs/2303.08137) | [code](https://github.com/CyberAgentAILab/layout-dm)<br><br>

[23]Diversity-Aware Meta Visual Prompting<br>
[paper](https://arxiv.org/abs/2303.08138) | [code](https://github.com/shikiw/dam-vp)<br><br>

[22]ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges<br>
[paper](https://arxiv.org/abs/2303.01498)<br><br>

[21]Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving<br>
[paper](https://arxiv.org/abs/2303.01788)<br><br>

[20]UniHCP: A Unified Model for Human-Centric Perceptions<br>
[paper](https://arxiv.org/abs/2303.02936) | [code](https://github.com/opengvlab/unihcp)<br><br>

[19]Where We Are and What We're Looking At: Query Based Worldwide Image Geo-localization Using Hierarchies and Scenes<br>
[paper](https://arxiv.org/abs/2303.04249)<br><br>

[18]Revisiting Rotation Averaging: Uncertainties and Robust Losses<br>
[paper](https://arxiv.org/abs/2303.05195) | [code](https://github.com/zhangganlin/globalsfmpy)<br><br>

[17]3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification<br>
[paper](https://arxiv.org/abs/2212.00338)<br><br>

[16]Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection<br>
[paper](https://arxiv.org/abs/2211.06368) | [code](https://github.com/open-mmlab/mmrotate)<br><br>

[15]Understanding and Improving Visual Prompting: A Label-Mapping Perspective<br>
[paper](https://arxiv.org/abs/2211.11635) | [code](https://github.com/optml-group/ilm-vp)<br><br>

[14]vMAP: Vectorised Object Mapping for Neural Field SLAM<br>
[paper](http://arxiv.org/abs/2302.01838) | [code](https://github.com/kxhit/vMAP)<br><br>

[13]EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization<br>
[paper](https://arxiv.org/abs/2303.01904) <br><br>

[12]Upcycling Models under Domain and Category Shift<br>
[paper](https://arxiv.org/abs/2303.07110) | [code](https://github.com/ispc-lab/GLC)<br><br>

[11]Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images<br>
[paper](https://arxiv.org/abs/2303.06873) | [code](https://github.com/HHHedo/IBMIL)<br><br>

[10]Dynamic Neural Network for Multi-Task Learning Searching across Diverse Network Topologies<br>
[paper](https://arxiv.org/abs/2303.06856)<br><br>

[9]Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples<br>
[paper](https://arxiv.org/abs/2301.01217) | [code](https://github.com/jiamingzhang94/Unlearnable-Clusters)<br><br>

[8]Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation<br>
[paper](https://arxiv.org/abs/2303.00914) <br><br>

[7]Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation<br>
[paper](https://arxiv.org/abs/2303.00914) <br><br>

[6]Physical-World Optical Adversarial Attacks on 3D Face Recognition<br>
[paper](https://arxiv.org/abs/2205.13412) <br><br>

[5]Improving Cross-Modal Retrieval with Set of Diverse Embeddings<br>
[paper](https://arxiv.org/abs/2211.16761) <br><br>

[4]Neural Video Compression with Diverse Contexts<br>
[paper](https://arxiv.org/abs/2302.14402) | [code](https://github.com/microsoft/DCVC)<br><br>

[3]Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger<br>
[paper](https://arxiv.org/abs/2302.14677) <br><br>

[2]Single Image Backdoor Inversion via Robust Smoothed Classifiers<br>
[paper](https://arxiv.org/abs/2303.00215) | [code](https://github.com/locuslab/smoothinv)<br><br>

[1]Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision<br>
[paper](https://arxiv.org/abs/2303.00462) | [code](https://github.com/Toytiny/CMFlow)<br><br>

<br>

<br>

<a name="2"/> 



# 3. CVPR2023 论文解读汇总

1.[CVPR2023｜打破对MIM（掩码图像建模）的数据缩放能力的误解！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638753&idx=2&sn=755ecd0052718db6c02f49f50e5934ef&chksm=ec123318db65ba0edc63d9a3c479ee1763300fd612761458a11f993dcb11517671041f141b88&token=1182786253&lang=zh_CN#rd))<br><br>

2.[CVPR 2023｜基于CLIP的微调新范式！训练速度和性能均创新高！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638629&idx=2&sn=4f24bbaf13968e9d0cabf4a984bd7bf4&chksm=ec12339cdb65ba8a45bd3fe6deb6aae9b5e9b53e7ac7683f462412a114e673e420b04effb99e&token=1182786253&lang=zh_CN#rd))<br><br>

3.[CVPR 2023｜浙大提出全归一化流模型PyramidFlow：高分辨率缺陷异常定位新范式]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638363&idx=1&sn=c1159fc20788f8087fc513d450d33a3f&chksm=ec1230a2db65b9b46a5b1a826d4f3d3743d1894394fe8c010c40773bd5a14ccf6abeaf80bdd3&token=1182786253&lang=zh_CN#rd))<br><br>

4.[CVPR 2023｜大脑视觉信号被Stable Diffusion复现图像！“人类的谋略和谎言不存在了”]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638334&idx=1&sn=e1cfe285aa51210de985ca3c82763d50&chksm=ec1230c7db65b9d10420dc3d332b9ec46a69922b7771b29238c2953ae018d7bca301bf6a8986&token=1182786253&lang=zh_CN#rd))<br><br>

5.[CVPR 2023｜港科大 DA-BEV: 3D目标检测新 SOTA，一种强大的深度信息挖掘方法]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638334&idx=2&sn=ceb12a2dcad0df502d72ab7c211e199f&chksm=ec1230c7db65b9d1e0e1fb43cf1d6c87c0adcabbac7fef1ed31277ee133db696722103709ba5&token=1182786253&lang=zh_CN#rd))<br><br>

6.[CVPR 23｜表征学习超MAE，谷歌等提出MAGE：无监督图像生成超越 Latent Diffusion]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638272&idx=1&sn=454898151d1489a6b668302f8087dd20&chksm=ec1230f9db65b9ef5535a6ecd6b683768dcadca752c074c6ea2d1768106600d3c6b19d07d2f3&token=1182786253&lang=zh_CN#rd))<br><br>

7.[CVPR2023｜不好意思我要加速度了！FasterNet：更高FLOPS才是更快更强的底气]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638140&idx=1&sn=ef59001e77950b1333111f6686a2bf71&chksm=ec123185db65b89334d21dbd993240bbe15d7d5f01460d57e937dfdc6e4032dadfb90a15586a&token=1182786253&lang=zh_CN#rd))<br><br>

8.[CVPR 2023｜大模型流行之下，SN-Net给出一份独特的答卷]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638044&idx=1&sn=c0c8117a6ab456e7344479b2f9452f9d&chksm=ec1231e5db65b8f375fc2f21cac46f23db6431b66c42a6d405fe7a63aab1e14655c0f56f55a6&token=1182786253&lang=zh_CN#rd))<br><br>

9.[CVPR 2023｜结合特征金字塔结构的自监督学习 iTPNs]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638044&idx=3&sn=9ff7c2f74531c958c100d5aa4deeb89d&chksm=ec1231e5db65b8f360c97314e545cad2e98a706a3bf78dcc99f56fc677d5ce45aa2f41805d22&token=1182786253&lang=zh_CN#rd))<br><br>

10.[CVPR 2023｜SQR：对于训练DETR-family目标检测的探索和思考]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637856&idx=1&sn=e72e7c78a5efb8be20880ae6351d8595&chksm=ec123699db65bf8f964bece49f549d6879f92c60976b4ab28ce5d4e63dc89b865fe36b8be62b&token=1182786253&lang=zh_CN#rd))<br><br>

11.[CVPR 2023｜COCO新纪录65.4mAP！InternImage：注入新机制，扩展DCNv3，探索视觉大模型]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637856&idx=2&sn=9ee6fbd7e4d7126a12692d9b0827a4f2&chksm=ec123699db65bf8f4519f6b48fc663dfafc9f6b9b42312cd0ae700db66888598524b36283b24&token=1182786253&lang=zh_CN#rd))<br><br>

12.[CVPR 2023｜YOLOv7强势收录！时隔6年，YOLOv系列再登CVPR！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637752&idx=1&sn=44f42365e781ee941a3470ea875d0f13&chksm=ec123701db65be176c3a6d783030e2565613ef3199753001df509e035a6cd813b4a57612b9af&token=1182786253&lang=zh_CN#rd))<br><br>

13.[CVPR 2023｜谷歌提出Imagic：扩散模型只用文字就能PS照片了！]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247637752&idx=3&sn=f9d479791e4b392b0790e5b5a0a5e5ef&chksm=ec123701db65be178f96580cfa74f58ff965dbcdbf274aa947c7b717c67e0b416c47322673a0&token=1182786253&lang=zh_CN#rd))<br><br>

14.[CVPR 2023｜Lite DETR：计算量减少60%！高效交错多尺度编码器]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639326&idx=3&sn=dd995e387abc37ede3794b39f92cb59d&chksm=ec124ce7db65c5f132c34a3ebfd0038b1532ab0418444dc46766c82724599c53b220bb0b6428&token=693221699&lang=zh_CN#rd))<br><br>

15.[CVPR 2023｜白翔团队新作：借助CLIP完成场景文字检测]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639207&idx=2&sn=90efb1cf701cf8fe09e1ba8711430a2a&chksm=ec124d5edb65c448da904439aa89c8f78d9f2acc6f03df94bdb983574dad58597dc0ca73345f&token=693221699&lang=zh_CN#rd))<br><br>

16.[CVPR'23｜即插即用系列！一种轻量高效的自注意力机制助力图像恢复网络问鼎 SOTA]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639153&idx=1&sn=cab1c897df12dd95d32f9fbb041fb727&chksm=ec124d88db65c49ef92f615aa0d06d735629cb80726ed57b98da38ebcf957b2d5d0b8e315c52&token=693221699&lang=zh_CN#rd))<br><br>

17.[CVPR 2023｜英伟达提出VoxFromer: 单目3D语义场景补全新SOTA]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639124&idx=2&sn=cfcffd7905063c14ccf74b1c952f8fe4&chksm=ec124daddb65c4bb5bad6bddb11b023716b5da38125af15531560ab96d0568f79a131112a95c&token=693221699&lang=zh_CN#rd))<br><br>

18.[CVPR 2023｜EMA-VFI: 基于帧间注意力提取运动和外观信息的高效视频插帧]([url](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247638921&idx=2&sn=0bb7f63c6cf4e58893e0d118fb34ece6&chksm=ec123270db65bb6645e82632228a959f67d776dd7c3ac6d7082e313b81b2588787d40625bf92&token=693221699&lang=zh_CN#rd))<br><br>

19.[CVPR 2023｜Point-NN：​首次实现0参数量、0训练的3D点云分析](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247641114&idx=2&sn=8ea18574b2f792514c416af5453a5374&chksm=ec1245e3db65ccf56a7a722552885e648a234162ce79ab16e9133595e949dcbe4f5743c03e84&token=1084531620&lang=zh_CN#rd)

20.[CVPR 2023｜Prophet: 用小模型启发大语言模型解决外部知识图像问答](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247641011&idx=2&sn=7bb881a16453a6cac32a1bed9c0d3775&chksm=ec124a4adb65c35c0030feff129574ccbd4f39bf75080f2fdd3e2280c36b5668e987b1f3b291&token=1084531620&lang=zh_CN#rd)

21.[CVPR23｜港中文和IDEA联合推出首个大规模全场景人体数据集Human-Art](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640922&idx=3&sn=dd1099fd591d6ff56d8a7d6ec66e5839&chksm=ec124aa3db65c3b5e39d6701ddbc4f798ef5fce0b1dd62e778c0808f985c8b4de1df7b75ba25&token=1084531620&lang=zh_CN#rd)

22.[CVPR 2023 Highlight｜PDPP：基于扩散模型的教学视频过程规划](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640696&idx=3&sn=256219fc6b3b5845b4b092d511bb17b4&chksm=ec124b81db65c297af8b91b1a5d2e8815ae0bb40393aa9338ca90b3e8ad4c5510d4ba433089c&token=1084531620&lang=zh_CN#rd)

23.[CVPR'23｜训练出首个十亿参数量视频自监督大模型！VideoMAE V2: 可扩展的视频基础模型预训练范式](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640600&idx=1&sn=fe0ef4f404a050824173983f2b8fe9a7&chksm=ec124be1db65c2f7bb03cefa7a5bd3a6530c42fabd08712c17af080222d6bf69460d20ea34a5&token=1084531620&lang=zh_CN#rd)

24.[CVPR2023｜基于视觉提示器微调的多模态单目标跟踪算法](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640600&idx=2&sn=5635575dec28e369a8855e3a4cc28b67&chksm=ec124be1db65c2f7cfb17c9c498af9ae76b0399d50ba8b4e4b59b6d079e97f89fa7ccc1b7475&token=1084531620&lang=zh_CN#rd)

25.[CVPR 2023｜全新基于消费者移动设备采集的多样性RGB-D目标跟踪数据集](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640491&idx=2&sn=cd727849e8e61b69ab4369f0f16c2f6c&chksm=ec124852db65c1446fae280b41362202332440d7e7c16ea4e4dd5f8cd01f8f60dcebbf42153b&token=1084531620&lang=zh_CN#rd)

26.[CVPR2023部署Trick｜解决量化误差振荡问题，让MobileNetv2的能力超过ResNet家族](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640425&idx=2&sn=2ec2cb9622858992c8cfcde3e735ce6b&chksm=ec124890db65c186cedbc4a6eb2bcc548e1c3a9718eb0ad780e628c17570d9a5768e2d8a4681&token=1084531620&lang=zh_CN#rd)

27.[CVPR 2023｜IGEV-Stereo & IGEV-MVS：双目立体匹配网络新SOTA!](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640273&idx=2&sn=791533baeabc5216c15998ffd51a2c03&chksm=ec124928db65c03e2bbe32985148f48cfb8737e270f41cac0aa3b687256bf6f5ee9287e40dea&token=1084531620&lang=zh_CN#rd)

28.[CVPR 2023｜UniMatch: 重新审视半监督语义分割中的强弱一致性](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640147&idx=1&sn=b20c20f1dc4db3e13184c61772916eaa&chksm=ec1249aadb65c0bcdc4829c3bba60bd0a2d5e42621478040532924f7a70e5a1b323bb2436240&token=1084531620&lang=zh_CN#rd)

29.[CVPR'23｜DepGraph：任意架构的结构化剪枝，CNN、Transformer、GNN等都适用！](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640147&idx=2&sn=39cc866ba1a473294e799e85f60729ef&chksm=ec1249aadb65c0bcb1b441f15bae5ea297c5b454207ce5e944eb6a32fe4e07ce0c50a08fcca8&token=1084531620&lang=zh_CN#rd)

30.[CVPR 2023｜打破CAM的局限性！ToCo：进一步激发 ViT 在弱监督语义分割的潜力](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247640028&idx=1&sn=b7222d4a4a842c00475606bbc25a59b9&chksm=ec124e25db65c733ba8ad3230b6bf0ed1d57c16528ec3ec17811c796dcdeb491170de8fdd573&token=1084531620&lang=zh_CN#rd)

31.[CVPR 2023｜一键去除视频闪烁，该研究提出了一个通用框架](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639967&idx=3&sn=a72655384a4605697665a4cc0de97dc5&chksm=ec124e66db65c770c36a441f9aa36c5779e10a257a6d49fcd0abefad15413d639c6abed46376&token=1084531620&lang=zh_CN#rd)

32.[CVPR2023｜TriDet:高效时序动作检测网络，刷新三个数据集SOTA！](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639877&idx=2&sn=5a70c3460be1c17952bea674db45a935&chksm=ec124ebcdb65c7aab83727e4014751623980f19d04381916ca7c4e3901f86fdcb6a90ce8eec7&token=1084531620&lang=zh_CN#rd)

33.[CVPR'23｜3D模型分割新方法！不用人工标注，只需一次训练，未标注类别也能识别](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639783&idx=1&sn=7c963096d2786a4d4e130c9039b40e34&chksm=ec124f1edb65c608e71ad8e84c10d69f7b732d434a21900c452107b15a8fa3decae69af93f07&token=1084531620&lang=zh_CN#rd)

34.C[VPR 2023｜标注500类，检测7000类！清华大学等提出通用目标检测算法UniDetector](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639748&idx=2&sn=5b7f454d0dea151aef7b0b76e191970c&chksm=ec124f3ddb65c62b8456542e5b783460d8100aa33f8a6bdafe9980c2b20594cf30af2259d9cc&token=1084531620&lang=zh_CN#rd)

35.[CVPR 2023｜用于半监督目标检测的知识蒸馏方法](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639720&idx=2&sn=4c17e54d8041aae9e8bb1e9159527b8f&chksm=ec124f51db65c647b2d6087cbfb8f25af585afbef1a87da877491963060f880255ab6fc95a87&token=1084531620&lang=zh_CN#rd)

36.[CVPR 2023｜基于图像质量评价的半监督水下复原](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639601&idx=2&sn=a7dd06652130259824ce2ae2b4836ae7&chksm=ec124fc8db65c6deaf4fb24c1b29679780080c771ec1d29374987b13fd239762c8095dc50261&token=1084531620&lang=zh_CN#rd)

37.[CVPR 2023｜基于多层多尺度重建任务的MIM改进算法](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639498&idx=2&sn=2e845d17ef70e5a963c7de7d170919e9&chksm=ec124c33db65c5257b8a942295a86ec1c944a2d0f1ff84b11aba6f970fdcfeaf918ac2f4498d&token=1084531620&lang=zh_CN#rd)

38.[CVPR 2023｜神经网络家族添新丁！小步快跑追求高速的FasterNet](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247639403&idx=1&sn=7f90001b6eaee605ff4edc5dad0c3f9d&chksm=ec124c92db65c584bd0770bcfb5137d3f6538dc3e2bb354d702153f78374bde6b05f1c3db522&token=1084531620&lang=zh_CN#rd)




<br>



<br>

<a name="4"/> 

# 4. CVPR2023论文分享

[极市直播回放第108期丨 潘梓正：模型部署新范式—可缝合神经网络（CVPR 2023）]([url](https://www.cvmart.net/community/detail/7439))

<br>

<br>

<a name="5"/> 

# 5. To do list

* CVPR2023 Workshop
